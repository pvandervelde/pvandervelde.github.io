<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
	<channel>
		<title>Mind vortex</title>
		<link>https://www.petrikvandervelde.nl/</link>
		<description>Welcome!</description>
		<copyright>2022</copyright>
		<pubDate>Sat, 07 May 2022 02:10:48 GMT</pubDate>
		<lastBuildDate>Sat, 07 May 2022 02:10:48 GMT</lastBuildDate>
		<item>
			<title>Starting robotics - Learning Robot Operating System (ROS)</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-learning-ros</link>
			<description>&lt;p&gt;As part of my &lt;a href="tags/Robotics"&gt;journey into robotics&lt;/a&gt; I have been working on updating my
&lt;a href="https://www.scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; to use the
&lt;a href="http://wiki.ros.org/"&gt;Robot Operating System (ROS)&lt;/a&gt;. ROS provides a number of different things
that make robot development much easier. The main items are a middleware layer for communication
between different parts of the robot, hardware abstractions for different sensors, motors and
controllers, device drivers and many other libraries and packages.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-learning-ros</guid>
			<pubDate>Tue, 03 May 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As part of my &lt;a href="tags/Robotics"&gt;journey into robotics&lt;/a&gt; I have been working on updating my
&lt;a href="https://www.scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; to use the
&lt;a href="http://wiki.ros.org/"&gt;Robot Operating System (ROS)&lt;/a&gt;. ROS provides a number of different things
that make robot development much easier. The main items are a middleware layer for communication
between different parts of the robot, hardware abstractions for different sensors, motors and
controllers, device drivers and many other libraries and packages.&lt;/p&gt;
&lt;p&gt;The main benefit of using ROS is that it provides a lot of integrations and functionality that you
can quickly use. On the other hand the drawback that comes with all of this is that the learning curve
for ROS is very steep. The documentation is pretty good and so are the tutorials, however there are
a lot of different parts in ROS, which makes for a lot of interesting ways to get confused. So to
speed up my progress with ROS I decided to do the &lt;a href="https://www.udemy.com/course/ros-essentials/"&gt;ROS for beginners I&lt;/a&gt;
and &lt;a href="https://www.udemy.com/course/ros-navigation/"&gt;II courses&lt;/a&gt; on Udemy. These courses were very helpful
to reduce the learning curve for ROS and quickly get me familiar with ROS.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Scuttle in Gazebo" src="/assets/images/robotics/scuttle/scuttle-in-gazebo.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot in Gazebo&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This post won't explain how ROS works, there are
&lt;a href="https://www.google.com/search?q=getting+started+with+ros&amp;amp;rlz=1C1CHBF_enNZ825NZ825&amp;amp;oq=getting+started+with+ros&amp;amp;aqs=chrome..69i57j69i61.2633j0j7&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8"&gt;many, many, many tutorials out on the web&lt;/a&gt;
that will do a far better job than I can. However I do want to share some of the things I learned
from working with ROS.&lt;/p&gt;
&lt;p&gt;The first thing to note is the operating system on which you want to run ROS. ROS is developed
to be run on Ubuntu. My home PC runs on the Windows Operating System. ROS 1 wasn't designed to run
directly on Windows (ROS2 will be able to) but there are several ways to run it. First you can run
ROS Noetic straight on Windows using &lt;a href="https://robostack.github.io/"&gt;Robostack&lt;/a&gt;. This uses the Conda
package manager and provides packages for all operating systems. I found that this works moderately
well, there are a number of packages missing and occasionally things error out. This approach works
well for simple learning exercises but may yet not be suitable for large ROS applications.&lt;/p&gt;
&lt;p&gt;A second approach is to &lt;a href="https://ishkapoor.medium.com/how-to-install-ros-noetic-on-wsl2-9bbe6c00b89a"&gt;run ROS on WSL2&lt;/a&gt;.
This is able to run the Ubuntu native packages so you can run all parts of ROS and with the help of
an XServer like &lt;a href="https://sourceforge.net/projects/vcxsrv/"&gt;VcXsrv&lt;/a&gt; you can even run all the graphical
tools. One thing to keep in mind if you use WSL is that networking may cause problems if you run
ROS distributed over more than one computing device, e.g. a laptop and a physical robot. With WSL
there is no easy way to expose WSL applications to uninitiated network connections, i.e.
a request started from inside WSL works, but a request started from outside WSL won't be able to
connect. This is important because ROS nodes need to be able to communicate with each other freely.
The result will be that the nodes on the WSL side will seem to be connected and functional while the
other nodes won't be able to send messages to the WSL nodes.&lt;/p&gt;
&lt;p&gt;The final approach to running ROS is to create an &lt;a href="https://gist.github.com/pvandervelde/2282dafc080945ecb7981edb740ed47c"&gt;Ubuntu VM&lt;/a&gt;
or physical machine. In this case as long as the machine is reachable over the network for other
compute devices, it is possible to run ROS distributed over the network. This is the way I currently
run ROS.&lt;/p&gt;
&lt;figure style="float:right"&gt;
&lt;img alt="Scuttle in RViz" src="/assets/images/robotics/scuttle/scuttle-in-rviz-no-sensors.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot in RViz&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Once you have a working ROS installation the next thing you'll find out is that ROS configurations
can be difficult to get right, especially when you're working with a physical robot where visibility
of what is going on may not be the best. There are a number of useful tools available to provide
insights into what is going on with your robot.&lt;/p&gt;
&lt;p&gt;The first tool is &lt;a href="https://gazebosim.org/"&gt;Gazebo&lt;/a&gt; which provides a simulated environment for
ROS robots. The simulation is based on a physics engine with good accuracy of real world physics. It
also provides models for sensors, like LIDAR and cameras, and sensor noise to simulate real-world
sensor behaviour. Having a simulated environment allows you to repeat behaviours many times in
the same way in rapid succession. Having a way to easily repeat behaviours and control the environment
means that you can quickly test and debug specific behaviours, something which can be much more difficult
with a physical robot.&lt;/p&gt;
&lt;p&gt;The second tool, &lt;a href="http://wiki.ros.org/rviz"&gt;RViz&lt;/a&gt;, provides visualization of the environment of the
robot and how the robot perceives that environment. It allows you to visualize what the robot can
&amp;lsquo;see&amp;rsquo;. RViz works by subscribing to the different message topics that are available. This means
it works both for simulated robots (using Gazebo) and physical robots.&lt;/p&gt;
&lt;p&gt;The final tool worth discussing is &lt;a href="https://foxglove.dev/"&gt;Foxglove studio&lt;/a&gt; which also provides
insight into the data that the robot generates, both from sensors but also in the form of messages
sent between the different components of the robot. One of the nice features of Foxglove is that
you can make plots with values provided by messages. For instance you can plot the velocity
components of a &lt;a href="http://docs.ros.org/en/lunar/api/geometry_msgs/html/msg/Twist.html"&gt;Twist message&lt;/a&gt;.
This is useful to compare requested velocities compared to actual achieved velocities.
Another great feature of Foxglove is that it is able to display the &lt;a href="http://wiki.ros.org/rosout"&gt;ROS logs&lt;/a&gt;
and it also allows you to filter and search these logs. Given that ROS logs can quickly become
large the ability to filter is very useful.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Scuttle in RViz with LIDAR overlay" src="/assets/images/robotics/scuttle/scuttle-in-rviz-slam-enabled.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot in RViz with LIDAR overlay&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;When working with a mobile robot, like I am, getting the robot to navigate a space is often one of
the first achievable goals. The &lt;a href="http://wiki.ros.org/navigation"&gt;navigation stack&lt;/a&gt; in ROS provides
a lot of the basic capabilities to get started with robot navigation in a reasonable time span. Do
note however that the navigation stack in ROS is fairly large and has a lot of different configuration
options so it is wise to set some time aside for learning about the different options. I'll talk about
navigating with SCUTTLE in a future post.&lt;/p&gt;
&lt;p&gt;As mentioned I started learning ROS1 with Udemy. My goal for learning ROS was to use it for
navigation with my SCUTTLE  robot, more on that in a future post. Once I manage to get navigation
working for SCUTTLE I plan to start adding different sensors. Finally I want to enable task planning
for SCUTTLE, e.g. tasks like &amp;ldquo;drive to the living room and collect my coffee cup and bring it back
to me&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Another part of my plans is to upgrade to using &lt;a href="https://docs.ros.org/en/galactic/index.html"&gt;ROS2&lt;/a&gt;.
ROS1 end-of-life is 2025, which is only 3 years away, and additionally ROS2 has a more modern stack
with python 3, better communication security, an improved navigation stack and more active development.
More on this will follow in a future post once I have upgraded my robot to ROS2&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics with the SCUTTLE robot</title>
			<link>https://www.petrikvandervelde.nl/posts/Starting-robotics-with-scuttle</link>
			<description>&lt;p&gt;As mentioned in my last post I have started tinkering with mobile robots. My current goal is to
build an outdoor capable &lt;a href="https://en.wikipedia.org/wiki/Autonomous_robot"&gt;autonomous mobile robot&lt;/a&gt;.
The first problem I have to solve in order to move towards my goal is that I know a decent amount
about software, a reasonable amount about structures and mechanics and very little about electronics.
Oh and I know nothing about the robotics algorithms like how navigation works, the fact that robots
may have a hard time
&lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;figuring out where they are&lt;/a&gt;
and that decision making is hard for robots.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Starting-robotics-with-scuttle</guid>
			<pubDate>Fri, 18 Mar 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As mentioned in my last post I have started tinkering with mobile robots. My current goal is to
build an outdoor capable &lt;a href="https://en.wikipedia.org/wiki/Autonomous_robot"&gt;autonomous mobile robot&lt;/a&gt;.
The first problem I have to solve in order to move towards my goal is that I know a decent amount
about software, a reasonable amount about structures and mechanics and very little about electronics.
Oh and I know nothing about the robotics algorithms like how navigation works, the fact that robots
may have a hard time
&lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;figuring out where they are&lt;/a&gt;
and that decision making is hard for robots.&lt;/p&gt;
&lt;p&gt;So in order to not have to learn all the things at the same time I decided it would be sensible to
start off buying a kit that I could assemble and learn to work with. The basic requirements were&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Something that didn't require me to solder electronics or 3d print parts, because I have neither
of those tools, yet ...&lt;/li&gt;
&lt;li&gt;Capable of actually carrying a load of some sort. Most robot kits are fun platforms to play with
but other than driving around they're not actually capable of carrying things. I want my robot to
be able to carry things for me.&lt;/li&gt;
&lt;li&gt;With accessible hardware and software so that I could modify and extend it.&lt;/li&gt;
&lt;li&gt;Affordable, because money is still finite&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="float:right"&gt;
&lt;img alt="Scuttle assembled" src="/assets/images/robotics/scuttle/scuttle-assembled.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot assembled&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After a little bit of searching I decided to buy the &lt;a href="https://scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; kit.
The SCUTTLE kit is an &lt;a href="https://github.com/scuttlerobot"&gt;open source kit&lt;/a&gt; for which all the build
information is available online, from the &lt;a href="https://grabcad.com/library/scuttle-robot-v2-3-1"&gt;3D drawings&lt;/a&gt;
to the material BOM. Additionally there is a lot of sample code that makes it easy to get going with
the robot. There are code samples that allow you to drive the robot with a gamepad or by putting it in
follow mode where it follows a coloured object. Note that when you pick a coloured object apparently
orange is the best colour because of the colour difference with the surroundings. In my case initially
I picked a dark red object in a poorly lit environment with lots of other variations of red around.
You can probably imagine how well that went ...&amp;hellip; [*]&lt;/p&gt;
&lt;p&gt;Assembly of the SCUTTLE robot is pretty easy, it consists of aluminium T-slot lengths, some 3D
printed parts and some electronics parts. The T-slot lengths are fastened with angle brackets and
the 3D printed parts bolt to the T-slot lengths. The kit I bought only required connecting electronic parts
with connectors, no soldering required. If you build a SCUTTLE from scratch there is some soldering to
be done.&lt;/p&gt;
&lt;p&gt;Once you have assembled your SCUTTLE you can test the functionality by using the code samples to
verify the encoders and the motors. Note that it is wise to review your cabling before turning anything
on because it is possible to connect some of the electronics incorrectly. I ended up breaking my
Raspberry Pi, quite possibly by connecting the encoders backwards or something similar.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Scuttle in RViz" src="/assets/images/robotics/scuttle/scuttle-rviz.png" /&gt;
&lt;figcaption&gt;SCUTTLE in RViz&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After verifying that the motors rotate in the correct direction you can try controlling the
robot via the gamepad and drive it around the house.&lt;/p&gt;
&lt;p&gt;Currently I'm working to update my SCUTTLE with the &lt;a href="https://www.ros.org/"&gt;ROS&lt;/a&gt; software. Currently
I'm testing with ROS noetic but I am looking to eventually switch to using ROS2 as it seems to have
a more flexible navigation stack. More on that in a future post.&lt;/p&gt;
&lt;p&gt;I'm also planning to add some sensors to my SCUTTLE to make it a bit more autonomous. The first
plan is to add a bumper that will tell the robot if it has hit something. I have picked up some
contact switches but am still thinking about the design for the bumper. Later on I want to add
sonar, &lt;a href="https://en.wikipedia.org/wiki/Time-of-flight_camera"&gt;Time of Flight (ToF) sensors&lt;/a&gt; and
potentially cameras as well.&lt;/p&gt;
&lt;p&gt;[*] SCUTTLE drove straight at the red coloured couch instead of following the red object I wanted
it to follow&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Building an autonomous mobile robot - But why?</title>
			<link>https://www.petrikvandervelde.nl/posts/Building-autonomous-mobile-robot-why</link>
			<description>&lt;p&gt;Over the last decade I have been developing software of all kinds. I have coded both for work and
for my own learning and entertainment. The majority of the coding I did over that period was code
that lived only in the virtual world. Things from from numerical simulations to building software
infrastructure.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Building-autonomous-mobile-robot-why</guid>
			<pubDate>Mon, 07 Mar 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Over the last decade I have been developing software of all kinds. I have coded both for work and
for my own learning and entertainment. The majority of the coding I did over that period was code
that lived only in the virtual world. Things from from numerical simulations to building software
infrastructure.&lt;/p&gt;
&lt;p&gt;While I have always greatly enjoyed writing code and learning new skills I have found myself being
less and less interested in writing more code that only lives in a virtual world. It feels like
there is no purpose for this code, like something is missing.&lt;/p&gt;
&lt;p&gt;The main thing I feel is missing is interaction with the physical world. The ability to see the
effects of the code when it is executed, to see things move and react to the world. While in
university I studied &lt;a href="https://www.tudelft.nl/en/ae"&gt;Aerospace engineering&lt;/a&gt; and
&lt;a href="https://www.auckland.ac.nz/en/engineering.html"&gt;Mechanical engineering&lt;/a&gt;. Two disciplines heavily
involved with real world physics. The fact that a design lives in the physical world adds all kinds
of additional constraints and behaviours. Some of which the result of interesting physical
behaviour and others the result of the continuous and analogue nature of the physical world.
All of this makes the engineering problems more interesting and challenging.&lt;/p&gt;
&lt;p&gt;One of the ways I have been incorporating making physical things is by working with timber. I've
build several bits of furniture and generally greatly enjoy working with wood. However working with
wood misses the technological side that I do also enjoy.&lt;/p&gt;
&lt;p&gt;A domain that combines both the physical and virtual worlds with large amounts of technology that
recently caught my eye is the world of robotics. As robotics is a combination of many different
fields there is a lot to discover and learn about. Even for the design of a simple robot you will
have to deal with fields like structural mechanics, electronics, software, perception, machine
intelligence etc.. As a bonus thanks to the availability of relatively cheap electronics and
structural components it is possible to build an interesting robot yourself.&lt;/p&gt;
&lt;p&gt;My current goal is to build an &lt;a href="https://en.wikipedia.org/wiki/Autonomous_robot"&gt;autonomous mobile robot&lt;/a&gt;
that is capable of navigating outdoor spaces while carrying some kind of cargo from
one location to another. Before I get to designing and building a robot from the ground
up there are a lot of things to learn. In order to speed up the learning process I
picked up a &lt;a href="https://scuttlerobot.org/"&gt;SCUTTLE&lt;/a&gt; robot kit and started learning about
&lt;a href="https://www.ros.org/"&gt;ROS&lt;/a&gt;. More on that will follow in a future post.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Security</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-security</link>
			<description>&lt;p&gt;One of the final chapters in the description of the development pipeline deals with security. In
this case I specifically mean the security of the pipeline and the underlying infrastructure,
not the security of the applications which are created using the pipeline.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-security</guid>
			<pubDate>Sun, 08 Nov 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;One of the final chapters in the description of the development pipeline deals with security. In
this case I specifically mean the security of the pipeline and the underlying infrastructure,
not the security of the applications which are created using the pipeline.&lt;/p&gt;
&lt;p&gt;The first question is why should you care about the security of the pipeline? After all
developers use the development pipelines via secured networks and their access permissions will be
set at the source control level. Additionally high trust levels exist between the pipeline
processes, the infrastructure and the source repository. In general this leads to the
security of the pipeline being placed lower on the priority list.&lt;/p&gt;
&lt;p&gt;Which issues could you run into if you deem the security of the pipeline less critical? One
argument comes from
&lt;a href="https://www.researchgate.net/publication/332834111_Vulnerabilities_in_Continuous_Delivery_Pipelines_A_Case_Study"&gt;pen tests&lt;/a&gt;
which show that CI/CD systems are a great way into
&lt;a href="https://www.blackhat.com/docs/eu-15/materials/eu-15-Mittal-Continuous-Intrusion-Why-CI-Tools-Are-An-Attackers-Best-Friend.pdf"&gt;corporate networks&lt;/a&gt;.
Additionally there have been a number of attacks aimed at
&lt;a href="https://medium.com/&amp;#64;hkparker/analysis-of-a-supply-chain-attack-2bd8fa8286ac"&gt;distributing&lt;/a&gt; malicious
code through trusted software packages. These so called
&lt;a href="https://en.wikipedia.org/wiki/Supply_chain_attack"&gt;supply chain attacks&lt;/a&gt; try to compromise the user
by inserting malicious code in third-party dependencies, i.e. the source code supply chain.&lt;/p&gt;
&lt;p&gt;In essence the problem comes down to the fact that the build pipeline and its associated infrastructure
have access to many different systems and resources which are normally not easily accessible for
its users. This makes your pipeline a target for malicious actors who could abuse some of the
following states for their own purposes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The development pipeline runs all tasks with the same user account on the executors and thereby
the same permissions. Obviously the worst case scenario would be running as an administrator.&lt;/li&gt;
&lt;li&gt;Multiple pipeline invocations executed on a single machine, either in parallel or sequential,
which allows a task in a pipeline to access the workspace of another pipeline. This ability can
for instance be used to by-pass access controls on source code.&lt;/li&gt;
&lt;li&gt;Downloading packages directly from external package repositories, e.g. NPM or Docker.&lt;/li&gt;
&lt;li&gt;Direct access to the internet, which allows downloading of malicious code and uploading of artefacts
to undesired locations.&lt;/li&gt;
&lt;li&gt;The development pipeline has the ability to update or overwrite existing artefacts.&lt;/li&gt;
&lt;li&gt;The executors have direct access to different resources that normal pipeline users don't have
access to. Specifically if the same infrastructure is used to build artefacts and
to deploy them to the production environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the problems with securing the development pipeline is that all the actions mentioned above
are in one way or another required for the pipeline to function, after all the pipeline needs to be
able to build and distribute artefacts. The follow up question then becomes can you distinguish between
normal use and malicious use?&lt;/p&gt;
&lt;p&gt;It turns out that distinguishing that this will be difficult because both forms of actions are
essentially the same, they both use the development pipeline for its intended purpose. So then in
order to prevent malicious use put up as many barriers to malicious use as possible, aka
&lt;a href="https://en.wikipedia.org/wiki/Defense_in_depth_(computing)"&gt;defence in depth&lt;/a&gt;. The following are a
number of possible ways to add barriers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grant the minimal possible permissions for the executors, both on the executor and from the executor
to the external resources. It is better to run the pipeline actions as a local user on the executor,
rather than using a domain user. Grant permissions to a specific resource to the action that
interacts with the resource.&lt;/li&gt;
&lt;li&gt;Execute a single pipeline per executor and never reuse the executor.&lt;/li&gt;
&lt;li&gt;Limit network connections to and from executors. In general executors do not need internet access,
save for a few pre-approved sites, .e.g. an artefact storage. There is also very little reason
for executors to connect to each other, especially if executors are short lived.&lt;/li&gt;
&lt;li&gt;Pull packages, e.g. NPM or Docker, only from an internal feed. Additions to the internal feed are made
after the specific package has been reviewed.&lt;/li&gt;
&lt;li&gt;The artefacts created with the pipeline should be tracked so that you know the origin, creation time,
storage locations and other data that can help identity an exact instance of an artefact. Under
ideal circumstances you would know exactly which sources and packages were used to create
the artefact as well.&lt;/li&gt;
&lt;li&gt;Artefacts should be immutable and never be allowed to overwritten.&lt;/li&gt;
&lt;li&gt;Do not use the executors that perform builds for deployments, use a set of executors that only
have deployment permissions but no permissions to source control etc..&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beyond these changes there are many other ways to reduce the attack surface as documented in the
security literature. In the end the goal of this post is more to point out that the security of the
development pipeline is important, rather than providing ways to make a pipeline more secure. The
exact solutions for pipeline security depend very heavily on the way the pipeline is constructed and
what other forms of security validation have been placed around the pipeline.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Infrastructure dependency reduction</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-infrastructure-dependency-reduction</link>
			<description>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-considerations-for-infrastructure-improvements.html"&gt;last post&lt;/a&gt;
I explained a few ways to improve the development pipeline infrastructure while
keeping downtime minimal. One important consideration for the
&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;resilience&lt;/a&gt; of the
pipeline is to reduce the dependencies between the pipeline and the infrastructure.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-infrastructure-dependency-reduction</guid>
			<pubDate>Sun, 01 Nov 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-considerations-for-infrastructure-improvements.html"&gt;last post&lt;/a&gt;
I explained a few ways to improve the development pipeline infrastructure while
keeping downtime minimal. One important consideration for the
&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;resilience&lt;/a&gt; of the
pipeline is to reduce the dependencies between the pipeline and the infrastructure.&lt;/p&gt;
&lt;p&gt;So what does unwanted coupling between the pipeline and the
infrastructure mean? After all the pipeline code makes assumptions about the capabilities
of the infrastructure it runs on, just like every other piece of code.
For development pipelines examples of unwanted dependencies are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pipeline is created by using tasks specific to a CI/CD system which means that the artefacts
can only be created on the infrastructure&lt;/li&gt;
&lt;li&gt;Pipeline stages expect the outputs from previous stages to be available on the executor,
or worse, they expect the outputs from other pipelines to be on the executor&lt;/li&gt;
&lt;li&gt;A pipeline task assumes that certain tools have been installed on the executors&lt;/li&gt;
&lt;li&gt;A pipeline task assumes that it has specific permissions on an executor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first two items mentioned impacts the velocity at which development can take place. By relying
completely on the CI/CD system for artefact creation cycle times increases. Additionally
making changes to the pipeline often requires executing it multiple times to debug it.&lt;/p&gt;
&lt;p&gt;The second set of items are related to the ease of switching tooling, e.g. when changing vendors
or during disaster recovery. These may or may not be of concern depending on the direction of
development and technology. In my experience vendor lock-in is something to keep in mind if for no
other reason then that switching vendors can be prohibitively complicated if pipeline processes are
too tightly coupled to the CI/CD system.&lt;/p&gt;
&lt;p&gt;If any of the issues mentioned are concerning to you then partially or completely decoupling
the development pipeline from the infrastructure will be a worthwhile exercise. This
can be achieved with some of the following steps.&lt;/p&gt;
&lt;h4&gt;Versions for everybody&lt;/h4&gt;
&lt;p&gt;Ensure that all code, tools, scripts and resources used in the pipeline are versioned. That way you
know exactly what is required for a specific pipeline execution. Which make executions repeatable
in the future when newer versions of tools and resources have been released.&lt;/p&gt;
&lt;h4&gt;Only the workspace is yours&lt;/h4&gt;
&lt;p&gt;Pipeline actions should work only in their own workspace. Any code, scripts or tools they require
are put in the workspace. This reduces scattering of data during the execution of a pipeline and
reduces pollution of the executors.&lt;/p&gt;
&lt;h4&gt;Lean executors&lt;/h4&gt;
&lt;p&gt;Make your pipeline actions assume they will run on a bare minimum runtime. This will
ensure that your pipelines will obtain their preferred version of tools they need and
install them in their own workspace. The benefit of doing this that a pipeline can be executed on
any available executor, either in your CI/CD system or on a developer machine, as it will not be
making any assumptions about the capabilities of an executor.&lt;/p&gt;
&lt;h4&gt;Single use executors are cleaner&lt;/h4&gt;
&lt;p&gt;Configure the CI/CD system to use a clean executor for every job. This will ensure that changes made
during a previous job don't interfere with the current job. Additionally it will enforce the use of
immutable infrastructure for the executors, thus allowing versioning of the executors.&lt;/p&gt;
&lt;h4&gt;Use the CI/CD system as a task executor&lt;/h4&gt;
&lt;p&gt;Keep the configuration for the jobs in the CI/CD system to a minimum. Ideally the entire configuration
is the execution of a script or tools with a simple set of arguments. By reducing the job of the
CI/CD system to executing a script or tool it is simple to execute the pipeline actions somewhere
else, e.g. on a developer machine or a different CI/CD system.&lt;/p&gt;
&lt;h4&gt;Treat pipeline services as critical&lt;/h4&gt;
&lt;p&gt;All services used by the pipeline should be treated as mission critical systems. After all a failure
in one of these systems can stop all your pipelines from executing. So reduce the number of services
you use in your pipeline and improve the resilience the services you have to rely on using the standard
approaches.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Calvinverse - An example build infrastructure</title>
			<link>https://www.petrikvandervelde.nl/posts/Calvinverse-an-example-build-infrastructure</link>
			<description>&lt;p&gt;This post introduces the &lt;a href="https://www.calvinverse.net/"&gt;Calvinverse&lt;/a&gt;
&lt;a href="https://github.com/Calvinverse"&gt;project&lt;/a&gt; which provides the source code for the different resources
required to create the infrastructure for a build pipeline. The Calvinverse resources have been
developed for two main reasons:&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Calvinverse-an-example-build-infrastructure</guid>
			<pubDate>Fri, 28 Jun 2019 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;This post introduces the &lt;a href="https://www.calvinverse.net/"&gt;Calvinverse&lt;/a&gt;
&lt;a href="https://github.com/Calvinverse"&gt;project&lt;/a&gt; which provides the source code for the different resources
required to create the infrastructure for a build pipeline. The Calvinverse resources have been
developed for two main reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To provide me with a way to experiment with and learn more about
&lt;a href="https://thenewstack.io/a-brief-look-at-immutable-infrastructure-and-why-it-is-such-a-quest/"&gt;immutable&lt;/a&gt; &lt;a href="https://twitter.com/jezhumble/status/970334897544900609"&gt;infrastructure&lt;/a&gt; and
&lt;a href="https://en.wikipedia.org/wiki/Infrastructure_as_code"&gt;infrastructure-as-code&lt;/a&gt; as applied to build infrastructure.&lt;/li&gt;
&lt;li&gt;To provide resources that can be used to set up the infrastructure for a complete
&lt;a href="/posts/On-prem-vs-cloud-build-systems"&gt;on-prem&lt;/a&gt; build system. The system should provide a build
controller with build agents, artefact storage and all the necessary tools to monitor the different
services and diagnose issues.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Calvinverse resources can be configured and deployed for different sizes of infrastructure, from
small setups only used by a few developers to a large setup used by many developers for the development
of many products. How to configure the resources for small, medium or large environments and their
hardware requirements will be discussed in future posts.&lt;/p&gt;
&lt;p&gt;The resources in the Calvinverse project are designed to be run as a self-contained system. While
daily maintenance is minimal it is not a hosted system so some maintenance is required. For instance
OS updates will be required on a regular basis. These can either be applied to existing resources,
through the automatic updates, or by applying the new updates to the templates and then replacing
the existing resources with a new instance. The latter approach case can be automated, however there
is no code in any of the Calvinverse repositories to do this automatically.&lt;/p&gt;
&lt;p&gt;The different resources in the Calvinverse project contain a set of tools and applications which
provide all the necessary capabilities to create the infrastructure for a build pipeline. Amongst these
capabilities are service discovery, build execution, artefact storage, metrics, alerting and
log processing.&lt;/p&gt;
&lt;p&gt;The following applications and approaches are used for service discovery and configuration storage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using &lt;a href="https://www.consul.io"&gt;Consul&lt;/a&gt; to provide service discovery and machine discovery via
&lt;a href="https://www.consul.io/docs/agent/dns.html"&gt;DNS&lt;/a&gt; inside an environment. An environment is defined
as all machines that are part of a &lt;a href="https://www.consul.io/docs/internals/architecture.html"&gt;consul datacenter&lt;/a&gt;.
It is possible to have multiple environments where the machines may all be on the same network but
in general will not be communicating across environments. This is useful for cases where having
multiple environments makes sense, for instance when having a production environment and a test
environment. The benefit of using Consul as a DNS is that it allows a resource to have a consistent
name across different environments without the DNS names clashing. For instance if there is a
production environment and a test environment then it is possible to use the same DNS name
for a resource, even though the actual machine names will be different. This allows using the
Consul DNS name in tools and scripts without having to keep in mind the environment the tool
is deployed in.
Finally Consul is also used for the distributed key-value store that all applications can obtain
configuration information from thereby centralizing the configuration information.&lt;/li&gt;
&lt;li&gt;Using one or more &lt;a href="https://vaultproject.io"&gt;Vault&lt;/a&gt; instance to handle all the secrets required
for the environment. Vault provides authenticated access for resources to securely access secrets,
login credentials and other information that should be kept secure. This allows centralizing the
storage and distribution of secrets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the build work Calvinverse uses the following applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://jenkins.io"&gt;Jenkins&lt;/a&gt; is used as the
&lt;a href="https://github.com/Calvinverse/resource.build.master"&gt;build controller&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Build executors connect to Jenkins using the &lt;a href="https://plugins.jenkins.io/swarm"&gt;swarm plugin&lt;/a&gt; so
that agents can connect when it starts. In the Calvinverse project there are currently only
&lt;a href="https://github.com/Calvinverse/resource.build.agent.windows"&gt;Windows&lt;/a&gt; based executors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;a href="https://github.com/Calvinverse/resource.artefacts"&gt;artefact storage&lt;/a&gt; Calvin verse uses the
&lt;a href="https://www.sonatype.com/nexus-repository-oss"&gt;Nexus&lt;/a&gt; application. The image is configured such that
a new instance of the image will create artefact repositories for &lt;a href="https://www.nuget.org"&gt;NuGet&lt;/a&gt;,
&lt;a href="https://www.npmjs.com/"&gt;Npm&lt;/a&gt;, &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; and general ZIP artefacts.&lt;/p&gt;
&lt;p&gt;For &lt;a href="https://github.com/Calvinverse/resource.queue"&gt;message distribution&lt;/a&gt; Calvinverse uses the &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt; application. The image is configured such that a new instance of the image will
try to connect to the existing cluster in the environment. If no cluster exists then the first
instance of the image will form the start of the cluster in the environment.&lt;/p&gt;
&lt;p&gt;Metrics, monitoring and alerting capabilities are provided by the
&lt;a href="https://www.influxdata.com/"&gt;Influx&lt;/a&gt; stack, consisting of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.influxdata.com/time-series-platform/"&gt;InfluxDB&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.metrics.storage"&gt;metrics collection&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; for &lt;a href="https://github.com/Calvinverse/resource.metrics.dashboard"&gt;dashboards&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.rabbitmq.com/"&gt;Telegraf&lt;/a&gt; is installed on each resource to collect metrics and send
them to InfluxDb.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.influxdata.com/time-series-platform/chronograf/"&gt;Chronograf&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.metrics.monitoring"&gt;alert configurations&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.influxdata.com/time-series-platform/kapacitor/"&gt;Kapacitor&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.metrics.monitoring"&gt;alerting&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Build and system logs are processed by the &lt;a href="https://www.elastic.co/"&gt;Elastic&lt;/a&gt; stack consisting off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/products/elasticsearch"&gt;Elasticsearch&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.documents.storage"&gt;log storage&lt;/a&gt;, both system logs
and build logs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/products/kibana"&gt;Kibana&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.documents.dashboard"&gt;log dashboards&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Logs collected via &lt;a href="https://www.syslog-ng.com/products/open-source-log-management/"&gt;syslog-ng&lt;/a&gt; on
Linux and a modified version of &lt;a href="https://github.com/pvandervelde/filebeat.mqtt"&gt;Filebeat&lt;/a&gt; on windows.
Logs are sent to RabbitMQ to ensure that the unprocessed logs aren't lost when something any part
of the log stack goes offline.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/products/logstash"&gt;Logstash&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.logs.processor"&gt;processing logs&lt;/a&gt; from RabbitMQ to
Elasticsearch using &lt;a href="https://github.com/Calvinverse/calvinverse.logs.filters"&gt;filters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It should be noted that while the Calvinverse resources combine to create a complete build environment
the resources might need some alterations to fit in with the work flow and processes that are being
followed. After all each company is different and applies different workflows. Additionally users
might want to replace some of the resources with versions of their own, e.g. to replace Influx with
&lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - On-prem or in the cloud?</title>
			<link>https://www.petrikvandervelde.nl/posts/On-prem-vs-cloud-build-systems</link>
			<description>&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Continuous_integration"&gt;Continuous integration (CI) systems&lt;/a&gt; originally and
build pipelines recently have traditionally been available on-prem only with systems like &lt;a href="https://jenkins.io"&gt;Jenkins&lt;/a&gt;,
&lt;a href="https://www.jetbrains.com/teamcity/"&gt;TeamCity&lt;/a&gt;, &lt;a href="https://www.atlassian.com/software/bamboo"&gt;Bamboo&lt;/a&gt; and
&lt;a href="https://en.wikipedia.org/wiki/Team_Foundation_Server"&gt;TFS&lt;/a&gt;. This is possibly due to the fact that these
systems needs relatively powerful hardware, mostly consisting of powerful CPU and fast IO, something
which was not easily available in the cloud until the last few years.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/On-prem-vs-cloud-build-systems</guid>
			<pubDate>Thu, 27 Jun 2019 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Continuous_integration"&gt;Continuous integration (CI) systems&lt;/a&gt; originally and
build pipelines recently have traditionally been available on-prem only with systems like &lt;a href="https://jenkins.io"&gt;Jenkins&lt;/a&gt;,
&lt;a href="https://www.jetbrains.com/teamcity/"&gt;TeamCity&lt;/a&gt;, &lt;a href="https://www.atlassian.com/software/bamboo"&gt;Bamboo&lt;/a&gt; and
&lt;a href="https://en.wikipedia.org/wiki/Team_Foundation_Server"&gt;TFS&lt;/a&gt;. This is possibly due to the fact that these
systems needs relatively powerful hardware, mostly consisting of powerful CPU and fast IO, something
which was not easily available in the cloud until the last few years.&lt;/p&gt;
&lt;p&gt;However in the last few years a number of cloud based CI systems have appeared e.g.
&lt;a href="https://azure.microsoft.com/en-in/services/devops/"&gt;Azure DevOps&lt;/a&gt;, &lt;a href="https://www.appveyor.com/"&gt;AppVeyor&lt;/a&gt;,
&lt;a href="https://circleci.com/"&gt;CircleCi&lt;/a&gt;), &lt;a href="https://codeship.com/"&gt;CloudShip&lt;/a&gt;,
&lt;a href="https://cloud.google.com/cloud-build/"&gt;Google cloud build&lt;/a&gt; and &lt;a href="https://travis-ci.org/"&gt;Travis CI&lt;/a&gt;. This
has lead to the question of where to locate a CI system? Should it be on-prem or in the cloud or potentially
even a combination of the two. This post should provide some suggestions on how to make the selection
between the different options.&lt;/p&gt;
&lt;h3&gt;Cloud-based CI systems&lt;/h3&gt;
&lt;p&gt;As with other cloud systems when using a cloud based CI system the user gets the benefits of not
having to worry about the underlying infrastructure and resources and having the ability to scale
the CI system to the size required, provided one pays for the additional resources.&lt;/p&gt;
&lt;p&gt;The other side of the coin is that because the user has no influence on the infrastructure of the
CI system there is also no direct control over the hardware or the controller software. Thus the user
cannot increase the hardware specs for the controller or the agents and the user cannot determine
which plugins or capabilities are available in the CI system.
As a side effect this also means that the user does not have access to the logs, metrics and
file system for the underlying system, which provide information that may be useful when issues
arise. In general the controller specific logs and metrics are only useful if you have access to
the controller, however the build specific information is useful either for diagnostics
or future planning.&lt;/p&gt;
&lt;p&gt;Besides the CI part of the system in some cases the entire pipeline will require other resources, e.g
artefact storage or test systems. Some cloud systems provide these additional systems as well, for a price
of course. Other systems require that these additional resources are provided in some other way.&lt;/p&gt;
&lt;h3&gt;On-prem CI systems&lt;/h3&gt;
&lt;p&gt;When running the CI system on-prem one has to both provide and maintain the infrastructure, hardware and networking etc.,
and the controller and executor software. This increases the overhead for running a CI system. Additionally
scaling the system either requires manual intervention or building the scaling capabilities.&lt;/p&gt;
&lt;p&gt;On the other hand having control over the infrastructure means that the CI system can be configured
so that it fits the use case for the development teams, the desired plugins installed, executors with
all the right tools, full control over executor workspaces and with that the ability to lock down
sensitive information. Additionally logs and metrics can be collected from everywhere which
helps diagnostics, alerting and predictive capabilities on both the infrastructure side and the build
capacity side.&lt;/p&gt;
&lt;p&gt;Finally having full control over the CI system means that it is possible to extend the system if
that is required with custom capabilities, either directly added to the CI system or as additional
services. It should of course be noted that this requires resources and is thus not free.&lt;/p&gt;
&lt;h3&gt;Selecting a location for your CI system&lt;/h3&gt;
&lt;p&gt;So how does one select a location for a CI system. Both cloud and on-prem have pros and cons and in the
end the location of the system depends very much on the situation of the development team. If the team works
for a company where there is no on-prem server infrastructure then a cloud based system will be the
only sensible approach. However there will also be cases where an on-prem system is the only sensible
option.&lt;/p&gt;
&lt;p&gt;In order to decide for one system or the other the first thing that should be done is a cost
comparison, comparing the total cost of ownership, i.e. initial purchasing costs, running costs,
staff costs, training costs etc.. As part of the cost comparison the costs for additional parts
of the system should also be included, e.g. artefact storage or test systems. One should also note
that while cloud systems reduce maintenance, they are not maintenance free. The maintenance of the
infrastructure disappears but the maintenance of the builds and the workflow does not, after all no
matter where the build pipeline is located it is still important that it delivers the
&lt;a href="/posts/Software-development-pipeline-Design-accuracy"&gt;accuracy&lt;/a&gt;,
&lt;a href="/posts/Software-development-pipeline-Design-performance.html"&gt;performance&lt;/a&gt;,
&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;resilience&lt;/a&gt; and
&lt;a href="/posts/Software-development-pipeline-Design-flexibility.html"&gt;flexibility&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once the cost comparison is done there are other things to bring into the decision process. Because
while costs are important they are not the only reason to select one system or another. For instance
other comparison elements could be related to regulations that specify how source needs to be treated
or specific processes that should be followed, or the capabilities of the different CI systems for
example is the ability to execute builds on a specific OS. Not all cloud CI systems provide
executors for all the different OSes.&lt;/p&gt;
&lt;p&gt;In the end the decision to select a cloud build system or a on-prem build system depends
very strongly on the situation the company is in. It is even possible that as time progresses
the best type of system may change from on-prem to cloud or visa versa. Both systems have their
own advantages and disadvantages. In the end all that matters is that a system that fits the
development process is selected, independent of what the different vendors say is the best thing.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Considerations for infrastructure improvements</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-considerations-for-infrastructure-improvements</link>
			<description>&lt;p&gt;In one of the post from a &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;while ago&lt;/a&gt; we
discussed what a software development pipeline is and what the &lt;a href="/posts/Software-development-pipeline-Design-accuracy.html"&gt;most&lt;/a&gt;
&lt;a href="/posts/Software-development-pipeline-Design-performance.html"&gt;important&lt;/a&gt;
&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;characteristics&lt;/a&gt;
&lt;a href="/posts/Software-development-pipeline-Design-flexibility.html"&gt;are&lt;/a&gt;. Given that the pipeline is used
during the large majority of the development, test and release process it is fair to say that for
a software company the build and deployment pipeline infrastructure should be considered critical
infrastructure because without it the development team will be more limited in their ability to
perform their tasks. Note that at no stage should any specific tool, including the pipeline, be the
single point of failure. More on how to reduce the dependency on CI systems and the pipeline will
follow in another post.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-considerations-for-infrastructure-improvements</guid>
			<pubDate>Sun, 27 Jan 2019 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In one of the post from a &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;while ago&lt;/a&gt; we
discussed what a software development pipeline is and what the &lt;a href="/posts/Software-development-pipeline-Design-accuracy.html"&gt;most&lt;/a&gt;
&lt;a href="/posts/Software-development-pipeline-Design-performance.html"&gt;important&lt;/a&gt;
&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;characteristics&lt;/a&gt;
&lt;a href="/posts/Software-development-pipeline-Design-flexibility.html"&gt;are&lt;/a&gt;. Given that the pipeline is used
during the large majority of the development, test and release process it is fair to say that for
a software company the build and deployment pipeline infrastructure should be considered critical
infrastructure because without it the development team will be more limited in their ability to
perform their tasks. Note that at no stage should any specific tool, including the pipeline, be the
single point of failure. More on how to reduce the dependency on CI systems and the pipeline will
follow in another post.&lt;/p&gt;
&lt;p&gt;Just like any other piece of infrastructure the development pipeline will need to be updated
and improved on a regular basis, either to fix bugs, patch security issues or to add new features
that will make the development team more productive. Because the pipeline falls in the critical
infrastructure category it is important to keep disturbances to a minimum while performing these
changes. There are two main parts to providing (nearly) continuous service while still providing
improvements and updates. The first is to ensure that the changes are tracked and tested properly,
the second is to deploy the exact changes that were tested to the production system in a way that no
or minimal interruptions occur. A sensible approach to the first part is to follow a solid software
development process so that the changes are controlled, verified and monitored which can be achieved
by creating infrastructure resources completely from information stored in source control, i.e. using
&lt;a href="https://en.wikipedia.org/wiki/Infrastructure_as_code"&gt;infrastructure-as-code&lt;/a&gt;, making the resources
as &lt;a href="https://thenewstack.io/a-brief-look-at-immutable-infrastructure-and-why-it-is-such-a-quest/"&gt;immutable&lt;/a&gt;
as &lt;a href="https://twitter.com/jezhumble/status/970334897544900609"&gt;possible&lt;/a&gt; and performing automated tests
on these resources after deploying them in a test environment using the same deployment process that
will be used to deploy the resources to the production environment.&lt;/p&gt;
&lt;p&gt;Using this approach should allow the creation of resources that are thoroughly tested and can be
deployed in a sensible fashion. It should be noted that no amount of automated testing will
guarantee that the new resources are free of any issues so it will always be important use deployment
techniques that allow for example &lt;a href="https://martinfowler.com/bliki/BlueGreenDeployment.html"&gt;quick roll-back&lt;/a&gt;
or &lt;a href="https://martinfowler.com/bliki/CanaryRelease.html"&gt;staged roll-outs&lt;/a&gt;. Additionally deployed
resources should be carefully monitored so that issues will be discovered quickly.&lt;/p&gt;
&lt;p&gt;To achieve the goal of being able to deploy updates and improvements to the development
infrastructure the following steps can be taken&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using infrastructure-as-code to create new resource images each time a resource needs to be updated.
Trying to create resources by hand will drastically reduce the ease at which they can be build and
be made consistently. Resources that are deployed into an environment should never be changed. If
bugs need to be fixed or new features need to be added then a new version of the resource image
should be created, tested and deployed. That way changes can be tested before deployment and
the configuration of the deployed resources will be known.&lt;/li&gt;
&lt;li&gt;Resources should be placed on virtual machines or in (Docker) containers. Both technologies provide
an easy way to create one or more instances of the resource which is required in order to test or
scale a service. The general idea is to have one resource per VM / container instance. One resource
may contain multiple services or daemons but it always serves a single goal. Note that in some cases
people will state that you should only use containers and not VMs but there are still cases where
a VM works better, e.g. in some cases executing software builds works better in a VM or running
a service that stores large quantities of data. Additionally if all or a large part of the
infrastructure is running on VMs then using VMs might make more sense. In all cases the correct
approach, container or VMs, is the one that makes sense for the environment the resources will
be deployed into.&lt;/li&gt;
&lt;li&gt;Some way of getting configurations into the resource. Some configurations can be hard-coded into
the resource, if they are never expected to be changed. The draw-back of encoding a configuration
into a resource is that this configuration cannot be changed if the resource is used in different
environments, e.g. a test environment and a production environment. Configurations which are
different between environments should not be encoded in the resource since that may prevent the
resource from being deployed in a test environment for testing. Provisioning a resource requires
that you can apply all the environment specific information to a resource which is a difficult
problem to solve especially for the initial set of configurations, e.g. the configurations which
determine where to get the remaining configurations. Several options are:
&lt;ul&gt;
&lt;li&gt;For VMs you can use DVD / ISO files that are linked on first start-up of the resource.&lt;/li&gt;
&lt;li&gt;Systems like &lt;a href="https://github.com/hashicorp/consul-template"&gt;consul-template&lt;/a&gt; can generate
configurations from a distributed key-value store.&lt;/li&gt;
&lt;li&gt;Resources can be pull their own configurations from a shared store.&lt;/li&gt;
&lt;li&gt;For containers often environment variables are used. These might be sufficient but note that they
are not secure, both inside the container and outside the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Configurations that should be provided when a resource is provisioned should be stored
in source control, just like the resource code is, in order to be able to automate the verification
and delivery of the configuration values.
&lt;ul&gt;
&lt;li&gt;The infrastructure should have it's own shared storage for configurations so that the &amp;lsquo;build&amp;rsquo;
process can push to the shared storage and configurations are distributed from there. That ensures
that the build process doesn't need to know where to deliver exactly (which can change as the
infrastructure changes). One option is to use SQL / no-SQL type storage (e.g. Elasticsearch),
another option is to use a system like &lt;a href="https://consul.io"&gt;consul&lt;/a&gt; which has a distributed key-value
store&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Automatic testing of a resource once it is deployed into an environment. For the very least the
smoke tests should be run automatically when the resource is deployed to a test environment.&lt;/li&gt;
&lt;li&gt;Automatic deployments when a new resource becomes available or approved for an environment, for
the very least to the test environment but ideally to all environments. Using the same deployment
system for all environments is highly recommended because this allows testing the deployment
process as well as the resource.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A general workflow for the creation of a new resource or to update a resource could be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Update the code for the resource. This code can consist of Docker files, &lt;a href="https://www.chef.io/"&gt;Chef&lt;/a&gt;
or &lt;a href="https://puppet.com/"&gt;Puppet&lt;/a&gt;, scripts etc.. The most important thing is that the files are
stored in source control and a sensible source control strategy is used.&lt;/li&gt;
&lt;li&gt;Once the changes are made a new resource can be created from the code.
&lt;ul&gt;
&lt;li&gt;It is sensible to validate the sources using one or more suitable linters. Especially for infrastructure
resources it is sensible to validate the sources before trying to create the resource because it
potentially takes a long time to build a resource. Any errors that can be found sooner in the
process will reduce the cycle time.&lt;/li&gt;
&lt;li&gt;Execute unit tests, e.g. &lt;a href="https://docs.chef.io/chefspec.html"&gt;ChefSpec&lt;/a&gt;, against the sources.
Again, building a resource can take a long time so validation before trying to create the resource
will reduce the cycle time.&lt;/li&gt;
&lt;li&gt;Actually create the new resource. For Docker containers this can be done from a
&lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;docker file&lt;/a&gt;. For a VM this can be done with
&lt;a href="https://packer.io"&gt;Packer&lt;/a&gt;. Building a VM will take longer than building a docker container in
most cases. Note that building resources will in general take longer than building applications
it is sensible to use the build / deployment pipeline to build the resources that make up the
build / deployment pipeline. By using the pipeline it is possible to create the artefacts for
the services and then use these artefacts to create the resource.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deploy the resource to a (small) test environment and execute the tests against the newly created
resource.&lt;/li&gt;
&lt;li&gt;Once the tests have passed the newly made image can be &amp;lsquo;promoted&amp;rsquo;, i.e. approved for use in the
production environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the approaches mentioned above it is possible to improve the development pipeline without
causing unnecessary disturbances for the development team.&lt;/p&gt;
&lt;p&gt;Edit: Changed the title from &lt;code&gt;software delivery pipeline&lt;/code&gt; to &lt;code&gt;software development pipeline&lt;/code&gt; to match
the other posts.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - In the build system or not</title>
			<link>https://www.petrikvandervelde.nl/posts/Sofware-development-pipeline-In-the-build-system-or-not</link>
			<description>&lt;p&gt;Over the last few years the use of build pipelines has been gaining traction backed by the ever growing
use of Continuous Integration (CI) and Continuous Delivery and Deployment (CD) processes. By using a
build pipeline the development team get benefits like being able to execute parts of the build, test,
release and deployment processes in parallel, being able to restart the process part way through
in case of environmental issues, and vastly improved feedback cycles which improve the velocity
at which features can be delivered to the customer.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Sofware-development-pipeline-In-the-build-system-or-not</guid>
			<pubDate>Mon, 03 Dec 2018 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Over the last few years the use of build pipelines has been gaining traction backed by the ever growing
use of Continuous Integration (CI) and Continuous Delivery and Deployment (CD) processes. By using a
build pipeline the development team get benefits like being able to execute parts of the build, test,
release and deployment processes in parallel, being able to restart the process part way through
in case of environmental issues, and vastly improved feedback cycles which improve the velocity
at which features can be delivered to the customer.&lt;/p&gt;
&lt;p&gt;Most modern build systems have the ability to create a build pipelines in one form or another, e.g.
&lt;a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?toc=/azure/devops/pipelines/toc.json&amp;amp;bc=/azure/devops/boards/pipelines/breadcrumb/toc.json&amp;amp;view=vsts"&gt;VSTS / Azure Devops builds&lt;/a&gt;,
&lt;a href="https://jenkins.io/solutions/pipeline/"&gt;Jenkins pipeline&lt;/a&gt;,
&lt;a href="https://docs.gitlab.com/ee/ci/pipelines.html"&gt;GitLab&lt;/a&gt;, &lt;a href="https://bitbucket.org/product/features/pipelines"&gt;BitBucket&lt;/a&gt;
and &lt;a href="https://confluence.jetbrains.com/display/TCD18/Build+Chain"&gt;TeamCity&lt;/a&gt;. With these capabilities
built into the build system it is easy for developers to quickly create a new pipeline from scratch.
While this is quick and easy often the pipeline for a product is created by the development team
without considering if this is the best way to achieve their goal, which is to deliver their product
faster with higher quality. Before using the built-in pipeline capability in the build system the second
question a development team should ask is when should one use this ability and when should one not
use this ability? Obviously the first question is, do we need a pipeline at all, which is a question
for another post.&lt;/p&gt;
&lt;p&gt;The advantages of creating a pipeline in your build system are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is easy to quickly create pipelines. Either the is a click and point UI of some form or the
pipeline is defined by a, relatively, simple configuration file. This means that a development
team can configure a new build pipeline quickly when one is desired.&lt;/li&gt;
&lt;li&gt;Pipelines created in a build system can often use multiple build executors or have a job move
from one executor to another if different capabilities are required for a new step, for instance
if different steps in the pipeline need different operating systems to be executed.&lt;/li&gt;
&lt;li&gt;In many cases, but not all, the build system provides a way for humans to interact with a running
pipeline, for instance to approve the continuation of the pipeline in case of deployments or
to mark a manual test phase as passed or failed.&lt;/li&gt;
&lt;li&gt;If the configuration of the pipeline is stored in a file it can generally be stored in a source
control system, thus providing all the benefits of using a source control system. In these cases
the build system can generally update the build configurations in response to a commit / push
notification from the version control system. Thus ensuring that the active build configuration
is always up to date.&lt;/li&gt;
&lt;li&gt;The development team has nearly complete control over the build configuration which ensures that
it is easy for the development teams to have a pipeline that suits their needs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on the advantages of having a pipeline in the build system it seems pretty straight forward to
say that having the pipeline in the build system is a good thing. However as with all things there
are also drawbacks to having the pipeline in the build system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Having the pipeline in the build system makes some assumptions that may not be correct in certain
cases.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first assumption is that the build system is the centre of all the work being done
because the pipeline is controlled by the build system, thus requiring that all actions feed back
into said build system. This however shouldn't be a given, after all why would the build system
be the core system and not the source control system or the issue tracker. In reality all systems
are required to deliver high quality software. This means in most cases that none of these systems
have enough knowledge by themselves to make decisions about the complete state of the pipeline.
By making the assumption that the build system is at the core of the pipeline the result will
be that the knowledge of the pipeline work flow will end up being encoded in the build configurations
and the build scripts. For simple pipelines this is a sensible thing to do but as the pipeline
gets more complex this approach will be sub-optimal at best and more likely detrimental due to
the complexity of providing all users with the overview of how the pipeline functions.&lt;/li&gt;
&lt;li&gt;The second, but potentially more important, assumption is that the item the development teams
care most about is &amp;lsquo;build&amp;rsquo; or 'build job'. This however is not the case most of the time because
a &amp;lsquo;build&amp;rsquo; is just a way to create or alter an artefact, i.e. the package, container, installer
etc.. It is artefacts that people care about most because artefacts are the carrying vehicle
for the features and bug fixes that the customer cares about. From this perspective it makes
sense to track the artefacts instead of builds because the artefact flows through the entire pipeline
while builds are only part of the pipeline.&lt;/li&gt;
&lt;li&gt;A third assumption is that every task can somehow be run through the build system, but this is
not always the case and even when it is possible it is not necessarily sensible. For instance
builds and deploys are fundamentally different things, one should be repeatable (builds) and
can just be stopped on failure and restarted if necessary and the other is often not exactly
repeatable (because artefacts can only be moved from a location once etc.) and should often
not just be stopped (but rolled-back or not &amp;lsquo;committed&amp;rsquo;). Another example is long running tests
for which the results may be fed back into the build system if required but that doesn't
necessarily make sense.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the build system is the the centre of the pipeline then that means that the build system has to
start storing persistent data about the state of the pipeline with all the issues that come with this
kind of data, for instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The data stored in the pipeline is valuable to the development team both at the current time and
in the future when the development team needs to determine where an artefact comes from. This means
that the data potentially needs to be kept safe for much longer than build information is
normally kept. In order to achieve this the standard data protection rules apply for instance
access controls and backups.&lt;/li&gt;
&lt;li&gt;The information about the pipeline needs to be easily accessible and changeable both by the build
system and by systems external to the build system. It should be possible to add additional
information, e.g. the versions / names of artefacts created by a build. The status of the artefact
as it progresses through the pipeline etc.. All this information is important either during the
pipeline process or after the artefacts have been delivered to the customer. Often build
systems don't have this capability, they store just enough information that they can do what
they need to do, and in general they are not database systems (and if they are it is recommended
that you don't tinker with them and in general it is made difficult to append or add information).&lt;/li&gt;
&lt;li&gt;Build systems work much better if they are immutable, i.e. created from standard components (e.g.
controller and agents) with automatically generated build jobs (more about the reasons both of
these will follow in future posts). This allows a build system to be expanded or replaced really
easily (&lt;a href="https://medium.com/&amp;#64;Joachim8675309/devops-concepts-pets-vs-cattle-2380b5aab313"&gt;cattle not pets&lt;/a&gt;
even for build systems). That is much harder if the build system is the core of your pipeline
and stores all the data for it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Having the pipeline in the build system in general provides more control for the development teams,
which is a great benefit, but less control for the administrators. Because the pipeline provides the
development teams with all the abilities there is, in general, less ability for the admins to guide
things in the right direction or to block developers from doing things that they shouldn't be doing
or have access to. While this may seem to be a benefit for the developers, no more annoying admins
getting in the way, it is in fact a drawback because this behaviour means that the developers take
on the responsibility to administer some or all of the underlying build system. Examples of the
change of control are for instance in the Jenkins pipeline it is possible for developers to use
all the credentials that Jenkins has access to. However this might not be desirable for high power
credentials or credentials for highly restricted resources. An other example is that the selection
of the build executor is done in the pipeline configuration, however in some cases it may make sense
to limit access to executors, after all having a build that can migrate from node to node makes
sense in some cases but it's not free. Further the ease with which parallel steps can be created will
lead to many parallel jobs. This might be great for one pipeline but isn't necessarily the best for
the overall system. In some cases serializing the steps for a single pipeline can lead to greater
overall throughput if there are many different jobs for many different teams.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on all the advantages and disadvantages that are listed here it may be difficult to decide whether
or not a development team should use the pipeline in their build system or not. In general it will be
sensible to use the pipeline capabilities that are build into your build system in cases where you either
have a fairly simple pipeline that is easy to reason about or where no external systems need to interact
with the data in the pipeline.&lt;/p&gt;
&lt;p&gt;Once the pipeline gets more complicated, external systems need access to the metadata describing the
pipeline or the pipeline gets stages that are incompatible with being executed by a build system it
will be time to migrate to a different approach to the build and deployment pipeline. In this case
it is worth it to develop some custom software that tracks artefacts through the pipeline. This makes
it possible to treat the pipeline system as the critical infrastructure that it is, with the appropriate
separation of data and business rule processing, data security and controlled access to the data for
external systems.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Design flexibility</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-flexibility</link>
			<description>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;fourth property&lt;/a&gt; to consider is
&lt;em&gt;flexibility&lt;/em&gt;, i.e. the ability of the pipeline to be able to be modified or adapted without
requiring large changes to be made to the underlying pipeline code and services.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-flexibility</guid>
			<pubDate>Wed, 31 Oct 2018 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;fourth property&lt;/a&gt; to consider is
&lt;em&gt;flexibility&lt;/em&gt;, i.e. the ability of the pipeline to be able to be modified or adapted without
requiring large changes to be made to the underlying pipeline code and services.&lt;/p&gt;
&lt;p&gt;A pipeline should be flexible because the products being build, tested and deployed with that
pipeline may require different workflows or processes in order for them to complete all the
stages in the pipeline. For example building and packaging a library will require a different
approach then building, testing and deploying a cloud service.
Additionally the different stages in the pipeline will require different approaches, e.g. build steps
will in general be executed by a build system returning the results in a synchronous way, however
test steps might run on a different machine from the process that controls the test steps so those
results might come back via an asynchronous route.
Finally flexibility in the pipeline also improves resilience since in case of a disruption
an adaptable or flexible pipeline will allow restoring services through alternate means.&lt;/p&gt;
&lt;p&gt;Making a flexible pipeline is achieved in the same way flexibility is achieved in other software
products, by using modular parts, standard inputs and outputs and carefully considered design. Some
of the appropriate options are for instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Split the pipeline into stages that take standard inputs and deliver standard outputs. There might
be many different types of inputs and outputs but they should be known and easily shared between
processes and applications. There can be one or more stages, e.g. build, test and deploy, which
are dependent on each other only through their inputs and outputs. This allows adding more stages
if required.&lt;/li&gt;
&lt;li&gt;Allow steps or stages in the pipeline to be started through a response to a standard notification.
That allows each step to determine what information it needs to start execution. Additional
information can be downloaded from the appropriate sources upon receiving a notification. This
approach allows notifications to be generic while steps can still acquire the information they
need to execute. Additionally having pipeline steps respond to notifications means that it is
very easy to add new steps in the process because a new executor only has to be instantiated
and connected to the message source, e.g. a distributed queue.&lt;/li&gt;
&lt;li&gt;If a stage consists of multiple, dependent steps, then it should be easy to add and remove
steps based on the requirements. In these cases it would generally be preferred that a stage like
this executes one or more scripts as they are easier to extend than services. As with the stages steps
should ideally use well-known inputs and produce well-known outputs.&lt;/li&gt;
&lt;li&gt;Inputs for stages and steps are for instance
&lt;ul&gt;
&lt;li&gt;Source information, e.g. a commit ID&lt;/li&gt;
&lt;li&gt;Artefacts, e.g. packages installers, zip files etc.&lt;/li&gt;
&lt;li&gt;Meta data, additional information attached to a given output or input, e.g. build or test results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Outputs generated by stages and steps are for instance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Flexibility of the workflow is can further be improved by making sure that the artefacts
generated in the pipeline are not created, tested and deployed in a single monolithic process even
if the end result should be a single artefact. In many cases artefacts can be assembled from smaller
components. Using this approach improves the workflow for the development teams because smaller
components can be created much quicker and in general assembly of a larger piece from components
is quicker and more flexible than regeneration of the entire piece from scratch. In many cases only
a few components will be recreated which both saves time and allows much of the process to be executed
in parallel.&lt;/p&gt;
&lt;p&gt;The exact implementation of the pipeline determines how flexible and easy to extend it will be.
Given that the use and implementation of the pipeline vary quite a lot it is hard to provide
detailed implementation details, however some standard suggestions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep the build part of the pipeline described in the scripts given that scripts are, in general,
easier to adapt. By pulling the scripts from a package, e.g. a NuGet or NPM package, it is
quick and easy to update to a later version of these scripts. An additional benefit of
keeping the process in the scripts is that developers can execute the individual steps of the pipeline
from their local machines. That allows them to ensure builds / tests work before pushing to the
pipeline and provides a means of building things if the pipeline is not available.&lt;/li&gt;
&lt;li&gt;Any part of the process that cannot be done by a script, e.g. test systems, items that need services, e.g.
certificate signing, which require that the certificates are present on the current machine, something
which might not be possible to do on every machine etc., should have a service that is available to both
the pipeline and the developers executing the scripts locally. For any services that should only
be provided to the build server, e.g. signing, the scripts should allow skipping the steps that
need the service.&lt;/li&gt;
&lt;li&gt;For stages that execute scripts, e.g. the build stage, jobs can be automatically generated
from information stored in source control. This makes it easy to update the actions executed by these
stages without requiring developers to perform the configuration manually.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a final note one should consider how the pipeline will be described. It is easier to reason about
a pipeline if the entire description of that pipeline is stored in a single file, ideally in source
control. However as the pipeline evolves and more steps and stages are executed in parallel it will
become increasingly difficult to capture the entire pipeline in a single file. While harder to
reason about it is in the end simpler and more flexible to let the pipeline layout, as in the stages,
steps and orders of these items, be determined by the executors that are available and listening for
notifications. That way it's easy to change the layout of the pipeline.&lt;/p&gt;
&lt;p&gt;And with that we have come to the end of this journey into the guiding principles of designing
a build and release pipeline. There are of course many additions that can be made with regards to
the general design process and even more additions for specific use cases. Those however will have
to wait until another post.&lt;/p&gt;
&lt;h4&gt;Edits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;December 3rd 2018: Fixed a typo in the post title&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Design resilience</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-resilience</link>
			<description>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;third property&lt;/a&gt; to consider is
&lt;em&gt;resilience&lt;/em&gt;, which in this case means that the pipeline should be able to cope with
expected and unexpected changes to the environment it executes in and uses.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-resilience</guid>
			<pubDate>Tue, 19 Dec 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;third property&lt;/a&gt; to consider is
&lt;em&gt;resilience&lt;/em&gt;, which in this case means that the pipeline should be able to cope with
expected and unexpected changes to the environment it executes in and uses.&lt;/p&gt;
&lt;p&gt;David Woods defines &lt;a href="https://www.researchgate.net/publication/276139783_Four_concepts_for_resilience_and_the_implications_for_the_future_of_resilience_engineering"&gt;four different types of &amp;lsquo;resilience&amp;rsquo;&lt;/a&gt; in a paper in the journal of reliability engineering and system safety. One of the different types
is the generally well known form of robustness, i.e. the ability to absorb perturbations or disturbances.
In order to be robust for given disturbances one has to know in advance where the disturbances will
come from, e.g. in the case of a development pipeline it might be expected that pipeline stages will
fail and will pollute or damage parts or all of the executor it was running on. Robustness in this
case would be defined as the ability of the pipeline to handle this damage, for instance by repairing
or replacing the executor. The other definitions for resilience are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rebound, the ability to recover from trauma: In order to achieve this capacity ahead of time is
required, i.e. in order to recover from a disturbance one needs to be able to deploy capabilities
and capacity that was available in excess before the issues occurred.&lt;/li&gt;
&lt;li&gt;Graceful extensibility, the ability to extend adaptive capacity in the face of surprise. This is
the ability to stretch resources and capabilities in the face of surprises.&lt;/li&gt;
&lt;li&gt;Sustained adaptability, which is the ability to adapt and grow new capabilities in the face of
unexpected issues. In general this definition applies more to systems / layered networks where
the loss of sub-systems can be compensated.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which ever definition of resilience is used in general the goal is to be able to recover from
unexpected changes and return back to the normal state, ideally with minimal intervention. An interesting
side note is that returning back to normal after major trauma can be deceiving because the &amp;lsquo;normal&amp;rsquo; as
experienced before the trauma will be different from the 'normal' experienced after the trauma due
to the lessons learned from the trauma and permanent changed caused by the trauma.&lt;/p&gt;
&lt;p&gt;Additionally it is not just the unexpected or traumatic changes that are interesting in the case of a
development pipeline but also the expected ones, e.g. upgrades, maintenance etc., because in general
it is important for the pipeline to continue functioning while those changes are happening.&lt;/p&gt;
&lt;p&gt;For a development pipeline resilience can be approached on different levels. For instance the
pipeline should be resilient against:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Changes in the environment which range from small changes, e.g. additional tools being deployed,
to big changes, e.g. migration of many of the services, and from expected, i.e. maintenance or
planned upgrades, to unexpected&lt;/li&gt;
&lt;li&gt;Changes in the inputs and the results of processing those inputs which may range from build and test
errors to issues with executors&lt;/li&gt;
&lt;li&gt;Invalid or incorrect configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once it is known what resilience actually means and what type of situations the pipeline is expected
to be able to handle the next question is how the pipeline can handle these situations, both in
terms of what the expected responses are and in terms of how the pipeline should be designed.&lt;/p&gt;
&lt;p&gt;There are a myriad of simple steps that can be taken to provide a base level of resilience. None
of these simple steps will guard against major trauma but they will be able to either prevent or
smooth out many of the smaller issues that would otherwise cause the development team to lose faith
in the pipeline outputs. Some examples of simple steps that can be taken to improve resilience in
a development pipeline are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each pipeline step ensure that it is executed in a clean &amp;lsquo;workspace&amp;rsquo;, i.e. a directory or drive,
that will only ever be used by that specific single step. This workspace should be 'private' to the
specific pipeline step and no other processes should be allowed to execute in this workspace. This
prevents issues with unexpected changes to the file system. There are still cases where &amp;lsquo;unexpected&amp;rsquo;
changes to the file system can occur, for instance when running parallel executions within the same
pipeline step in the same workspace. This type of behaviour should therefore be avoided as much as
possible&lt;/li&gt;
&lt;li&gt;Do not depend on global, i.e. machine, container or network, state. Global state has a tendency
to change in random ways at random times.&lt;/li&gt;
&lt;li&gt;Avoid using source which are external to the pipeline infrastructure because these are prone to
unexpected random changes. If a build step requires data from an external source then the external
source should be mirrored and mirrors should be carefully controlled for their content. This should
prevent issues with external packages and inputs changing or disappearing, e.g.&lt;a href="https://www.theregister.co.uk/2016/03/23/npm_left_pad_chaos/"&gt;leftpad&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;If external sources are suitably mirrored inside the pipeline infrastructure then it is possible
to remove the caches for these external sources on the executors. By pulling data in fresh from the
local data store cache pollution issues can be prevented&lt;/li&gt;
&lt;li&gt;Ensure that each resource is appropriately secured against undesirable access. This is especially
true for the executor resources. It is important to note that pipeline steps are essentially random
scripts from an unknown source, even if the scripts are pulled from internal sources, because the
scripts will not be security verified before being used. This means that the pipeline scripts should
not be allowed to to make any changes or to obtain secrets that they shouldn't have access to.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As mentioned the aforementioned steps form a decent base for improving resilience and they are
fairly easy to implement, hence they make a good first stage in the improvement of the resilience
of the development pipeline. Once these steps have been implemented more complex steps can be taken
to further improve the state of the development pipeline. These additional steps can be divided into
items that help prevent issues, items that test and verify the current state, items that aid in
recovery and finally items, like logging and metrics, that help during post-mortems of failure cases.&lt;/p&gt;
&lt;p&gt;One way prevention of trauma / outages can partially be improved is by ensuring that all parts of the
development pipeline are able to handle different error states which can be achieved by building in
extensive error handling capabilities, both for known cases, e.g. service offline, and general error
handling for unexpected cases. For the tooling / script side of the pipeline this means for instance
adding error handling structures nearly everywhere and providing the ability to retry actions.
For the infrastructure side of the pipeline this could mean providing highly available services and
ensuring that service delivery gracefully degrades if it can no longer be provided at the required
standard.&lt;/p&gt;
&lt;p&gt;Even if every possible precaution is taken it is not possible to prevent all modes of failure. Unexpected
failures will always occur no matter what the capabilities of the development pipeline are. This means
that some of the way to improve resilience is to provide capabilities to recover from failures and to
recognise that unexpected conditions exist and to notify the users and administrators of this situation.
It should be noted that providing these capabilities may be much harder to implement due to the
flexible nature of the issues that are being solved for these cases.&lt;/p&gt;
&lt;p&gt;By exposing the system continuously to semi-controlled unexpected conditions it is possible to
provide early and controlled feedback to the operators and administrators regarding the resilience
of the development pipeline. One example of this is the
&lt;a href="https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey"&gt;chaos monkey approach&lt;/a&gt; which tests the
resilience of a system by randomly taking down parts of the system. In a well designed system this
should result in a response of the system in order to restore the now missing capabilities.&lt;/p&gt;
&lt;p&gt;The actual handling of unexpected conditions requires that the system has some capability to instigate
recovery which can for instance consist of having fall-back options for the different sub-systems,
providing automatic remediation services which monitor the system state and apply different standard
recovery techniques like restarting failing services or machines or creating new resources to replace
missing ones.&lt;/p&gt;
&lt;p&gt;From the high level descriptions given above it is hopefully clear that it will not be easy to create
a resilient development pipeline and depending on the demands placed on the pipeline many hours of
work will be consumed by improving the current state and learning from failures. In order to ensure
that this effort is not a wasted effort it is definitely worth applying iterative improvement approaches
and only continuing the improvement process if there is actual demand for improvements.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Design performance</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-performance</link>
			<description>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;second property&lt;/a&gt; to consider is
&lt;em&gt;performance&lt;/em&gt;, which in this case means that the pipeline should provide feedback on the quality of
the current input set as soon as possible in order to reduce the length of the feedback cycle. As is
&lt;a href="https://www.richard-banks.org/2013/04/why-short-feedback-cycle-is-good-thing.html"&gt;known&lt;/a&gt;
&lt;a href="https://www.ambysoft.com/essays/whyAgileWorksFeedback.html"&gt;having&lt;/a&gt; a short
&lt;a href="https://continuousdelivery.com/2012/08/why-software-development-methodologies-suck/"&gt;feedback&lt;/a&gt;
cycle makes it easier for the development teams to make improvements and fix issues.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-performance</guid>
			<pubDate>Sun, 05 Nov 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;second property&lt;/a&gt; to consider is
&lt;em&gt;performance&lt;/em&gt;, which in this case means that the pipeline should provide feedback on the quality of
the current input set as soon as possible in order to reduce the length of the feedback cycle. As is
&lt;a href="https://www.richard-banks.org/2013/04/why-short-feedback-cycle-is-good-thing.html"&gt;known&lt;/a&gt;
&lt;a href="https://www.ambysoft.com/essays/whyAgileWorksFeedback.html"&gt;having&lt;/a&gt; a short
&lt;a href="https://continuousdelivery.com/2012/08/why-software-development-methodologies-suck/"&gt;feedback&lt;/a&gt;
cycle makes it easier for the development teams to make improvements and fix issues.&lt;/p&gt;
&lt;p&gt;There are two main components to development pipeline performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How quickly can one specific input set be processed completely by the pipeline. In other words
how much time does it take to push a single input set through the pipeline from the initial change
to the delivery of the final artefacts. This depends on the number of steps in the development
pipeline and how quickly each step can be executed.&lt;/li&gt;
&lt;li&gt;How quickly can a large set of input sets be processed. The maximum number of executors will most
likely be limited to some maximum value. The pipeline is limited in the number of simultaneous
input sets it can process by the number of available executors. How quickly the pipeline can
process large number of input sets depends both on the time necessary to process a single input set
and the relation between the total number of input sets and the number of executors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Optimizing the combination of these two components will lead to a development pipeline which is
designed for maximum throughput for the development team. One important note to make is that a high
performing pipeline will not not necessarily be the most resource efficient pipeline. For instance
the development pipeline may only be fully loaded a few times a week. From a resource perspective
the pipeline components are more than capable of dealing with the load, in fact the components may
even be oversized. However because one of the main goals of the pipeline is to deliver fast feedback
to the development teams the actual sizing of the pipeline and its components depends more on the
way the pipeline will be loaded over time, e.g. will the jobs come as a constant stream or in
blocks, will the jobs be small or large or will it be a mixture of both. In some cases the loading
pattern can be accurately predicted while in other cases it is completely unpredictable.
In general the pattern will depend on the workflow followed by the development team and the geographical
distribution of the team. For instance when the team follows the Scrum methodology it is
likely, though not necessary, that there will be more builds in the middle of the sprint than at the
start or end. On the other hand when using the Kanban methodology the load on the system should be
fairly consistent. Additionally geographical distribution of the development team influences the
times that the pipeline will be loaded. If all of the team is in a single geographical location then
higher loads can be expected during the day while lighter loads are be expected during the evening
and night. However if the team is distributed across the globe it is more likely that the loading will
be more consistent across the day due to the fact that the different locations have &amp;lsquo;office hours&amp;rsquo; at
different times in the day, as seen from the perspective of the different servers which are part of
the development pipeline. Taking these issues into account when sizing the capacity of the development
pipeline may lead to increasing the capacity of the pipeline because the the current peak loading
during working hours results in wait times which are too large.&lt;/p&gt;
&lt;p&gt;With this high level information it is possible to start improving the performance of the development
pipeline. This obviously leads to the question: &amp;ldquo;What practical steps can we take&amp;rdquo;. As per normal when
dealing with performance improvements it is hard to provide solutions because these depend
on the specific situation. It is however possible to provide some more general advise.&lt;/p&gt;
&lt;p&gt;The very first step to take when dealing with performance is always to measure everything. In the case
of the development pipeline it will be useful to gather metrics constantly and to automatically process
these metrics into several key performance indicators, e.g. the number of input sets per time span, which
describes the loading pattern, the waiting times for each input set before it is processed and the
time taken to process each input set. These key performance indicators can then be used to keep
track of performance improvements as changes are made to the pipeline.&lt;/p&gt;
&lt;p&gt;One important issue to keep in mind with regards to performance is that unlike with accuracy performance
may change over time even if there are no &lt;em&gt;changes&lt;/em&gt; to the system because the performance of the
underlying infrastructure might change, for instance when disks fill up, the network load changes or
the hardware ages. This means it will be important to track performance trends over longer periods of
time to average out the influences of temporary infrastructure changes, e.g. network loading.&lt;/p&gt;
&lt;p&gt;With all that out of the way some of the standard steps that can be taken are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each pipeline stage should only perform the necessary steps to achieve the desired goal. This for
instance means that partial builds are better than full rebuilds, from a performance perspective.&lt;/li&gt;
&lt;li&gt;Only gather data that will be used during the current stage. Gathering data that is not required
wastes time, thus smaller input sets are quicker to process.&lt;/li&gt;
&lt;li&gt;When pulling data locality matters. Pulling data off the local disk is faster than pulling it off
the network, pulling data off the local network is faster than pulling it from the WAN or the internet.
Additionally data that is not local should be cached so that it only needs to be retrieved once.&lt;/li&gt;
&lt;li&gt;Ensure that pipeline stages run on suitable &amp;lsquo;hardware&amp;rsquo;, either physical or virtual. Ideally the
stage is executed on hardware that is optimized for the performance demands of the step, e.g.
execute I/O bound steps on hardware that has fast I/O etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to these improvements it will be important to review and improve the ability of the
pipeline to execute many input sets in parallel.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensure that the pipeline applications which deal with the distribution of input sets are efficient
at this task. It's not very useful to start processing an input set only to find out that there
are no executors which can process this given input set (I'm looking at you TFS2013).&lt;/li&gt;
&lt;li&gt;Splitting a single stage into multiple parallel stages will improve throughput for a single input
set. However it might decrease overall throughput due to the fact that a single input set requires
the use of multiple executors. Note that splitting a single stage into many parallel stages might
lead to reductions in performance due to the overhead of transitioning between stages.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The mentioned improvements form a start for improving the performance of the pipeline. Depending on
the specific characteristics of a given pipeline other improvements and design choices may be valid.&lt;/p&gt;
&lt;p&gt;Finally it must be mentioned that some performance improvements will have negative influences on the
other &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;properties&lt;/a&gt;. For instance using partial
builds may influence accuracy. In the end a trade-off will need to be made when it comes to changes
that influence multiple properties.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Exportable Linux virtual hard-drives for Hyper-V</title>
			<link>https://www.petrikvandervelde.nl/posts/Exportable-Linux-virtual-hard-drives-for-hyper-v</link>
			<description>&lt;p&gt;As part of learning more about infrastructure creation, testing and deployment one of the projects
I'm working on is creating a set of virtual machine images for &lt;a href="https://github.com/Calvinverse/base.windows"&gt;Windows&lt;/a&gt;
and &lt;a href="https://github.com/Calvinverse/base.linux"&gt;Linux&lt;/a&gt; which can be used as a base for more complex
virtual machine based resources, e.g. &lt;a href="https://github.com/Calvinverse/resource.hashi.server"&gt;a consul host&lt;/a&gt;
or &lt;a href="https://github.com/Calvinverse/resource.container.host.linux"&gt;a docker host&lt;/a&gt;.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Exportable-Linux-virtual-hard-drives-for-hyper-v</guid>
			<pubDate>Thu, 14 Sep 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As part of learning more about infrastructure creation, testing and deployment one of the projects
I'm working on is creating a set of virtual machine images for &lt;a href="https://github.com/Calvinverse/base.windows"&gt;Windows&lt;/a&gt;
and &lt;a href="https://github.com/Calvinverse/base.linux"&gt;Linux&lt;/a&gt; which can be used as a base for more complex
virtual machine based resources, e.g. &lt;a href="https://github.com/Calvinverse/resource.hashi.server"&gt;a consul host&lt;/a&gt;
or &lt;a href="https://github.com/Calvinverse/resource.container.host.linux"&gt;a docker host&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main virtualization technology I use is Hyper-V on both Windows 10 and Windows 2016 which allows
creating &lt;a href="https://technet.microsoft.com/en-us/library/dn282285(v=ws.11).aspx"&gt;Generation 2&lt;/a&gt; virtual machines.
Some of the
&lt;a href="https://docs.microsoft.com/en-us/windows-server/virtualization/hyper-v/plan/should-i-create-a-generation-1-or-2-virtual-machine-in-hyper-v"&gt;benefits of a generation 2&lt;/a&gt;
virtual machine are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Boot volume up to 64 Tb&lt;/li&gt;
&lt;li&gt;Use of &lt;a href="https://en.wikipedia.org/wiki/Unified_Extensible_Firmware_Interface"&gt;UEFI&lt;/a&gt; for the boot process&lt;/li&gt;
&lt;li&gt;Faster boot&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The initial version of the base resources allowed creating a virtual machine with
&lt;a href="https://packer.io"&gt;Packer&lt;/a&gt; and exporting that virtual machine to be used as a base. However ideally
all one would need is the virtual hard-drive. The virtual machine configuration can easily be
created for each individual resource and the configuration is usually specific to the original
host by virtue of it containing the absolute path of the virtual hard drive, the name of the network
interfaces etc..&lt;/p&gt;
&lt;p&gt;When building Ubuntu virtual disk images one of the issues with using a Generation 2 virtual machine is that
it uses UEFI for the boot process. It turns out that the Ubuntu install process stores the UEFI files
in the &lt;a href="https://blogs.msdn.microsoft.com/virtual_pc_guy/2015/02/11/copying-the-vhd-of-a-generation-2-linux-vmand-not-booting-afterwards/"&gt;virtual machine configuration file&lt;/a&gt;.
This means that when one creates a new virtual machine from the base virtual disk image it runs into
a problem when booting because the boot files are not present in the new machine. The result is this&lt;/p&gt;
&lt;p&gt;&lt;img align="center" alt="Hyper-V error message due to missing UEFI sector" src="/assets/images/infrastructure/hyperv-gen2-ubuntu-missing-uefi-sector-result.png" /&gt;&lt;/p&gt;
&lt;p&gt;The solution to this issue obviously is to force the Ubuntu installer to write the UEFI files to the
virtual hard disk which can be achieved by adding the correct configuration values to the
&lt;a href="https://help.ubuntu.com/lts/installation-guide/armhf/apb.html"&gt;preseed file&lt;/a&gt;. Unfortunately the
documentation for the different options in the preseed files is hard to find. In the end a combination
of the &lt;a href="https://help.ubuntu.com/lts/installation-guide/example-preseed.txt"&gt;ubuntu sample preseed&lt;/a&gt; file,
bug reports, old forum messages and a few blog
&lt;a href="https://blog.jhnr.ch/2017/02/23/resolving-no-x64-based-uefi-boot-loader-was-found-when-starting-ubuntu-virtual-machine/"&gt;posts&lt;/a&gt;
allowed me to determine that to make the Ubuntu installer place the UEFI files in the correct location
two parts of the preseed file needed to be changed from the default Ubuntu one. The first
part is the partitioning section which requires that at least an &lt;code&gt;EFI&lt;/code&gt; partition and (most likely)
a &lt;code&gt;boot&lt;/code&gt; partition are defined. This almost requires that a custom recipe is defined. The one
I currently use looks as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Or provide a recipe of your own...
# If not, you can put an entire recipe into the preconfiguration file in one
# (logical) line. This example creates a small /boot partition, suitable
# swap, and uses the rest of the space for the root partition:
d-i partman-auto/expert_recipe string       \
    grub-efi-boot-root ::                   \
        1 1 1 free                          \
            $bios_boot{ }                   \
            method{ biosgrub }              \
        .                                   \
        256 256 256 fat32                   \
            $primary{ }                     \
            method{ efi }                   \
            format{ }                       \
        .                                   \
        512 512 512 ext4                    \
            $primary{ }                     \
            $bootable{ }                    \
            method{ format }                \
            format{ }                       \
            use_filesystem{ }               \
            filesystem{ ext4 }              \
            mountpoint{ /boot }             \
        .                                   \
        4096 4096 4096 linux-swap           \
            $lvmok{ }                       \
            method{ swap }                  \
            format{ }                       \
        .                                   \
        10000 20000 -1 ext4                 \
            $lvmok{ }                       \
            method{ format }                \
            format{ }                       \
            use_filesystem{ }               \
            filesystem{ ext4 }              \
            mountpoint{ / }                 \
        .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that syntax for the partioner section is very particular. Note especially the  dots (&lt;code&gt;.&lt;/code&gt;) at
the end of each section. If the syntax isn't completely correct nothing will work but no sensible
error messages will be provided.
Additionally the Ubuntu install complained when there was no &lt;code&gt;swap&lt;/code&gt; section so I added one. This
shouldn't be necessary to get the UEFI files in the correct location but it is apparently necessary
to get Ubuntu to install in the first place.&lt;/p&gt;
&lt;p&gt;The second part of the preseed file that should be changed is the &lt;code&gt;grub-installer&lt;/code&gt; section. There
the following line should be added&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;d-i grub-installer/force-efi-extra-removable boolean true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line indicates that grub should force install the UEFI files, thus overriding the normal state
of not installing the UEFI boot files.&lt;/p&gt;
&lt;p&gt;This means that the complete preseed file looks as follows&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# preseed configuration file for Ubuntu.
# Based on: https://help.ubuntu.com/lts/installation-guide/armhf/apbs04.html

#
# *** Localization ***
#
# Originally from: https://help.ubuntu.com/lts/installation-guide/armhf/apbs04.html#preseed-l10n
#

# Preseeding only locale sets language, country and locale.
d-i debian-installer/locale string en_US.utf8

# Keyboard selection.
# Disable automatic (interactive) keymap detection.
d-i console-setup/ask_detect boolean false
d-i console-setup/layout string us

d-i kbd-chooser/method select American English

#
# *** Network configuration ***
#
# Originally from: https://help.ubuntu.com/lts/installation-guide/armhf/apbs04.html#preseed-network
#

# netcfg will choose an interface that has link if possible. This makes it
# skip displaying a list if there is more than one interface.
d-i netcfg/choose_interface select auto

# If you want the preconfiguration file to work on systems both with and
# without a dhcp server, uncomment these lines and the static network
# configuration below.
d-i netcfg/dhcp_failed note ignore
d-i netcfg/dhcp_options select Configure network manually

# Any hostname and domain names assigned from dhcp take precedence over
# values set here. However, setting the values still prevents the questions
# from being shown, even if values come from dhcp.
d-i netcfg/get_hostname string unassigned-hostname
d-i netcfg/get_domain string unassigned-domain

# Disable that annoying WEP key dialog.
d-i netcfg/wireless_wep string


#
# *** Account setup ***
#
# Originally from: https://help.ubuntu.com/lts/installation-guide/armhf/apbs04.html#preseed-account
#

# To create a normal user account.
d-i passwd/user-fullname string localadmin
d-i passwd/username string localadmin

# Normal user's password, either in clear text
d-i passwd/user-password password reallygoodpassword
d-i passwd/user-password-again password reallygoodpassword

# The installer will warn about weak passwords. If you are sure you know
# what you're doing and want to override it, uncomment this.
d-i user-setup/encrypt-home boolean false
d-i user-setup/allow-password-weak boolean true

# Set to true if you want to encrypt the first user's home directory.
d-i user-setup/encrypt-home boolean false


#
# *** Clock and time zone setup ***
#
# Originally from: https://help.ubuntu.com/lts/installation-guide/armhf/apbs04.html#preseed-time
#

# Controls whether or not the hardware clock is set to UTC.
d-i clock-setup/utc boolean true
d-i clock-setup/utc-auto boolean true

# You may set this to any valid setting for $TZ; see the contents of
# /usr/share/zoneinfo/ for valid values.
d-i time/zone string UTC


#
# *** Partitioning ***
#
# Originally from: https://help.ubuntu.com/lts/installation-guide/armhf/apbs04.html#preseed-partman
#

# This makes partman automatically partition without confirmation, provided
# that you told it what to do using one of the methods below.
d-i partman/choose_partition select finish
d-i partman/confirm boolean true
d-i partman/confirm_nooverwrite boolean true

# In addition, you'll need to specify the method to use.
# The presently available methods are:
# - regular: use the usual partition types for your architecture
# - lvm:     use LVM to partition the disk
# - crypto:  use LVM within an encrypted partition
d-i partman-auto/method string lvm
d-i partman-auto/purge_lvm_from_device boolean true

# If one of the disks that are going to be automatically partitioned
# contains an old LVM configuration, the user will normally receive a
# warning. This can be preseeded away...
d-i partman-lvm/device_remove_lvm boolean true
d-i partman-lvm/device_remove_lvm_span boolean true

# And the same goes for the confirmation to write the lvm partitions.
d-i partman-lvm/confirm boolean true
d-i partman-lvm/confirm_nooverwrite boolean true

# For LVM partitioning, you can select how much of the volume group to use
# for logical volumes.
d-i partman-auto-lvm/guided_size string max
d-i partman-auto-lvm/new_vg_name string system

# You can choose one of the three predefined partitioning recipes:
# - atomic: all files in one partition
# - home:   separate /home partition
# - multi:  separate /home, /usr, /var, and /tmp partitions
d-i partman-auto/choose_recipe select grub-efi-boot-root

d-i partman-partitioning/confirm_write_new_label boolean true

# If you just want to change the default filesystem from ext3 to something
# else, you can do that without providing a full recipe.
d-i partman/default_filesystem string ext4

# Or provide a recipe of your own...
# If not, you can put an entire recipe into the preconfiguration file in one
# (logical) line. This example creates a small /boot partition, suitable
# swap, and uses the rest of the space for the root partition:
d-i partman-auto/expert_recipe string       \
    grub-efi-boot-root ::                   \
        1 1 1 free                          \
            $bios_boot{ }                   \
            method{ biosgrub }              \
        .                                   \
        256 256 256 fat32                   \
            $primary{ }                     \
            method{ efi }                   \
            format{ }                       \
        .                                   \
        512 512 512 ext4                    \
            $primary{ }                     \
            $bootable{ }                    \
            method{ format }                \
            format{ }                       \
            use_filesystem{ }               \
            filesystem{ ext4 }              \
            mountpoint{ /boot }             \
        .                                   \
        4096 4096 4096 linux-swap           \
            $lvmok{ }                       \
            method{ swap }                  \
            format{ }                       \
        .                                   \
        10000 20000 -1 ext4                 \
            $lvmok{ }                       \
            method{ format }                \
            format{ }                       \
            use_filesystem{ }               \
            filesystem{ ext4 }              \
            mountpoint{ / }                 \
        .

d-i partman-partitioning/no_bootable_gpt_biosgrub boolean false
d-i partman-partitioning/no_bootable_gpt_efi boolean false

# enforce usage of GPT - a must have to use EFI!
d-i partman-basicfilesystems/choose_label string gpt
d-i partman-basicfilesystems/default_label string gpt
d-i partman-partitioning/choose_label string gpt
d-i partman-partitioning/default_label string gpt
d-i partman/choose_label string gpt
d-i partman/default_label string gpt

# Keep that one set to true so we end up with a UEFI enabled
# system. If set to false, /var/lib/partman/uefi_ignore will be touched
d-i partman-efi/non_efi_system boolean true


#
# *** Package selection ***
#
# originally from: https://help.ubuntu.com/lts/installation-guide/armhf/apbs04.html#preseed-pkgsel
#

tasksel tasksel/first multiselect standard, ubuntu-server

# Minimum packages (see postinstall.sh). This includes the hyper-v tools
d-i pkgsel/include string openssh-server ntp linux-tools-$(uname -r) linux-cloud-tools-$(uname -r) linux-cloud-tools-common

# Upgrade packages after debootstrap? (none, safe-upgrade, full-upgrade)
# (note: set to none for speed)
d-i pkgsel/upgrade select none

# Policy for applying updates. May be &amp;quot;none&amp;quot; (no automatic updates),
# &amp;quot;unattended-upgrades&amp;quot; (install security updates automatically), or
# &amp;quot;landscape&amp;quot; (manage system with Landscape).
d-i pkgsel/update-policy select none

# Language pack selection
d-i pkgsel/install-language-support boolean false

#
# Boot loader installation
#

# This is fairly safe to set, it makes grub install automatically to the MBR
# if no other operating system is detected on the machine.
d-i grub-installer/only_debian boolean true

# This one makes grub-installer install to the MBR if it also finds some other
# OS, which is less safe as it might not be able to boot that other OS.
d-i grub-installer/with_other_os boolean true

# Alternatively, if you want to install to a location other than the mbr,
# uncomment and edit these lines:
d-i grub-installer/bootdev string /dev/sda
d-i grub-installer/force-efi-extra-removable boolean true


#
# *** Preseed other packages ***
#

d-i debconf debconf/frontend select Noninteractive
d-i finish-install/reboot_in_progress note

choose-mirror-bin mirror/http/proxy string
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The complete preseed file can also be found in the
&lt;a href="https://github.com/ops-resource/ops-tools-baseimage/tree/master/src/linux/ubuntu/http"&gt;http preseed directory&lt;/a&gt;
of the &lt;a href="https://github.com/ops-resource/ops-tools-baseimage"&gt;Ops-Tools-BaseImage&lt;/a&gt; project. This
project also publishes a &lt;a href="https://www.nuget.org/packages/Ops.Tools.BaseImage.Linux/"&gt;NuGet&lt;/a&gt; package
which has all the configuration files and scripts that were used to create the Ubuntu base virtual
hard drive.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Design accuracy</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-accuracy</link>
			<description>&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Accuracy_and_precision#ISO_definition_.28ISO_5725.29"&gt;ISO&lt;/a&gt; defines
accuracy as the combination of &lt;a href="https://dictionary.cambridge.org/dictionary/english/correct?q=correctness"&gt;correctness&lt;/a&gt;,
in agreement with the true facts, and &lt;a href="https://dictionary.cambridge.org/dictionary/english/consistency"&gt;consistency&lt;/a&gt;,
always behaving or performing in a similar way.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-accuracy</guid>
			<pubDate>Mon, 04 Sep 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Accuracy_and_precision#ISO_definition_.28ISO_5725.29"&gt;ISO&lt;/a&gt; defines
accuracy as the combination of &lt;a href="https://dictionary.cambridge.org/dictionary/english/correct?q=correctness"&gt;correctness&lt;/a&gt;,
in agreement with the true facts, and &lt;a href="https://dictionary.cambridge.org/dictionary/english/consistency"&gt;consistency&lt;/a&gt;,
always behaving or performing in a similar way.&lt;/p&gt;
&lt;p&gt;The reason to value &lt;em&gt;accuracy&lt;/em&gt; as the number one characteristic of the development pipeline is
because it is important for the development teams to be able to rely on the outputs of the
pipeline, whether they are product artefacts, test results or output logs. Without the accuracy
the development teams will eventually lose their trust in the development pipeline, meaning that
they will start ignoring the results because the teams assume that a failure is one of the system
instead of one caused by the input set. Once the development teams lose the trust in the
pipeline it will take a lot of work to regain their trust.&lt;/p&gt;
&lt;p&gt;Once we know that having a development pipeline which delivers correct results is important the
next step is to determine how accuracy can be built into the development pipeline.
In theory this task is a simple one, all one has to do is to ensure that all the parts that form the
pipeline behave correctly for all input sets. However as indicated by
&lt;a href="https://wiki.c2.com/?DifferenceBetweenTheoryAndPractice"&gt;many&lt;/a&gt; -&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In theory there is no difference between theory and practice. In practice there is&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;which means that practically achieving accuracy is a difficult task due to the
many, often complex, interactions between the pipeline components. As a reminder
the &lt;a href="/posts/Software-development-pipeline-Design-introduction.html"&gt;components&lt;/a&gt; the
development pipeline consists of are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The scripts that are used during the different parts of the cycle, i.e. the build, test
and release scripts.&lt;/li&gt;
&lt;li&gt;The continuous integration system which is used to execute the different scripts.&lt;/li&gt;
&lt;li&gt;The tools, like the compiler, test frameworks, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on this categorization of the pipeline parts and the previous statement one possible
way of approaching accuracy for a development pipeline is to first ensure that all the parts
are individually accurate. As a second stage the changes in accuracy due to interaction between
the parts can be dealt with.&lt;/p&gt;
&lt;p&gt;For the scripts, tools and continuous integration system this means that each input returns a
correct response and does so consistently for each input set. Fortunately most scripts
and tools do so for the majority of the inputs. In cases where a tool returns an incorrect response
the standard software development process should be followed by recording an issue, scheduling the
issue and implementing, testing and deploying a new version of the tool. In this process
it is important to test thoroughly to ensure that the changes do not negatively impact
tool accuracy. Additionally it is important to execute both (automated) regression testing against known
input sets as well as high level (automated) smoke tests of the entire development pipeline
to validate that the issue has been fixed and no further issues have been introduced.
In order to minimize disruption to the development teams tests should be conducted outside
business hours if no test environment is available, i.e. if the production development
pipeline has to be used for the final tests. It is of course better if a test environment
is available so that testing can take place during business hours without affecting
the development teams. As a side note; having a test environment with a copy of the
development pipeline allows for the development of new features and other changes
while the production pipeline is in use, thus making it easier and quicker to evolve
the pipeline and its capabilities.&lt;/p&gt;
&lt;p&gt;With the approach to development and improvement of the tools taken care of the other area that
needs to be carefully controlled is the infrastructure on top of which the development pipeline
executes. For infrastructure the biggest issues are related to outages of different parts of
the infrastructure, e.g. the network or the different services. In most cases
failures on the infrastructure level do not directly influence the correctness of the development
pipeline. It is obviously possible for a failure in the infrastructure to lead
to an incorrect service being used, e.g. the test package manager instead of the
production one. However unless other issues are present, i.e. the test package manager
has packages of the same version but different content, it is unlikely that
a failure in the infrastructure will allow artefacts to pass the development pipeline
while they should not. A more likely result is that failures in the infrastructure
lead to failures in the development pipeline thus affecting the ability of the
pipeline to deliver the correct results consistently.&lt;/p&gt;
&lt;p&gt;The types of issues mentioned can mostly be prevented by using the modern approaches
to IT operations like &lt;a href="https://en.wikipedia.org/wiki/Software_configuration_management"&gt;configuration management&lt;/a&gt;
and &lt;a href="https://martinfowler.com/bliki/ImmutableServer.html"&gt;immutable servers&lt;/a&gt; to ensure
that the state of the infrastructure is known, &lt;a href="https://en.wikipedia.org/wiki/System_monitor"&gt;monitoring&lt;/a&gt;
to ensure that those responsible for operations are notified of issues and
standard operating procedures and potentially auto-remediation scripts to
quickly resolve issues that arise.&lt;/p&gt;
&lt;p&gt;It should be noted that it is not necessary, though extremely helpful, for the infrastructure
to be robust in order to provide an accurate development pipeline. Tooling can, and
probably should, be adapted to handle and correct for infrastructure failures as
much as possible. However as one expects it is much easier to build a development
pipeline on top of a robust infrastructure.&lt;/p&gt;
&lt;p&gt;The final part of the discussion on the accuracy of the development pipeline deals
with the relation between accuracy and the interaction of the tools and infrastructure.
The main issue with interaction issues is that they are often hard to understand
due to the, potentially large, number of components involved. Additionally certain
interaction issues may only occur under specific circumstances like high load or
specific times of the day / month / year, e.g. daylight savings or on leap days.&lt;/p&gt;
&lt;p&gt;Because of the complexity it is important when building and maintaining a development pipeline
to following the normal development process, i.e. using version control,
(unit) testing, continuous integration, delivery or deployment, work item tracking
and extensive testing etc. for all changes to the pipeline. This applies to the scripts
and tools as well as the &lt;a href="https://en.wikipedia.org/wiki/Infrastructure_as_Code"&gt;infrastructure&lt;/a&gt;.
Additionally it is especially important to execute thorough regression testing for any change to the
pipeline to ensure that a change to a single part does not negatively influence the
correctness of the pipeline.&lt;/p&gt;
&lt;p&gt;Finally in order to ensure that the correctness of the pipeline can be maintained, and improved
if required, it is sensible to provide monitoring of the pipeline and the underlying infrastructure
as well as ensuring that all parts of the pipeline are versioned, tools and infrastructure alike,
and versions of all the parts are tracked for each input set.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>nBuildKit release - V0.10.2</title>
			<link>https://www.petrikvandervelde.nl/posts/nBuildkit-Release-V0102</link>
			<description>&lt;p&gt;Version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.10.2"&gt;0.10.2&lt;/a&gt;
of &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild"&gt;nBuildKit&lt;/a&gt;
has been released.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/nBuildkit-Release-V0102</guid>
			<pubDate>Mon, 28 Aug 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.10.2"&gt;0.10.2&lt;/a&gt;
of &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild"&gt;nBuildKit&lt;/a&gt;
has been released.&lt;/p&gt;
&lt;p&gt;This release made the following minor changes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/316"&gt;Removed&lt;/a&gt; the custom tasks assembly
from the &lt;code&gt;nBuildKit.MsBuild.Actions&lt;/code&gt; NuGet package because those were never used from that package
as they are also in the &lt;code&gt;nBuildKit.MsBuild.Tasks&lt;/code&gt; NuGet package&lt;/li&gt;
&lt;li&gt;Added a deployment step to get &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/318"&gt;VCS information&lt;/a&gt;
during the deploy process.&lt;/li&gt;
&lt;li&gt;Fixed &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/317"&gt;missing bootstrap steps&lt;/a&gt;
in the test and deploy stages.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the work items that have been closed for this release can be found on
GitHub in the milestone list for the &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/milestone/36?closed=1"&gt;0.10.2&lt;/a&gt; release.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Design introduction</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-introduction</link>
			<description>&lt;p&gt;In order to deliver new or improved software applications within the desired
time span while maintaining or improving quality modern development moves towards
&lt;a href="https://techbeacon.com/doing-continuous-delivery-focus-first-reducing-release-cycle-times"&gt;shorter development and delivery&lt;/a&gt;
cycles. This is often achieved through the use of agile processes and a development
workflow including &lt;a href="https://en.wikipedia.org/wiki/Continuous_integration"&gt;Continuous Integration&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/Continuous_delivery"&gt;Continuous Delivery&lt;/a&gt;
or even &lt;a href="https://www.agilealliance.org/glossary/continuous-deployment"&gt;Continuous deployment&lt;/a&gt;.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-Design-introduction</guid>
			<pubDate>Mon, 21 Aug 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In order to deliver new or improved software applications within the desired
time span while maintaining or improving quality modern development moves towards
&lt;a href="https://techbeacon.com/doing-continuous-delivery-focus-first-reducing-release-cycle-times"&gt;shorter development and delivery&lt;/a&gt;
cycles. This is often achieved through the use of agile processes and a development
workflow including &lt;a href="https://en.wikipedia.org/wiki/Continuous_integration"&gt;Continuous Integration&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/Continuous_delivery"&gt;Continuous Delivery&lt;/a&gt;
or even &lt;a href="https://www.agilealliance.org/glossary/continuous-deployment"&gt;Continuous deployment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the consequences of this desire to reduce the development cycle time on the
development process is that more tasks in the development workflow
have to be automated in order to reduce the time taken for the specific task.
One way this automation can be achieved is by creating a
&lt;a href="https://www.informit.com/articles/article.aspx?p=1621865&amp;amp;seqNum=2"&gt;development pipeline&lt;/a&gt; which
takes the source code and moves it through a set of largely automatic transformations, e.g. compilation,
testing, packaging and potentially deployment, to obtain a validated, and potentially deployed, application.&lt;/p&gt;
&lt;p&gt;In order to configure a development pipeline, whether that is on-prem or in the cloud, for
one or more development teams one will have to understand what the requirements are which
are placed on the pipeline, which tooling is available to create the pipeline, whether the
pipeline will be situated on-prem, in the cloud or a combination and
how the pipeline will be assembled and managed. In this post series some of
these issues will be discussed starting with the requirements or considerations that need
to be given to the characteristics or behaviours of the development pipeline.&lt;/p&gt;
&lt;p&gt;Prior to discussing what the considerations are for selecting tooling and infrastructure
for a development pipeline, it is important to decide what elements are part of the pipeline
and which elements are not. For the remainder of this
post series the development pipeline is considered to consist of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The scripts that are used during the different parts of the cycle, i.e. the build, test
and release scripts.&lt;/li&gt;
&lt;li&gt;The continuous integration system which is used to execute the different scripts.&lt;/li&gt;
&lt;li&gt;The tools, like the compiler, test frameworks, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Items like version control, package management, issue tracking, system monitoring,
customer support and others are not included in the discussion. While these systems
are essential in application development and deployment each of these systems spans
a large enough area that it warrants a more thorough discussion than can be given in
this post series.&lt;/p&gt;
&lt;p&gt;Additionally, the following terms will be used throughout the series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input set - A collection of information that is provided to the pipeline to start
the process of turning these generating the desired artefacts. An input set may consist
of source code, e.g. in terms of a given commit to the source control system, files or
documents, configuration values or any other combination of information which is required
to create, validate and deploy the product artefacts.
An input set should contain all the information needed to generate and deploy the product
artefacts and each time a specific input set is provided to the pipeline exactly the
same artefacts will be produced.&lt;/li&gt;
&lt;li&gt;Executor - In general the development pipeline will be driven by a continuous integration
system which itself consists of a controlling unit, which receives the tasks and distributes them,
and a set of executing units which perform the actual computational tasks. In small systems
the controlling unit may also perform the computational tasks, however even in this case
a distinction can be made between the controlling and executing parts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to start the selection of suitable components for a development pipeline
it is important to know what the desirable properties of such a system are. The following
ordered properties are thought to be the most important ones to consider.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/posts/Software-development-pipeline-Design-accuracy.html"&gt;Accurate&lt;/a&gt;: The pipeline must return the right
outputs for a specific input, i.e it should report errors to the interested parties if there are any
and report success and produce the desired artefacts if there are no errors.&lt;/li&gt;
&lt;li&gt;&lt;a href="/posts/Software-development-pipeline-Design-performance.html"&gt;Performance&lt;/a&gt;: The pipeline must push changes
through the different stages fast in order to get results back to the interested parties as soon
as possible.&lt;/li&gt;
&lt;li&gt;&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;Resilience&lt;/a&gt;: The pipeline must be able to
cope with environmental changes, both expected and unexpected.&lt;/li&gt;
&lt;li&gt;&lt;a href="/posts/Software-development-pipeline-Design-flexibility.html"&gt;Flexibility&lt;/a&gt;: The pipeline must be easy to
adapt to new products and tools so that it can be used without having to replace large parts each
time a new product or tool needs to be included.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are of course other desirable properties for a development pipeline like ease-of-use,
maintainability, etc.. It is however considered that the aforementioned properties are the
most critical ones. The linked posts provide additional reasons why each of these properties
are important and how a software development pipeline can be designed to satisfy these important
considerations.&lt;/p&gt;
&lt;p&gt;Additional posts about this topic can be found via the
&lt;a href="/tags/software-development-pipeline.html"&gt;Software development pipeline&lt;/a&gt; tag.&lt;/p&gt;
&lt;h4&gt;Edits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;August 30th 2017: Replaced the term &lt;code&gt;correctness&lt;/code&gt; with the term &lt;code&gt;accuracy&lt;/code&gt; because that
is a better description of the combined concepts of consistency and correctness.&lt;/li&gt;
&lt;li&gt;September 4th 2017: Added the link to the post providing the high level description on
how to achieve accuracy for a development pipeline.&lt;/li&gt;
&lt;li&gt;November 5th 2017: Replaced the term &lt;code&gt;robustness&lt;/code&gt; with the term &lt;code&gt;resilience&lt;/code&gt; because it
captures a broader perspective on the ability of the system to handle expected and unexpected
changes.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>nBuildKit release - V0.10.0</title>
			<link>https://www.petrikvandervelde.nl/posts/nBuildkit-release-V0100</link>
			<description>&lt;p&gt;Version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.10.0"&gt;0.10.0&lt;/a&gt;
and version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.10.1"&gt;0.10.1&lt;/a&gt;
of &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild"&gt;nBuildKit&lt;/a&gt;
have been released. The &lt;code&gt;0.10.0&lt;/code&gt; release is the first release in the stabilization
cycle. There are still a few major changes but the goal is to start stabilizing nBuildKit
in preparation for the 1.0.0 release.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/nBuildkit-release-V0100</guid>
			<pubDate>Sun, 16 Jul 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.10.0"&gt;0.10.0&lt;/a&gt;
and version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.10.1"&gt;0.10.1&lt;/a&gt;
of &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild"&gt;nBuildKit&lt;/a&gt;
have been released. The &lt;code&gt;0.10.0&lt;/code&gt; release is the first release in the stabilization
cycle. There are still a few major changes but the goal is to start stabilizing nBuildKit
in preparation for the 1.0.0 release.&lt;/p&gt;
&lt;p&gt;The highlights are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/308"&gt;Renamed&lt;/a&gt; &lt;code&gt;DirTest&lt;/code&gt; to &lt;code&gt;DirTests&lt;/code&gt;.
This is a &lt;em&gt;breaking&lt;/em&gt; change.&lt;/li&gt;
&lt;li&gt;Additional &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/248"&gt;meta data&lt;/a&gt; can
be provided with the build, test and deploy steps to indicate if a step should be inserted
before or after another step in the sequence. Steps which have no insertion limits, i.e
those steps that do not have the before or after meta data set, are inserted in document order.
Steps that do define insertion limits are inserted according to the limits. Note that
circular limits are not allowed as well as step sequences where there are no steps without
insertion limits.&lt;/li&gt;
&lt;li&gt;Deploying artefacts to an &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/303"&gt;HTTP server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Initial implementation of the &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/296"&gt;ability to verify GPG hash signatures&lt;/a&gt;.
The current implementation requires that the GPG tooling is allowed through the firewall.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to the major changes some additional build, test and deploy steps have also been defined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Addition of properties that indicate &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/304"&gt;which stage the script is currently executing&lt;/a&gt;. The
new properties are &lt;code&gt;IsBuild&lt;/code&gt;, &lt;code&gt;IsTest&lt;/code&gt; and &lt;code&gt;IsDeploy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Allowing &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/306"&gt;template variables&lt;/a&gt; in the
path and &lt;code&gt;destination&lt;/code&gt; of the &lt;code&gt;ArchiveFilesToCopy&lt;/code&gt; item group.&lt;/li&gt;
&lt;li&gt;Migrated the &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/297"&gt;hash calculation functions&lt;/a&gt;
from &lt;a href="https://github.com/ops-resource/ops-tools-build"&gt;Ops-Tools-Build&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the work items that have been closed for this release can be found on
GitHub in the milestone list for releases &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/milestone/30?closed=1"&gt;0.10.0&lt;/a&gt;
and &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/milestone/35?closed=1"&gt;0.10.1&lt;/a&gt;.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Ops-Tools-Build release - V0.3.0</title>
			<link>https://www.petrikvandervelde.nl/posts/Ops-Tools-Build-release-V030</link>
			<description>&lt;p&gt;Version &lt;a href="https://github.com/ops-resource/ops-tools-build/releases/tag/0.3.0"&gt;0.3.0&lt;/a&gt; of the
&lt;a href="https://github.com/ops-resource/ops-tools-build"&gt;Ops-Tools-Build&lt;/a&gt; toolkit was released.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Ops-Tools-Build-release-V030</guid>
			<pubDate>Sun, 16 Jul 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Version &lt;a href="https://github.com/ops-resource/ops-tools-build/releases/tag/0.3.0"&gt;0.3.0&lt;/a&gt; of the
&lt;a href="https://github.com/ops-resource/ops-tools-build"&gt;Ops-Tools-Build&lt;/a&gt; toolkit was released.&lt;/p&gt;
&lt;p&gt;This release was focussed on providing the capabilities to use the Docker toolset to create new
containers and to run validation of VM resources generated by &lt;a href="https://packer.io"&gt;Packer&lt;/a&gt;.
All the work items that have been closed for this release can be found on
&lt;a href="https://github.com/ops-resource/ops-tools-build/milestone/3?closed=1"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;New functionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/33"&gt;33&lt;/a&gt; - Added the ability to
create ISO files during the test stage.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/24"&gt;24&lt;/a&gt; - Added functions to invoke
docker to create container resources.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/32"&gt;32&lt;/a&gt; - Added functions to validate
a &lt;a href="https://packer.io"&gt;Packer&lt;/a&gt; image by executing packer with the previously created image and
executing tests during the packer test build.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>nBuildKit release - V0.9.2</title>
			<link>https://www.petrikvandervelde.nl/posts/nBuildKit-release-V092</link>
			<description>&lt;p&gt;Version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.9.2"&gt;0.9.2&lt;/a&gt;
of &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild"&gt;nBuildKit&lt;/a&gt; has been released. This release fixes
a number of bugs and adds a few new improvements&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/nBuildKit-release-V092</guid>
			<pubDate>Sun, 14 May 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Version &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/releases/tag/0.9.2"&gt;0.9.2&lt;/a&gt;
of &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild"&gt;nBuildKit&lt;/a&gt; has been released. This release fixes
a number of bugs and adds a few new improvements&lt;/p&gt;
&lt;p&gt;All the work items that have been closed for this release can be found on
&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/milestone/31?closed=1"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Bugs fixed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/280"&gt;280&lt;/a&gt; - The &lt;code&gt;deploy.pushto.gitbranch&lt;/code&gt;
script called the &lt;code&gt;GitCurrentBranch&lt;/code&gt; task with a &lt;code&gt;WorkingDirectory&lt;/code&gt; property which did not exist,
thus causing the pushing to a git branch deployment step to fail.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/281"&gt;281&lt;/a&gt; - The &lt;code&gt;deploy.pushto.gitbranch&lt;/code&gt;
script did not create the directory into which the repository should be cloned. This caused the
step to fail.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/286"&gt;286&lt;/a&gt; - The &lt;code&gt;GenerateTargetsFile&lt;/code&gt; custom
task applied a custom assembly resolver which did not resolve references to the MsBuild assemblies.
This is a problem for MsBuild 15.0 and higher because from that version onwards the MsBuild assemblies,
e.g. &lt;code&gt;Microsoft.Build.XX&lt;/code&gt; are no longer located in the GAC and can thus not be found by the standard
assembly resolver.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/287"&gt;287&lt;/a&gt; - The &lt;code&gt;shared.prepare.copy.files&lt;/code&gt;
task did not correctly copy the directory hierarchy.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/290"&gt;290&lt;/a&gt; - The &lt;code&gt;build.prepare.generatefiles&lt;/code&gt;
script only generated the files if they did not exist. This has been changed so that generated
files are always created and updated.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Improvements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/254"&gt;254&lt;/a&gt; - All warnings and errors have been
given a proper error code for easier parsing of errors from the log.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/282"&gt;282&lt;/a&gt; - The Git, Powershell and NuGet
base tasks have been moved to the Core targets assembly so that they can be used in third party
custom tasks as well.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/284"&gt;284&lt;/a&gt; - A new task has been added that
allows extracting files from one or more ZIP archive files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/285"&gt;285&lt;/a&gt; - Additional conditions have been
added to all &lt;code&gt;ItemGroup&lt;/code&gt; elements to ensure that these item groups are only loaded when they are
required. This reduces build times and improves stability.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/issues/288"&gt;288&lt;/a&gt; - A new task has been added that
allows downloading files from a HTTP URL. If the file in question is an archive file it can
additionally be unzipped before the final copy takes place.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally the first steps to better documentation have been taken. The &lt;a href="https://github.com/nbuildkit/nBuildKit.MsBuild/blob/master/README.md"&gt;readme&lt;/a&gt;,
the &lt;a href="https://nbuildkit.github.io/nBuildKit.MsBuild/"&gt;introduction&lt;/a&gt; and the &lt;a href="https://nbuildkit.github.io/nBuildKit.MsBuild/docs/contributing"&gt;contributing&lt;/a&gt;
documents have been updated.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Ops-Tools-Build release - V0.2.0</title>
			<link>https://www.petrikvandervelde.nl/posts/Ops-Tools-Build-release-V020</link>
			<description>&lt;p&gt;Version &lt;a href="https://github.com/ops-resource/ops-tools-build/releases/tag/0.2.0"&gt;0.2.0&lt;/a&gt; of the
&lt;a href="https://github.com/ops-resource/ops-tools-build"&gt;Ops-Tools-Build&lt;/a&gt; toolkit was released.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Ops-Tools-Build-release-V020</guid>
			<pubDate>Sun, 14 May 2017 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Version &lt;a href="https://github.com/ops-resource/ops-tools-build/releases/tag/0.2.0"&gt;0.2.0&lt;/a&gt; of the
&lt;a href="https://github.com/ops-resource/ops-tools-build"&gt;Ops-Tools-Build&lt;/a&gt; toolkit was released.&lt;/p&gt;
&lt;p&gt;This release was focussed on providing the capabilities to use the Chef toolset to create new
resources. All the work items that have been closed for this release can be found on
&lt;a href="https://github.com/ops-resource/ops-tools-build/milestone/2?closed=1"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;New functionality&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/5"&gt;5&lt;/a&gt; - A new function which provides
the ability to execute &lt;a href="https://github.com/sethvargo/chefspec"&gt;ChefSpec&lt;/a&gt; tests.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/6"&gt;6&lt;/a&gt; - A new function which
invokes the Chef linting tool &lt;a href="https://www.foodcritic.io/"&gt;Foodcritic&lt;/a&gt; on the selected Chef cookbooks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/7"&gt;7&lt;/a&gt; - A new function which invokes the
Ruby linting tool &lt;a href="https://github.com/bbatsov/rubocop"&gt;RuboCop&lt;/a&gt; on selected Ruby source files.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/20"&gt;20&lt;/a&gt; - A new package restore script
was added to allow restoring Chef cookbooks via &lt;a href="https://github.com/berkshelf/berkshelf"&gt;Berkshelf&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ops-resource/ops-tools-build/issues/21"&gt;21&lt;/a&gt; - A new switch was added to the
packer function which allows keeping the resources generated with packer in case the packer
build fails. This improves the ability to debug issues in the packer build.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
	</channel>
</rss>