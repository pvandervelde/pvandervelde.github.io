<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
	<channel>
		<title>Mind vortex</title>
		<link>https://www.petrikvandervelde.nl/</link>
		<description>Welcome!</description>
		<copyright>2024</copyright>
		<pubDate>Thu, 22 Feb 2024 09:15:57 GMT</pubDate>
		<lastBuildDate>Thu, 22 Feb 2024 09:15:57 GMT</lastBuildDate>
		<item>
			<title>Swerve drive - Motor limitations</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-drive-motor-limitations</link>
			<description>&lt;p&gt;The &lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;swerve controller&lt;/a&gt; I have implemented so far assumes
that the robot is able to follow all the movement commands it has been given. This can lead to extreme
velocity and acceleration for the drive and steering motors. The image below shows the result of a
simulation where the robot is commanded to move in a straight line followed by a rotate in-place. The
simulation used a s-curve &lt;a href="/posts/Swerve-motion-profiles"&gt;motion profile&lt;/a&gt; to generate a smooth body
movement between different states. The graphs show that the drive velocity and acceleration are
smooth and don't reach very high values. It is thus expected that these are well within the
capabilities of the drive motors. The steering velocity and acceleration show significant peaks which
are likely to tax the steering motors.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-drive-motor-limitations</guid>
			<pubDate>Thu, 22 Feb 2024 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The &lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;swerve controller&lt;/a&gt; I have implemented so far assumes
that the robot is able to follow all the movement commands it has been given. This can lead to extreme
velocity and acceleration for the drive and steering motors. The image below shows the result of a
simulation where the robot is commanded to move in a straight line followed by a rotate in-place. The
simulation used a s-curve &lt;a href="/posts/Swerve-motion-profiles"&gt;motion profile&lt;/a&gt; to generate a smooth body
movement between different states. The graphs show that the drive velocity and acceleration are
smooth and don't reach very high values. It is thus expected that these are well within the
capabilities of the drive motors. The steering velocity and acceleration show significant peaks which
are likely to tax the steering motors.&lt;/p&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/control/inplace_rotation_from_0_fwd_unlimited.png" target="_blank"&gt;
    &lt;img
        alt="The positions, velocities and accelerations of the robot and the drive modules as it moves in a straight line and then rotates in place with no limiters applied."
        src="/assets/images/robotics/control/inplace_rotation_from_0_fwd_unlimited.png"
        width="416"
        height="200"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    The positions, velocities and accelerations of the robot and the drive modules as it moves in a
    straight line and then rotates in place. No limitations were applied to the steering and drive
    velocities and accelerations.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In order to ensure that the motors are able to follow the movement commands while keeping the motions
of the different drive modules &lt;a href="/posts/Swerve-drive-body-focussed-control"&gt;synchronised&lt;/a&gt;, we need to
take into account the capabilities of the motors.&lt;/p&gt;
&lt;p&gt;Now there are many different motor characteristics that we could take into account. For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The maximum velocity the motor can achieve. This obviously limits how fast the drive module can steer
or rotate the wheels.&lt;/li&gt;
&lt;li&gt;The maximum acceleration the motor can achieve. This limits how fast the drive module can change the
drive or steering velocity and thus how quickly it can respond to command changes.&lt;/li&gt;
&lt;li&gt;The maximum jerk the motor can achieve. This limits how quickly the motor can achieve the desired
acceleration.&lt;/li&gt;
&lt;li&gt;The existence of any motor &lt;a href="https://en.wikipedia.org/wiki/Deadband"&gt;deadband&lt;/a&gt;. This is the region
around zero rotation speed where the motor doesn't have enough torque to overcome the static
friction of the motor and attached systems. Once enough torque is applied the motor will start
running at the rotation speed that it would normally have for that amount of torque. This means that
there is a minimum rotation speed that the motor can achieve.&lt;/li&gt;
&lt;li&gt;The behaviour of the motor under load. For instance the motor may not be able to reach the desired
rotation speed under load.&lt;/li&gt;
&lt;li&gt;The motor settling time, which is the time it takes the motor to reach the commanded speed.&lt;/li&gt;
&lt;li&gt;The behaviour of the motor when running &amp;lsquo;forwards&amp;rsquo; versus when running 'backwards'. For
instance the motor may have a different maximum velocity in the &amp;lsquo;forward&amp;rsquo; direction than it does
in the 'backwards' direction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the moment I will only be looking at the maximum motor velocities and the accelerations. These two
have a direct effect on the synchronisation between the drive modules. The other characteristics will
be dealt at a later stage as they require more information and thought.&lt;/p&gt;
&lt;p&gt;In order to limit the steering and drive velocities I've implemented the following process&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/d6c8349b7f184c85ac77fa8b19298bb79e22cebf/swerve_controller/control_profile.py#L612"&gt;Determine&lt;/a&gt;
the body movement profile that allows the body to achieve the desired movement state in
a smooth manner using &lt;a href="/posts/Swerve-motion-profiles"&gt;s-curve motion profiles&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Divide the body movement profiles into N+1 points, dividing the profile into N sections of equal
time. For each of these points
&lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/d6c8349b7f184c85ac77fa8b19298bb79e22cebf/swerve_controller/control_profile.py#L635"&gt;calculate the velocity and steering angle&lt;/a&gt;
for the drive modules. These velocities and steering angles then form the motion profiles for each
of the drive modules.&lt;/li&gt;
&lt;li&gt;For each point in time check the steering velocity for each module. Record the maximum velocity of
all the profiles. If the maximum velocity is larger than what the motor can deliver then
&lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/d6c8349b7f184c85ac77fa8b19298bb79e22cebf/swerve_controller/control_profile.py#L414"&gt;increase or decrease the current timestep&lt;/a&gt;
in order to limit the steering velocity to the maximum velocity of the motor. Changing the
duration of the timestep will change the steering velocities of all the modules. Thus we limit
the steering velocities while maintaining synchronisation between the modules.&lt;/li&gt;
&lt;li&gt;Repeat the previous step for the &lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/d6c8349b7f184c85ac77fa8b19298bb79e22cebf/swerve_controller/control_profile.py#L546"&gt;drive velocities&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following video shows the result of the simulation using the new velocity limiter code. The video
shows the robot moving in a straight line and then rotating in place. When the rotation starts the
steering velocity for the different drive modules increases. As the steering velocity reaches 2 rad/s
for the left front and left rear modules the limiter kicks in and maintains that steering velocity.
In response the steering velocities of the other wheels reduces in order to keep the drive modules
synchronised. The wheel velocities are slightly altered.&lt;/p&gt;
&lt;iframe
    style="float:none"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/H7HTa4b6f_0"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;With the steering and wheel velocities limited in a reasonable way I attempted to do the same for the
steering acceleration. The results of this can be seen in the next images. The first image limits the
steering acceleration to 15 rad/s^2. The second image limits the steering acceleration to 10 rad/s^2.&lt;/p&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/control/inplace_rotation_from_0_fwd_acc_limited_15_rad_s2.png" target="_blank"&gt;
    &lt;img
        alt="The positions, velocities and accelerations of the robot and the drive modules as it moves in a straight line and then rotates in place. Limits are applied to the velocities and the steering acceleration."
        src="/assets/images/robotics/control/inplace_rotation_from_0_fwd_acc_limited_15_rad_s2.png"
        width="416"
        height="200"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    The positions, velocities and accelerations of the robot and the drive modules as it moves in a
    straight line and then rotates in place. The steering velocity is limited to 2 rad/s, the drive
    velocity is limited to 1.0 m/s and the steering acceleration is limited to 15 rad/s^2.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/control/inplace_rotation_from_0_fwd_acc_limited_10_rad_s2.png" target="_blank"&gt;
    &lt;img
        alt="The positions, velocities and accelerations of the robot and the drive modules as it moves in a straight line and then rotates in place. Limits are applied to the velocities and the steering acceleration."
        src="/assets/images/robotics/control/inplace_rotation_from_0_fwd_acc_limited_10_rad_s2.png"
        width="416"
        height="200"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    The positions, velocities and accelerations of the robot and the drive modules as it moves in a
    straight line and then rotates in place. The steering velocity is limited to 2 rad/s, the drive
    velocity is limited to 1.0 m/s and the steering acceleration is limited to 10 rad/s^2.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;When comparing the two simulation results it becomes clear that something happens if we limit the
steering acceleration to 10 rad/s^2. The velocities over all for both cases looks reasonable, except
for the 10 rad/s^2 case where the steering velocity snaps to zero at the end of the transition from
the straight line to the rotation. This is reflected in a momentary acceleration of over 100 rad/s^2.&lt;/p&gt;
&lt;p&gt;The acceleration is calculated based on the difference between the velocity at the current timestep
and the velocity at the previous timestep. Additionally the algorithm has to ensure that the steering
angle change between the previous and current timestep is controlled so that synchronisation of the
drive modules is maintained. Generally to limit the velocity the duration of the timestep is increased.
However for the acceleration, especially when decelerating from a positive velocity, there is a limit
to how much the timestep can be increased. A larger time step increase decreases the velocity. This
then increases the deceleration needed. So this means that you get very large timesteps, or very small
timesteps. The current algorithm favours small timesteps.&lt;/p&gt;
&lt;p&gt;In either case the problem occurs when slowing down. At the end of the deceleration profile the desired
position is achieved, but the velocity and acceleration are not zero. This is not realistic or physically
possible. This behaviour is possibly due to the fact that the timesteps are individually calculated.
This means that the algorithm doesn't know what the desired end states are and so those are not taken
into account.&lt;/p&gt;
&lt;p&gt;Additionally if we look at this approach from a higher level we can see that there are two potential
areas where improvements can be made in the control model. The first area is related to the fact that
the steering velocity and acceleration are computed values, i.e. they are derived from the change in
the steering position. This means that there is no direct control over the steering velocity and
acceleration. A better algorithm would include these values directly and be able to apply boundary
conditions on these values. This could be achieved by extending the current kinematic model with the
body accelerations and &lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)"&gt;jerks&lt;/a&gt;.
The second area is related tot the fact that the module states are derived from the body state. This
is necessary to keep the modules synchronised with the body. However this means that the module states
are not directly controlled. Which means that it is more difficult to include the module limitations
in the control model.&lt;/p&gt;
&lt;p&gt;These issues will only become more pronounced if we want to start limiting the maximum jerk
values for the steering and drive directions. Limiting the maximum jerk is required to prevent excessive
forces on the drive module components.&lt;/p&gt;
&lt;p&gt;One possible solution to these issues is to switch to a dynamic model for the control of the robot,
i.e. one that is based on the forces and accelerations observed by the robot as it moves.
The current model is a kinematic model, i.e. based on the positions and velocities of the different
components. Using a dynamic model would allow for the inclusion of the body accelerations and jerks.
Additionally the dynamic model would allow for the inclusion of the behaviour of the steering modules
more directly and thus lead to a more accurate control model. Finally a dynamic model would also allow
for the inclusion of skid and slip conditions as well as three dimensional movement on uneven terrain.
The drawback of a dynamic model is that it is more complex and thus requires more effort during
implementation and at run time.&lt;/p&gt;
&lt;p&gt;At the moment I have not made any decisions on how to progress the swerve controller. It seems
logical to implement a dynamic model, however which model should be used and how to implement it
requires a bit more research. For the time being I'm planning my next step to be to design and
build a single drive module in real life.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Swerve drive - Using Nav2</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-drive-ros2-nav</link>
			<description>&lt;p&gt;The &lt;a href="/posts/Swerve-drive-introduction"&gt;goals for the development of my swerve drive robot&lt;/a&gt; were to
develop an off-road capable robot that can navigate autonomously between different locations to
execute tasks either by itself or in cooperation with other robots. So far I have implemented the
first version of the &lt;a href="/posts/Swerve-drive-body-focussed-control"&gt;steering and drive controller&lt;/a&gt;,
added &lt;a href="/posts/Swerve-motion-profiles"&gt;motion profiles&lt;/a&gt; for smoother changes in velocity and acceleration,
and I have created the &lt;a href="/posts/Robotics-making-urdf-models"&gt;URDF files&lt;/a&gt; that allow me to simulate the
robot in Gazebo. The next part is to add the ability to navigate the robot, autonomously,
to a goal location.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-drive-ros2-nav</guid>
			<pubDate>Wed, 17 Jan 2024 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The &lt;a href="/posts/Swerve-drive-introduction"&gt;goals for the development of my swerve drive robot&lt;/a&gt; were to
develop an off-road capable robot that can navigate autonomously between different locations to
execute tasks either by itself or in cooperation with other robots. So far I have implemented the
first version of the &lt;a href="/posts/Swerve-drive-body-focussed-control"&gt;steering and drive controller&lt;/a&gt;,
added &lt;a href="/posts/Swerve-motion-profiles"&gt;motion profiles&lt;/a&gt; for smoother changes in velocity and acceleration,
and I have created the &lt;a href="/posts/Robotics-making-urdf-models"&gt;URDF files&lt;/a&gt; that allow me to simulate the
robot in Gazebo. The next part is to add the ability to navigate the robot, autonomously,
to a goal location.&lt;/p&gt;
&lt;p&gt;So how does the navigation in general work? The first step is to figure out where the robot is, or
at least what is around the robot. Using the robot sensors, e.g. a lidar, we can see what the direct
environment looks like, e.g. the robot is in a room of a building. If we have a map of the larger
environment, e.g. the building, then using a &lt;a href="https://github.com/ros-planning/navigation2/tree/main/nav2_amcl"&gt;location algorithm&lt;/a&gt;
we can figure out where in the building we are. Assuming there are enough features in the
room to make it obvious which room we are in. If we don't have a map of the environment then we can
use a &lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;SLAM&lt;/a&gt; algorithm to create
a map while the robot is moving. Having a map makes planning more reliable, because we can plan a
path that takes into account the surroundings. However it is not strictly necessary, we can also
plan a path without a map, but then it is possible that the planner will plan a path that is not
possible to follow, e.g. because there is a wall in the way. Of course if there are dynamic obstacles,
e.g. if a door is closed, then the map might not be accurate and we might still not be able to get
to the goal.&lt;/p&gt;
&lt;p&gt;Once the robot knows where it is and it has a goal location, it can plan a path from the current
location to the goal location. This is generally done by a &lt;a href="https://navigation.ros.org/concepts/index.html#planners"&gt;global planner or planner for short&lt;/a&gt;.
There are many different algorithms for path planning each with their own advantages and disadvantages.
Most planners only plan a path, consisting of a number of waypoints, from the current location to
the goal location. More advanced planners can also provide velocity information along the path,
turning the path into a trajectory for the robot to follow. Once a path, or trajectory, is created
the robot can follow this path to reach the goal. For this a &lt;a href="https://navigation.ros.org/concepts/index.html#controllers"&gt;local planner or controller&lt;/a&gt;
is used. The controller works to make the robot follow the path while making progress towards
the goal and avoiding obstacles. Again there are a number of different algorithms, each with it own
constraints.&lt;/p&gt;
&lt;p&gt;Note that robot navigation is a very complex topic that is very actively being researched. A result
of this is that there are many ways in which navigation can fail or not work as expected. For example
some of the issues I have seen are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The planner is unable to find a path to the goal. This can happen when there is no actual way to
get to the goal, e.g. the robot is in a room with a closed door with the goal outside the room.
Or when the path would pass through a section that is too narrow for the robot to fit through.&lt;/li&gt;
&lt;li&gt;The controller is unable to follow path created by the planner, most likely because the path is not
kinematically possible for the robot. For example the path might require the robot to turn in place
when it is using an Ackerman steering system. Or the path requires the robot to pass through a
narrow space that the controller deems to narrow.&lt;/li&gt;
&lt;li&gt;The robot gets stuck in a corner or hallway. It considers the hallway too narrow to fit
through safely or has no way to perform the directional changes it wants to make. Sometimes this is
caused by the robot being too wide. However most of the cases I have seen it in is caused by the
planner configuration.&lt;/li&gt;
&lt;li&gt;The robot is slow to respond to movement commands, leading to ever larger commanded steering and velocities.
This was mostly an issue with the DWB controller, the MPPI controller seems to be more responsive.
However there is definitely an issue with the swerve controller code that I need to investigate.&lt;/li&gt;
&lt;li&gt;The VM I was running the simulation on wasn't quick enough to run the simulation close to real time.
So at some point I need to switch to running on a desktop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And these are only the ones I have seen. There are many more ways in which the navigation can fail.
So it pays to stay up to date with the latest developments and to ensure that you have good ways
to test and debug your robot and the navigation stack.&lt;/p&gt;
&lt;p&gt;Now that we know roughly what is needed for successful navigation let's have a look at what is
needed for the swerve drive robot. The first thing we need is a way to figure out where we are. Because
I don't have a map of the environment I am using an online SLAM approach, which creates the map
as the robot moves around the area. The &lt;a href="https://github.com/SteveMacenski/slam_toolbox"&gt;slam_toolbox&lt;/a&gt;
package makes this easy. The  &lt;a href="https://github.com/pvandervelde/zinger_nav/blob/master/config/slam.yaml"&gt;configuration&lt;/a&gt;
for the robot is mostly the default configuration, except for the changes required to match the URDF
model.&lt;/p&gt;
&lt;p&gt;For the navigation I am using the &lt;a href="https://navigation.ros.org/"&gt;Nav2&lt;/a&gt; package. From that package I
selected the &lt;a href="https://github.com/ros-planning/navigation2/tree/main/nav2_smac_planner"&gt;SMAC lattice&lt;/a&gt;
for the planner and the &lt;a href="https://github.com/ros-planning/navigation2/tree/main/nav2_mppi_controller"&gt;MPPI&lt;/a&gt;
controller. These two were selected because they both support omnidirectional movement for non-circular,
arbitrary shaped robots. The omnidirectional movement is required because the swerve drive robot
can move in all directions and it would be a waste not to use that capability. The non-circular robot
comes from the fact that the robot is rectangular and could pass through narrow passages in one
orientation but not the other. Originally I tried the
&lt;a href="https://github.com/ros-planning/navigation2/tree/main/nav2_dwb_controller"&gt;DWB controller&lt;/a&gt;
and the &lt;a href="https://github.com/ros-planning/navigation2/tree/main/nav2_navfn_planner"&gt;navfn planner&lt;/a&gt;.
Both are said to support omnidirectional movement. But when applied the DWB planner didn't really
use the omnidirectional capabilities. Additionally it gets stuck for certain movements for unknown
reasons. Once I changed to using the SMAC planner and the MPPI controller the robot was successfully
able to navigate around the environment.
Again the &lt;a href="https://github.com/pvandervelde/zinger_nav/blob/master/config/nav2.yaml"&gt;configuration&lt;/a&gt;
is mostly the default configuration except that I have updated some of the values to match the robot's
capabilities.&lt;/p&gt;
&lt;iframe
    style="float:none"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/U2IOLwuCvrQ"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;With all that set I got the robot to navigate to a goal. The video shows the robot starting the navigation
from one room to another. As it moves at the &lt;a href="https://youtu.be/U2IOLwuCvrQ?t=15"&gt;15 second&lt;/a&gt; mark it
starts sideways translating while rotating to get around the corner in order to get through the door
of the room. It continues its journey mostly traversing sideways, occasionally rotating into the
direction it is moving. At the &lt;a href="https://youtu.be/U2IOLwuCvrQ?t=52"&gt;52 second&lt;/a&gt;
mark it reaches the goal location and rotates into the orientation it was commanded to
end up in. Note that the planner inserted a turn, stop and back-up segment to get to the right
orientation but the local planner opted to perform an in-place rotation at the end.&lt;/p&gt;
&lt;p&gt;While there was a bit of tuning required to get the SLAM and navigation stacks to work, in the
end it worked well. Obviously this is only one test case so once I have a dedicated PC to run
the simulation I am going to do a lot more testing.&lt;/p&gt;
&lt;p&gt;One of the things that is currently not implemented is limits on the steering and drive velocities.
This means that currently the robot can move at any speed and turn at any rate. This is not
realistic and will in the real world lead to the motors in the robot being overloaded. So the
next step is to add the limits to the controller. The main issue with this is the need to keep the
drive modules synchronised. So when one of the modules exceeds its velocity limits the other modules
have to be slowed down as well so that we don't lose synchronisation between the modules. More on this
in one of my next posts.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Robotics - Making URDF models</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-making-urdf-models</link>
			<description>&lt;p&gt;In a &lt;a href="/posts/Robotics-driving-scuttle-with-ros-gazebo-simulation"&gt;previous post&lt;/a&gt; I talked about simulating
the &lt;a href="https://scuttlerobot.org"&gt;SCUTTLE robot&lt;/a&gt; using ROS and Gazebo.  I used Gazebo to
simulate the SCUTTLE robot so that I could learn more about ROS without needing to involve a real
robot with all the setup and complications that come with that. Additionally when I was designing the
&lt;a href="/posts/Robotics-a-bumper-for-scuttle-overview"&gt;bump sensor&lt;/a&gt; for SCUTTLE using the simulation allowed
me to reduce the feedback time compared to testing the design on a physical robot. This speeds up
the design iteration process and allowed me to quickly and verify the design and the code. In the end
you always need to do the final testing on a real robot, but by using simulation you can quickly
iterate to a solution that will most likely work without major issues.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-making-urdf-models</guid>
			<pubDate>Sat, 23 Dec 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In a &lt;a href="/posts/Robotics-driving-scuttle-with-ros-gazebo-simulation"&gt;previous post&lt;/a&gt; I talked about simulating
the &lt;a href="https://scuttlerobot.org"&gt;SCUTTLE robot&lt;/a&gt; using ROS and Gazebo.  I used Gazebo to
simulate the SCUTTLE robot so that I could learn more about ROS without needing to involve a real
robot with all the setup and complications that come with that. Additionally when I was designing the
&lt;a href="/posts/Robotics-a-bumper-for-scuttle-overview"&gt;bump sensor&lt;/a&gt; for SCUTTLE using the simulation allowed
me to reduce the feedback time compared to testing the design on a physical robot. This speeds up
the design iteration process and allowed me to quickly and verify the design and the code. In the end
you always need to do the final testing on a real robot, but by using simulation you can quickly
iterate to a solution that will most likely work without major issues.&lt;/p&gt;
&lt;p&gt;In order to progress my &lt;a href="/posts/Swerve-drive-introduction"&gt;swerve drive robot&lt;/a&gt; I wanted to verify that
the control algorithms that I had developed would work for an actual robot. Ideally before spending
money on the hardware. So I decided to use Gazebo to run some simulations that would enable me to
verify the control algorithm.&lt;/p&gt;
&lt;p&gt;The first step in using Gazebo is to create a model of the robot and its surrounding environment.
Gazebo natively uses the &lt;a href="http://sdformat.org/"&gt;SDF&lt;/a&gt; format to define both the robot and the
environment. However if you want ROS nodes to be able to understand the geometry definition of your
robot you need to use the &lt;a href="http://wiki.ros.org/urdf"&gt;URDF&lt;/a&gt; format. This is important for instance
if you want to make use of the &lt;a href="https://docs.ros.org/en/humble/Concepts/Intermediate/About-Tf2.html"&gt;tf2&lt;/a&gt;
library to transform between different coordinate frames then your ROS nodes will have to have the
&lt;a href="https://github.com/ros/robot_state_publisher/tree/humble"&gt;geometry of the robot&lt;/a&gt; available.&lt;/p&gt;
&lt;p&gt;There are different ways to create an URDF model. One way is to draw the robot in a CAD program
and then use a plugin to export the model to an URDF file. This is a relatively quick approach, it
takes time to create the robot in the CAD program but then the export is very quick. The drawback
of this method is that the resulting URDF file is not very clean which makes it harder to edit
later on. Additionally the CAD software will use meshes for both the visual geometry and the collision
geometry. As discussed below this can lead to issues with the collision detection.
Another way is to create the URDF manually. This approach is slower than the CAD export approach,
however it results in a much more minimal and clean URDF file. This makes editing the file at a
later stage a lot easier. Additionally manually creating the URDF file give you a better understanding
of the URDF format and how it works. In the end the model for my swerve drive robot is very simple,
consisting of a body, four wheels, the steering and drive controllers and the sensors. So I decided
to create the URDF file manually. The images below show the resulting model in RViz. The model consists
of the robot body in orange, the four drive modules in blue and black, and the lidar unit in red.&lt;/p&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/swerve/rviz_zinger_side_view.png" target="_blank"&gt;
    &lt;img
        alt="The Zinger swerve robot as seen from the side in RViz."
        src="/assets/images/robotics/swerve/rviz_zinger_side_view.png"
        width="375"
        height="186"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    The Zinger swerve robot as seen from the side in RViz.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure style="float:right"&gt;
  &lt;a href="/assets/images/robotics/swerve/rviz_zinger_front_top_view.png" target="_blank"&gt;
    &lt;img
        alt="The Zinger swerve robot as seen from the front and top in RViz."
        src="/assets/images/robotics/swerve/rviz_zinger_front_top_view.png"
        width="375"
        height="260"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    The Zinger swerve robot as seen from the front and top in RViz.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The URDF model describes the different parts of the robot relative to each other. The robot parts
consist of the physical parts of the robot, the sensors and the actuators. The physical parts, e.g.
the wheels or the body, are described using &lt;a href="https://wiki.ros.org/urdf/XML/link"&gt;links&lt;/a&gt;. The links
are connected to each other using &lt;a href="https://wiki.ros.org/urdf/XML/joint"&gt;joints&lt;/a&gt;. Joints allow the
links to move relative to each other. Joints can be fixed, e.g. when two structural parts are statically
connected to each other, or they can be movable. If a joint is movable then be one of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A revolute joint, which allows the two links to rotate relative to each other around a single axis.&lt;/li&gt;
&lt;li&gt;A continuous joint, which is similar to a revolute joint but allows the joint to rotate continuously
without any limits.&lt;/li&gt;
&lt;li&gt;A prismatic joint, which allows the two links to move relative to each other along a single axis.&lt;/li&gt;
&lt;li&gt;A floating joint, which allows the two links to move relative to each other in 6 degrees of freedom.&lt;/li&gt;
&lt;li&gt;A planar joint, which allows the two links to move relative to each other in 3 degrees of freedom
in a plane.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the URDF file each link defines the visual geometry, the collision geometry and the inertial
properties for that link. The visual and collision geometry can be defined either by a primitive
(box, cylinder or sphere) or a triangle mesh. It is recommended to use simple primitives for the
collision geometry since these make the collision calculations faster and more stable. If you use a
triangle mesh for the collision geometry then you can get issues with the collision detection. This
can lead to the robot moving around when it shouldn't. For instance when the wheel of the robot is
modelled as a triangle mesh then the collision calculation between the wheel and the ground may not
be numerically stable which leads to undesired movement of the robot. The current version of the SCUTTLE
URDF has this problem as is shown in the video below. In a future post I'll describe how to fix this.&lt;/p&gt;
&lt;iframe
    style="float:left"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/kyMvlBAQGoE"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Using meshes for the visual geometry is fine as this geometry is only used for the visual
representation of the robot. Note that the meshes should be relatively simple as well since Gazebo
will have to render the meshes in real time which is expensive if the mesh consists of a large number
of vertices and triangles.&lt;/p&gt;
&lt;p&gt;I normally start by defining a link that represents the &lt;a href="https://www.ros.org/reps/rep-0120.html#base-footprint"&gt;footprint&lt;/a&gt;
of the robot on the ground, called &lt;code&gt;base_footprint&lt;/code&gt;. This link doesn't define any geometry, it is
only used to define the origin of the robot. When navigation is configured it will be referencing
this footprint link. The next link that is defined is the &lt;code&gt;base_link&lt;/code&gt;. This link is generally defined
to be the &amp;lsquo;middle&amp;rsquo; of the robot frame and it forms the parent for all the other robot parts.&lt;/p&gt;
&lt;p&gt;The next step is to define the rest of the links and joints. For the swerve drive robot I have four
drive modules, each consisting of a wheel and a steering motor. The wheel is connected to the steering
motor using a continuous joint. The steering motor is connected to the base link using another continuous
joint. Because the four modules are all the same I use the &lt;a href="http://wiki.ros.org/xacro"&gt;xacro&lt;/a&gt; format.
This allows me to define &lt;a href="https://github.com/pvandervelde/zinger_description/blob/bb24b884f8bcc62c9c2e8f12bac431f4b62dea6f/urdf/macros.xacro"&gt;a single module as a template&lt;/a&gt;
and then use that template to create the four modules. The benefit of this is that it makes the URDF
file easier to edit and a lot smaller and easier to read.&lt;/p&gt;
&lt;p&gt;To add sensors you need to add two pieces of information, the information about the sensor body and,
if you are using gazebo, the information about the sensors behaviour. The former consist of the
visual and collision geometry of the sensor. The latter consists of the sensor plugin that is used
to simulate the sensor. For the swerve drive robot I have two sensors, a lidar and an IMU. The
lidar is used to provide a point cloud for the different &lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;SLAM&lt;/a&gt;
algorithms which allow the robot to determine where it is and what the environment looks like. The
&lt;a href="https://github.com/pvandervelde/zinger_description/blob/bb24b884f8bcc62c9c2e8f12bac431f4b62dea6f/urdf/gazebo.xacro#L90"&gt;Gazebo configuration&lt;/a&gt;
is based on the use of a &lt;a href="https://www.slamtec.com/en/Lidar/A1"&gt;rplidar&lt;/a&gt; sensor. The IMU is experimental
and currently not used in any of the control algorithms. The
&lt;a href="https://github.com/pvandervelde/zinger_description/blob/bb24b884f8bcc62c9c2e8f12bac431f4b62dea6f/urdf/gazebo.xacro#L26"&gt;Gazebo configuration&lt;/a&gt;
is based on a generic IMU.&lt;/p&gt;
&lt;p&gt;The last part of the URDF file is the definition of the &lt;a href="https://ros-controls.github.io/control.ros.org/"&gt;ROS2 control&lt;/a&gt;
interface. This interface allows you to control the different joints of the robot using ROS2 topics.
The ROS2 control configuration consists of the &lt;a href="https://github.com/pvandervelde/zinger_description/blob/bb24b884f8bcc62c9c2e8f12bac431f4b62dea6f/urdf/base.xacro#L205"&gt;general configuration&lt;/a&gt;
and the configuration specific to &lt;a href="https://github.com/pvandervelde/zinger_description/blob/bb24b884f8bcc62c9c2e8f12bac431f4b62dea6f/urdf/gazebo.xacro#L148"&gt;Gazebo&lt;/a&gt;.
It should be noted that the Gazebo specific configuration depends on linking to the correct control
plugin for Gazebo. In my case, using &lt;a href="https://docs.ros.org/en/humble/"&gt;ROS2 Humble&lt;/a&gt;,
and &lt;a href="https://gazebosim.org/docs/fortress/install"&gt;Gazebo Ignition Fortress&lt;/a&gt;, I need to use the
&lt;code&gt;ign_ros2_control-system&lt;/code&gt; plugin with the &lt;code&gt;ign_ros2_control::IgnitionROS2ControlPlugin&lt;/code&gt; entrypoint.
For other versions of ROS2 and Gazebo you may need to use a different plugin.&lt;/p&gt;
&lt;p&gt;When you are using this information to create your URDF model there are a number of issues that you may
run into. The main ones I ran in to are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ROS2 control documentation is lacking in that it doesn't necessarily exist for the combination of
your version of ROS and Gazebo. That means you need to look at the source code to figure out the
names of the plugins and which controllers are available. For instance ROS2 control defines a
&lt;a href="https://control.ros.org/humble/doc/ros2_controllers/joint_trajectory_controller/doc/userdoc.html"&gt;joint trajectory controller&lt;/a&gt;.
This controller should be able to work with a velocity trajectory, i.e. a trajectory that defines
changes in velocity. However the current implementation of the controller doesn't support this. Additionally
the Gazebo plugin doesn't support all the controller types that are available in ROS2 control. To
find out which controllers actually work in Gazebo you need to search the source code of the Gazebo
control plugin.&lt;/li&gt;
&lt;li&gt;In order to run ROS2 control you need to load the &lt;a href="https://control.ros.org/humble/doc/ros2_control/controller_manager/doc/userdoc.html"&gt;controller manager&lt;/a&gt;.
This component manages the lifecyle of the controllers. However when running in Gazebo you shouldn't
to load the controller manager as Gazebo loads one for you. If you do load the controller manager
then you will get an error message that the controller manager is already loaded. Also note that in
order to optionally load the controller when using the Python launch files you need to use the
&lt;a href="https://github.com/pvandervelde/zinger_description/blob/bb24b884f8bcc62c9c2e8f12bac431f4b62dea6f/launch/controllers.launch.py#L52"&gt;unless&lt;/a&gt;
construct, not the Python &lt;code&gt;if .. then&lt;/code&gt; approach. The latter doesn't work due to the delayed
evaluation of the launch file.&lt;/li&gt;
&lt;li&gt;The start up order of the ROS2 control controllers relative to other ROS nodes is important. When
the controllers start they will try to get the joint and link descriptions from the
&lt;a href="https://github.com/ros/robot_state_publisher/tree/humble"&gt;&lt;code&gt;robot state publisher&lt;/code&gt;&lt;/a&gt;. This won't
be running until after the simulation starts. So you need to &lt;a href="https://github.com/pvandervelde/zinger_description/blob/bb24b884f8bcc62c9c2e8f12bac431f4b62dea6f/launch/controllers.launch.py#L72"&gt;delay&lt;/a&gt;
the start of the controllers until after the robot is loaded in Gazebo and the state publisher node
is running.&lt;/li&gt;
&lt;li&gt;Unlike with ROS1 if you want your ROS2 nodes to get information from Gazebo, e.g. the simulation
time or the current position or velocity of a joint, then you need to set up a
&lt;a href="https://github.com/pvandervelde/zinger_ignition/blob/5e71f841d1db557c3e62ff38291701ddc31a0d73/launch/ignition_bridge.launch.py#L19"&gt;ROS bridge&lt;/a&gt;.
This is because Gazebo no longer connects to the ROS2 messaging system automatically. For my simulation
I bridged the following topics:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/clock&lt;/code&gt; - The simulation time. This was bridged unidirectionally from Gazebo to ROS2 into the
&lt;code&gt;/clock&lt;/code&gt; topic.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/cmd_vel&lt;/code&gt; - The velocity commands for the robot. This was bridged bidirectionally between Gazebo
and ROS2.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/model/&amp;lt;ROBOT_NAME&amp;gt;/pose&lt;/code&gt; and &lt;code&gt;/model/&amp;lt;ROBOT_NAME&amp;gt;/pose_static&lt;/code&gt; - The position of the robot
in the simulation as seen by the simulation. This was bridged unidirectionally from Gazebo to
ROS2 into the &lt;code&gt;/ground_truth_pose&lt;/code&gt; and &lt;code&gt;/ground_truth_pose_static&lt;/code&gt; topics.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/model/&amp;lt;ROBOT_NAME&amp;gt;/tf&lt;/code&gt; - The transformation between the different coordinate frames of the
robot as seen by the simulation. This was bridged unidirectionally from Gazebo to ROS2 into the
&lt;code&gt;/model/&amp;lt;ROBOT_NAME&amp;gt;/tf&lt;/code&gt; topic.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/&amp;lt;LIDAR_NAME&amp;gt;/scan&lt;/code&gt; and &lt;code&gt;/&amp;lt;LIDAR_NAME&amp;gt;/scan/points&lt;/code&gt; - The point cloud from the lidar sensor.
These were bridged unidirectionally from Gazebo to ROS2 into the &lt;code&gt;/scan&lt;/code&gt; and &lt;code&gt;/scan/points&lt;/code&gt; topics.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/world/&amp;lt;WORLD_NAME&amp;gt;/model/&amp;lt;MODEL_NAME&amp;gt;/link/&amp;lt;IMU_SENSOR_LINK_NAME&amp;gt;/sensor/&amp;lt;IMU_NAME&amp;gt;/imu&lt;/code&gt; - The
IMU data. This was bridged unidirectionally from Gazebo to ROS2 into the &lt;code&gt;/imu&lt;/code&gt; topic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally one issue  that applies specifically to ROS2 Humble and Gazebo Ignition has to do with the
fact that Gazebo Ignition was renamed back to Gazebo. This means that some of the plugins have been
renamed as well. So the information you find on the internet about the correct name of the plugin
may be out of date.&lt;/p&gt;
&lt;p&gt;Once you have the URDF you need to get Gazebo to load it. This is done in two parts. First you need
to load the &lt;code&gt;robot state publisher&lt;/code&gt; and provide it with the robot description (URDF). The code below
provides an example on how to achieve this.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def generate_launch_description():
    is_simulation = LaunchConfiguration(&amp;quot;use_sim_time&amp;quot;)
    use_fake_hardware = LaunchConfiguration(&amp;quot;use_fake_hardware&amp;quot;)
    fake_sensor_commands = LaunchConfiguration(&amp;quot;fake_sensor_commands&amp;quot;)

    robot_description_content = Command(
        [
            PathJoinSubstitution([FindExecutable(name=&amp;quot;xacro&amp;quot;)]),
            &amp;quot; &amp;quot;,
            PathJoinSubstitution([get_package_share_directory('zinger_description'), &amp;quot;urdf&amp;quot;, 'base.xacro']),
            &amp;quot; &amp;quot;,
            &amp;quot;is_simulation:=&amp;quot;,
            is_simulation,
             &amp;quot; &amp;quot;,
            &amp;quot;use_fake_hardware:=&amp;quot;,
            use_fake_hardware,
            &amp;quot; &amp;quot;,
            &amp;quot;fake_sensor_commands:=&amp;quot;,
            fake_sensor_commands,
        ]
    )
    robot_description = {&amp;quot;robot_description&amp;quot;: ParameterValue(robot_description_content, value_type=str)}

    # Takes the joint positions from the 'joint_state' topic and updates the position of the robot with tf2.
    robot_state_publisher = Node(
        package='robot_state_publisher',
        executable='robot_state_publisher',
        name='robot_state_publisher',
        output='screen',
        parameters=[
            {'use_sim_time': LaunchConfiguration('use_sim_time')},
            robot_description,
        ],
    )

    ld = LaunchDescription(ARGUMENTS)
    ld.add_action(robot_state_publisher)
    return ld

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you need to spawn the robot in Gazebo. Assuming you're using Gazebo Ignition then you can use
the &lt;code&gt;ros_ign_gazebo&lt;/code&gt; package with the command line as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
    def generate_launch_description():
        spawn_robot = Node(
            package='ros_ign_gazebo',
            executable='create',
            arguments=[
                '-name', LaunchConfiguration('robot_name'),
                '-x', x,
                '-y', y,
                '-z', z,
                '-Y', yaw,
                '-topic', '/robot_description'],
            output='screen')

        ld = LaunchDescription(ARGUMENTS)
        ld.add_action(spawn_robot)

        return ld
&lt;/code&gt;&lt;/pre&gt;
&lt;iframe
    style="float:right"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/fR47Y7p4mtQ"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Once you have set this all up you should be able to run Gazebo and see your robot in the simulation.
At this point you can then control the robot using the ROS2 control interface by sending the
appropriate messages. For an example you can look at the &lt;a href="https://github.com/pvandervelde/zinger_controller_test_nodes"&gt;test control library&lt;/a&gt;
for the swerve robot. That should allow you to get the robot moving as shown in the video. In the video
two different control commands are given. One controls the motion of the steering actuators and makes
the wheels steer from left to right and back again on a timed loop. The other controls the motion of
the wheels, driving them forwards and then backwards, also on a timed loop. Obviously this is not
a good way to control the robot, but it does allow you to verify that you have correctly configured
all the parts of your robot for use in Gazebo.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Swerve drive - Motion profiles</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-motion-profiles</link>
			<description>&lt;p&gt;In the last few posts I have described the simulations I did of a robot with a swerve drive. In
other words a robot with four wheels each of which is independently driven and steered. I did
simulations for the case where we specified movement commands &lt;a href="https://youtu.be/LlyopmLMlZY"&gt;directly for the drive modules&lt;/a&gt;
and one for the case where we specified movement commands for &lt;a href="https://youtu.be/U6Z_meFKNrI"&gt;the robot body&lt;/a&gt;
which were then translated in the appropriate movements for the drive modules.
One of the things you can see in both simulations is that the motions is quite ‘jerky’, i.e. with
sudden changes of velocity or acceleration. In real life this kind of change would be noticed by
humans as shocks which are &lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)#Physiological_effects_and_human_perception"&gt;uncomfortable and can potentially cause injury&lt;/a&gt;.
For the equipment, i.e. the robot parts, a jerky motion adds load which can cause failures. So for
both humans and equipment it is sensible to keep the 'jerkiness' as low as possible.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-motion-profiles</guid>
			<pubDate>Mon, 24 Jul 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In the last few posts I have described the simulations I did of a robot with a swerve drive. In
other words a robot with four wheels each of which is independently driven and steered. I did
simulations for the case where we specified movement commands &lt;a href="https://youtu.be/LlyopmLMlZY"&gt;directly for the drive modules&lt;/a&gt;
and one for the case where we specified movement commands for &lt;a href="https://youtu.be/U6Z_meFKNrI"&gt;the robot body&lt;/a&gt;
which were then translated in the appropriate movements for the drive modules.
One of the things you can see in both simulations is that the motions is quite &amp;lsquo;jerky&amp;rsquo;, i.e. with
sudden changes of velocity or acceleration. In real life this kind of change would be noticed by
humans as shocks which are &lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)#Physiological_effects_and_human_perception"&gt;uncomfortable and can potentially cause injury&lt;/a&gt;.
For the equipment, i.e. the robot parts, a jerky motion adds load which can cause failures. So for
both humans and equipment it is sensible to keep the 'jerkiness' as low as possible.&lt;/p&gt;
&lt;p&gt;In order to achieve this we first need to understand what jerk actually is. Once we understand it
we can figure out ways to control it. Jerk is defined as the change of acceleration
with time, i.e the first time derivative of acceleration. So jerk vector as a function of time,
&lt;la-tex&gt;\vec{{j}}(t)&lt;/la-tex&gt; can be defined as:&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    \vec{j}(t) = \frac{d\vec{a}(t)}{dt} = \frac{d^{2}\vec{v}(t)}{dt^{2}} = \frac{d^{3}\vec{r}(t))}{dt^{3}}
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;la-tex&gt;t&lt;/la-tex&gt; is time.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;\frac{{d}}{{dt}}&lt;/la-tex&gt;, &lt;la-tex&gt;\frac{{d}}{{dt^{2}}}&lt;/la-tex&gt;, &lt;la-tex&gt;\frac{{d}}{{dt^{3}}}&lt;/la-tex&gt;
are the &lt;a href="https://en.wikipedia.org/wiki/Time_derivative"&gt;first&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Second_derivative"&gt;second&lt;/a&gt;
and &lt;a href="https://en.wikipedia.org/wiki/Third_derivative"&gt;third&lt;/a&gt; time derivatives.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;\vec{{a}}&lt;/la-tex&gt; is the acceleration vector.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;\vec{{v}}&lt;/la-tex&gt; is the velocity vector.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;\vec{{r}}&lt;/la-tex&gt; is the position vector.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From these equations we can for instance deduce that a linearly increasing acceleration is caused by
a constant jerk value. And a constant jerk value leads to a quadratic behaviour in the velocity. A
more interesting deduction is that an acceleration that changes from a linear increase to a constant
value means that there must be a discontinuous change in jerk. After all a linear increasing acceleration
is caused by a constant positive jerk, and a constant acceleration is achieved by a zero jerk. Where
these two acceleration profiles meet there must be a jump in jerk.&lt;/p&gt;
&lt;p&gt;With that you can probably imagine what happens if the velocity has a change from a linearly increasing
value to a constant value. The acceleration drops from a positive constant value to zero. And the
jerk value displays spikes when the acceleration changes. For the linear motion profile this behaviour
is amplified as shown in the plot below. The change in position requires a constant value for velocity
which requires significant acceleration and jerk spikes at the start and end of the motion profile.&lt;/p&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/control/position_velocity_acceleration_and_jerk_for_linear_motion_profile.png" target="_blank"&gt;
    &lt;img
        alt="Position, velocity, acceleration and jerk curves for the linear motion profile"
        src="/assets/images/robotics/control/position_velocity_acceleration_and_jerk_for_linear_motion_profile.png"
        width="800"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Position, velocity, acceleration and jerk curves for the linear motion profile.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;So in order to move a robot, or robot part, from one location to another in a way that the jerk
values stay manageable we need to control the velocity and acceleration across time. This is normally
done using a motion profile which describes how the velocity and acceleration change over time in
order to arrive at the new state at the desired point in time. Two of the most well
known motion profiles are the trapezoidal profile and the s-curve profile.&lt;/p&gt;
&lt;h3&gt;The trapezoidal motion profile&lt;/h3&gt;
&lt;p&gt;The trapezoidal motion profile consist of three distinct phases. During the first phase a constant
positive acceleration is applied. This leads to a linearly increasing velocity until the maximum
velocity is achieved. During the second phase no acceleration is applied, keeping the velocity constant.
Finally in the third phase a constant negative acceleration is applied, leading to a decreasing
velocity until the velocity becomes zero.&lt;/p&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/control/position_velocity_acceleration_and_jerk_for_trapezoidal_motion_profile.png" target="_blank"&gt;
    &lt;img
        alt="Position, velocity, acceleration and jerk curves for the trapezoidal motion profile"
        src="/assets/images/robotics/control/position_velocity_acceleration_and_jerk_for_trapezoidal_motion_profile.png"
        width="800"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Position, velocity, acceleration and jerk curves for the trapezoidal motion profile.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The equations for the different phases are as follows&lt;/p&gt;
&lt;p&gt;The acceleration, velocity and position in each phase of the trapezoidal motion profile can be
described with the standard &lt;a href="https://en.wikipedia.org/wiki/Equations_of_motion"&gt;equations of motion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    v_n(t) = v_{n-1} + a_{n-1} t
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    r_n(t) = r_{n-1} + v_{n-1} t + \frac{1}{2} a_{n-1} t^2
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;la-tex&gt;t&lt;/la-tex&gt; is the amount of time spend in the current phase.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;n&lt;/la-tex&gt; is the current phase&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For my calculations I assumed that the acceleration phase and the deceleration phase take the same
amount of time, thus they have the same acceleration magnitude but different signs. With this the
differences for each phase are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;la-tex&gt;a(t) = a_{{max}}&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;a(t) = 0&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;a(t) = -a_{{max}}&lt;/la-tex&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;My implementation&lt;/h4&gt;
&lt;p&gt;To simplify my implementation of the trapezoidal motion profile I assumed that the different stages
of the profile all take the same amount of time, i.e. one third of the total time. In real life this
may not be true because the amounts of time spend in the different stages depend on the maximum
reachable acceleration and velocity as well as the minimum and maximum time in which the profile
needs to be achieved. Making this assumption simplifies the initial implementation. At a later stage
I will come back and implement more realistic profile behaviour.&lt;/p&gt;
&lt;p&gt;Additionally my motion profile code assumes that the motion profile is stored for a relative
timespan of 1 unit. If I want a different timespan I can just multiply by the desired
timespan to get the final result. For this case we can now determine what the
maximum velocity is that we need in order to travel the desired distance.&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    r = 0.5 * v * t_{accelerate} + v * t_{constant} + 0.5 * v * t_{decelerate}
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;la-tex&gt;t_{{accelerate}}&lt;/la-tex&gt; is the total time during which there is a positive acceleration,
which is one third of the total time.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;t_{{constant}}&lt;/la-tex&gt; is the time during which there is a constant velocity, which is
also one third of the total time.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;t_{{decelerate}}&lt;/la-tex&gt; is the time during which there is a negative acceleration, which
again is one third of the total time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simplifying leads to&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    r = \frac{2}{3} v t \Longrightarrow v = \frac{3}{2} \frac{s}{t}
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;Using this maximum velocity and the equations for the different phases I implemented a
&lt;a href="](https://github.com/pvandervelde/basic-swerve-sim/blob/b3cc1c7d5b4502d502493abfcb0c10cca3f5cf59/swerve_controller/profile.py#L449)"&gt;trapezoidal motion profile&lt;/a&gt;.
The results from running this motion profile are displayed in the plots above. These plots show that
the trapezoidal has no large spikes in the acceleration profile when compared to the linear motion profile.
additionally the jerk spikes for the trapezoidal motion profile are significantly smaller than the
ones generated by the linear motion profile. So we can conclude that the trapezoidal motion profile
is a better motion profile than the linear profile. However there are still spikes in the jerk values
that would be detrimental for both humans and machinery. So it would be worth finding a better
motion profile. That motion profile is the s-curve motion profile discussed in the next section.&lt;/p&gt;
&lt;h3&gt;The s-curve motion profile&lt;/h3&gt;
&lt;p&gt;Where the trapezoidal motion profile consisted of three different phases, the s-curve motion profile
has seven distinct phases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ramp up phase where a constant positive jerk is applied. Which leads to a linearly
increasing acceleration and a velocity ramping up from zero following a second order curve.&lt;/li&gt;
&lt;li&gt;The constant acceleration phase where the jerk is zero. In this phase the velocity is
increasing linearly.&lt;/li&gt;
&lt;li&gt;The first ramp down phase where a constant negative jerk is applied. This leads
to a linearly decreasing acceleration until zero acceleration is achieved. The velocity is still
increasing, following a slowing second order curve to a constant velocity value.&lt;/li&gt;
&lt;li&gt;The constant velocity phase where the jerk and acceleration are both zero.&lt;/li&gt;
&lt;li&gt;The first part of the deceleration phase where a constant negative jerk is applied.
Again this leads to a linearly decreasing acceleration, and a velocity decreasing following
a second order curve.&lt;/li&gt;
&lt;li&gt;In this phase the acceleration is kept constant and the velocity decreases linearly.&lt;/li&gt;
&lt;li&gt;The final phase where a constant positive jerk is applied with the goal to linearly increase the
acceleration until zero acceleration is reached. The velocity will keep decreasing following a
second order curve, until it too reaches zero.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/control/position_velocity_acceleration_and_jerk_for_scurve_motion_profile.png" target="_blank"&gt;
    &lt;img
        alt="Position, velocity, acceleration and jerk curves for the s-curve motion profile"
        src="/assets/images/robotics/control/position_velocity_acceleration_and_jerk_for_scurve_motion_profile.png"
        width="800"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Position, velocity, acceleration and jerk curves for the s-curve motion profile.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As with the trapezoidal motion profile, the acceleration, velocity and position in each phase of the
s-curve motion profile can be described with the standard equations of motion. The difference is
that the acceleration is a linear function, which introduces a jerk value to the equations.&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    a_n(t) = a_{n-1} + j_n t
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    v_n(t) = v_{n-1} + a_{n-1} t + \frac{1}{2} j_n t^2
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;la-tex class="block"&gt;
    r_n(t) = r_{n-1} + v_{n-1} t + \frac{1}{2} a_{n-1} t^2 + \frac{1}{6} j_n t^3
  &lt;/la-tex&gt;
&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;la-tex&gt;t&lt;/la-tex&gt; is the amount of time spend in the current phase.&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;n&lt;/la-tex&gt; is the current phase&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As with the trapezoidal motion profile I assumed that the acceleration and deceleration phases
span the same amount of time. Again this means the acceleration and deceleration have magnitude but
different signs. The differences for each phase are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;la-tex&gt;j(t) = j_{{max}}&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;j(t) = 0&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;j(t) = -j_{{max}}&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;j(t) = 0&lt;/la-tex&gt;, &lt;la-tex&gt;a(t) = 0&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;j(t) = -j_{{max}}&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;j(t) = 0&lt;/la-tex&gt;&lt;/li&gt;
&lt;li&gt;&lt;la-tex&gt;j(t) = j_{{max}}&lt;/la-tex&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;My implementation&lt;/h4&gt;
&lt;p&gt;To simplify my implementation of the s-curve motion profile I assumed that all stages, except stage 4,
all take the same amount of time, i.e. one eight of the total time. I assumed that stage 4 would take
one quarter of the time. Like with the trapezoidal profile making this assumption simplifies the
initial implementation. At a later stage I will come back and implement more realistic profile behaviour.&lt;/p&gt;
&lt;p&gt;Additionally my motion profile code assumes that the motion profile is stored for a relative
timespan of 1 unit. If I want a different timespan I can just multiply by the desired
timespan to get the final result. For this case we can now determine what the
maximum velocity is that we need in order to travel the desired distance.&lt;/p&gt;
&lt;p&gt;Using the equations above I implemented a
&lt;a href="](https://github.com/pvandervelde/basic-swerve-sim/blob/b3cc1c7d5b4502d502493abfcb0c10cca3f5cf59/swerve_controller/profile.py#L600)"&gt;s-curve motion profile&lt;/a&gt;.
The results from running this motion profile are displayed in the plots above. These plots show that
the s-curve removes the spikes in the jerk profile when compared to the trapezoidal motion profile.
This indicates that the s-curve motion profile achieves the goal we previously set, to minimize
the jerky motion.&lt;/p&gt;
&lt;p&gt;For some applications it is important to provide even smoother motions. In this case the motion
profiles may need to take into account the values of the
&lt;a href="https://en.wikipedia.org/wiki/Fourth,_fifth,_and_sixth_derivatives_of_position"&gt;fourth, fifth or even sixth order time derivatives of position&lt;/a&gt;,
snap, crackle and pop. At the moment I have not implemented these higher order motion profiles.&lt;/p&gt;
&lt;h3&gt;Use in the swerve simulation&lt;/h3&gt;
&lt;p&gt;Having these motion profiles is great, however by themselves they are of little use. So I added them
to my swerve drive simulation to see what the differences were between the different motion profiles.&lt;/p&gt;
&lt;p&gt;Before we look at the new results it is worth looking at the simulations using the linear
motion profile. I made one for &lt;a href="https://youtu.be/LlyopmLMlZY"&gt;module control&lt;/a&gt; and one for
&lt;a href="https://youtu.be/U6Z_meFKNrI"&gt;body control&lt;/a&gt;. In these simulations you can see that with the linear
motion profile the jerk spikes are quite large, especially in the case of the module control
simulation. The body control approach performs a little better with respect to the maximum levels
of jerk, however the values are still far too high.&lt;/p&gt;
&lt;p&gt;The simulations for &lt;a href="https://youtu.be/wUb6K0o4oW8"&gt;module control&lt;/a&gt; and
&lt;a href="https://youtu.be/Jyp3w_zgAlI"&gt;body control&lt;/a&gt; using the trapezoidal profile show a
significant decrease in the maximum value of the acceleration and jerk values, in some cases by a
factor 15. As expected from the previous discussion there are still some spikes, especially for the
steering angles. The changes for the module control case are more drastic than for the
body control case, probably due to the fact that the values were very high for the combination
of the module control with a linear motion profile. Interestingly the acceleration and jerk
maximum values are lower for the module control approach than they are for the body control
approach. This is most likely due to the fact that in order to keep the drive modules synchronised
relatively high steering velocities are required. For instance, the module control approach using the
trapezoidal motion profile sees a maximum steering velocity of about 1.8 radians per second. Compare
this to the the body control approach with the trapezoidal motion profile which sees a maximum
steering velocity of about 3.8 radians per second.&lt;/p&gt;
&lt;iframe
    style="float:none"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/PxMFjGLH0xY"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;When we look at the simulations using the s-curve motion profile we can see that the maximum
acceleration and jerk values actually increase when compared to the trapezoidal motion profile, except
for the values of the steering angle jerk when using the module control approach. It seems likely that
these increases are due to the fact that the motion is executed over the same time span, but some of
the time is used for a more smooth acceleration and deceleration. This means that there is less
time available to travel the desired &amp;lsquo;distance&amp;rsquo; which then requires higher maximum velocities and
maximum accelerations. The s-curve motion profile does smooth out the movement profiles which would
lead to a much smoother ride.&lt;/p&gt;
&lt;iframe
    style="float:none"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/B3CUdHifQCQ"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;h4&gt;What is next&lt;/h4&gt;
&lt;p&gt;So now that we have a swerve drive simulation that can use both module based control and body
based control as well as have different motion profiles, what is next? There are a few improvements
that can be made to the simulation code to further made to the simulation and a path of progression.&lt;/p&gt;
&lt;p&gt;The first improvement lies in the fact that none of the motion profiles, linear, trapezoidal and
s-curve, are aware of motor limits. This means that they will happily command velocity, acceleration
and jerk values that a real life motor would not be able to deliver. In order to make the simulation
better I would need to add some kind of limits on the maximum reachable values. This would be
especially interesting when using body oriented control. Because the high velocities and accelerations
are needed to keep the drive modules synchronised. If one of the drive modules is not able to reach
the desired velocity or acceleration then the other modules will have to slow down before they
reach their limits. To control the drive modules in such a way that all of the modules stay within
their motor limits while also keeping them synchronised requires some fancy math. At the moment
I'm going through a number of published papers to see what different algorithms are out there.&lt;/p&gt;
&lt;p&gt;The second improvement that is on my mind is to implement some form of path tracking, i.e. the
ability to follow a given path. This would give the simulation the ability to better show the
behaviour of a real life robot. In most cases when a robot is navigating an area the path planning
code will constantly be sending movement instructions to ensure that the robot follows the originally
planned path. This means that motion profiles need to be updated constantly, which will be a challenge
for the simulation code. Additionally having path tracking in the simulation would allow me to
experiment with different algorithms for path tracking and trajectory tracking, i.e. the ability to
follow a path and prescribe the velocity at every point on the path. And theoretically with a
swerve drive it should even be possible for the robot to follow a trajectory while controlling
the orientation of the robot body.&lt;/p&gt;
&lt;p&gt;Finally the path of progression is to take the controller code that I have written for this
simulation and use it with my &lt;a href="/posts/Swerve-drive-introduction"&gt;ROS2 based swerve robot&lt;/a&gt;.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Swerve drive - Movement animations</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-drive-control-animations</link>
			<description>&lt;p&gt;In my last two posts I talked about different control methods for a swerve drive robot. One method
controls the movement of drive modules directly. The other method controls the movement of
the body and derives the desired state for the drive modules from that.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-drive-control-animations</guid>
			<pubDate>Tue, 04 Jul 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In my last two posts I talked about different control methods for a swerve drive robot. One method
controls the movement of drive modules directly. The other method controls the movement of
the body and derives the desired state for the drive modules from that.&lt;/p&gt;
&lt;figure style="float:middle"&gt;
  &lt;a href="/assets/images/robotics/swerve/swerve_sim_45_linear_to_inplace_rotation.png" target="_blank"&gt;
    &lt;img
        alt="45 degree linear track to in-place rotation"
        src="/assets/images/robotics/swerve/swerve_sim_45_linear_to_inplace_rotation.png"
        width="840"
        height="368"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Simulation data for a 45 degree linear track that transitions to an in-place rotation.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To see the difference between these control methods my simulation code created all kinds of interesting
plots like the ones above. However I was still having trouble visualizing what was actually happening,
especially in the case of the movement of the Instantaneous Centre of Rotation (ICR), i.e. the
rotation point for the robot at a given point in time. The lower left graph shows the paths the
ICR for different combinations of drive modules. While it looks pretty it does not make a lot of
sense to me.&lt;/p&gt;
&lt;iframe
    style="float:left"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/LlyopmLMlZY"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;So to address that issue I updated the simulation code to produce some &lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/a83c0d8ce4cc3096548be51244ec0a40d2a7db8f/sim_output/animate.py"&gt;animations&lt;/a&gt;
that display the position of the robot and the wheels as well as a number of plots for the state of
the robot body and the drive modules. To create the animations I used the
&lt;a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FuncAnimation.html#matplotlib.animation.FuncAnimation"&gt;FuncAnimation class&lt;/a&gt;
that is available in matplotlib. The animations can then either be turned into an HTML page with
animation controls using the &lt;a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.HTMLWriter.html"&gt;HTMLWriter&lt;/a&gt;,
or into MP4 video files using the &lt;a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FFMpegWriter.html"&gt;FFMpegWriter&lt;/a&gt;.
In order to get reasonable performance when using the animation functions in matplotlib it is important
to update the plots instead of drawing new ones. This can be done using the &lt;a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_data"&gt;set_data&lt;/a&gt;
function, for instance for updating the position of the &lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/a83c0d8ce4cc3096548be51244ec0a40d2a7db8f/sim_output/animate.py#L784"&gt;robot body&lt;/a&gt;.
It is good to keep in mind that even with this performance improvement the creation of the animations
isn't very fast for our robot simulation because a large number of image frames need to be made. For
the 6 second movement in the animation anywhere between 150 and 600 frames need to be created.&lt;/p&gt;
&lt;p&gt;The animation above shows how the robot behaves when using the
&lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;direct module controller&lt;/a&gt;. As you can see in the video different
pairs of wheels have different rotation points, signified by the red dots. As the movement progresses
these rotation points have quite a large range of motion. This indicates that the wheels are not
synchronized and most likely some of the wheels are slipping.&lt;/p&gt;
&lt;iframe
    style="float:right"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/U6Z_meFKNrI"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;I created another animation for the same situation but with the
&lt;a href="/posts/Swerve-drive-body-focussed-control"&gt;body oriented controller&lt;/a&gt;. In this case the rotation points
are all in a single location leading me to conclude that all the wheels are synchronized and little
to no wheel slip is occurring.&lt;/p&gt;
&lt;p&gt;One other interesting thing you can see in the video is that the acceleration and
&lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)"&gt;jerk&lt;/a&gt; values change very abruptly. In real life this
would lead to significant loads on the robot and its drive system. In the simulation this behaviour
is due to the fact that linear profile that is being used to transition from one state to another.
As mentioned before the next improvement will be to replace this linear interpolation with a control
approach that will provide &lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)#In_motion_control"&gt;smooth transitions&lt;/a&gt;
for velocity and accelerations.&lt;/p&gt;
&lt;h4&gt;Edits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;July 6th 2023: Added a section discussing the use of the matplotlib animation functions.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>Swerve drive - Better movement by controlling the body motions</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-drive-body-focussed-control</link>
			<description>&lt;p&gt;In my &lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;first attempt&lt;/a&gt; to model a swerve drive I controlled
the movement of the robot by directly providing commands to the individual drive modules, i.e. by
controlling the steering angle and the wheel velocity of each of the four drive modules. Additionally
I assumed that the transition from one state (steering angle, wheel velocity) to another state would
follow a linear profile. An example of this can be seen in the graphs below. The bottom left graph shows
the linear change in steering angle for the four drive modules. It should be noted that a linear
profile isn't very realistic as this requires the motors to be capable of instant changes in velocity
and acceleration. However making this assumption keeps the calculations simple. At a later stage I
will be encoding different movement profiles amongst which a low
&lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)"&gt;jerk&lt;/a&gt; profile for smooth movements.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-drive-body-focussed-control</guid>
			<pubDate>Fri, 09 Jun 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In my &lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;first attempt&lt;/a&gt; to model a swerve drive I controlled
the movement of the robot by directly providing commands to the individual drive modules, i.e. by
controlling the steering angle and the wheel velocity of each of the four drive modules. Additionally
I assumed that the transition from one state (steering angle, wheel velocity) to another state would
follow a linear profile. An example of this can be seen in the graphs below. The bottom left graph shows
the linear change in steering angle for the four drive modules. It should be noted that a linear
profile isn't very realistic as this requires the motors to be capable of instant changes in velocity
and acceleration. However making this assumption keeps the calculations simple. At a later stage I
will be encoding different movement profiles amongst which a low
&lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)"&gt;jerk&lt;/a&gt; profile for smooth movements.&lt;/p&gt;
&lt;figure style="float:middle"&gt;
  &lt;a href="/assets/images/robotics/swerve/swerve_sim_45_linear_to_inplace_rotation.png" target="_blank"&gt;
    &lt;img
        alt="45 degree linear track to in-place rotation with a module-first control algorithm"
        src="/assets/images/robotics/swerve/swerve_sim_45_linear_to_inplace_rotation.png"
        width="840"
        height="368"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    Simulation data for a 45 degree linear track that transitions to an in-place rotation with a
    module-first control algorithm.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The benefit of controlling the robot by directly controlling the drive modules is that the control
algorithm is very simple and all movement patterns are possible. For instance it is possible to make
all the wheels change steering angle without moving the robot body, something which is not possible
with indirect control methods.&lt;/p&gt;
&lt;p&gt;However there are also a number of drawbacks. The main one is that
for a swerve drive to be efficient the motions of the different drive modules need to be synchronized.
That means that all the drive modules needs to be controlled so that none of the wheels slip along
the surface. For simple linear or rotational movements of the robot body this is easy to achieve but
for complicated movements this is much more difficult. An example is shown in image above which
describes the behaviour of the robot as it moves linearly at a 45 degree angle and then transitions
to an in-place rotation around the robot centre axis. The bottom right graph shows the
&lt;a href="https://en.wikipedia.org/wiki/Instant_centre_of_rotation"&gt;instantaneous centre of rotation&lt;/a&gt; calculated
for different combinations of two wheels. In a synchronised motion all wheels should point to the same
instantaneous centre of rotation at any given moment. If the ICR's for the different wheel pairs are
in different locations then one or more of the wheels is out of sync and likely slipping along
the ground. The graph clearly shows that in this case the drive modules are not synchronized thus
causing wheel slip.&lt;/p&gt;
&lt;p&gt;One way to improve the synchronization between the drive modules is apply the movement control at
an indirect level, for instance on the robot body, and then to work out from there what the drive modules
should be doing at any that given point in time along the transition profile.&lt;/p&gt;
&lt;p&gt;As with the previous approach I will use a
&lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/103b321c471ced6c8865680d1e550ab4f5893526/swerve_controller/profile.py#L47"&gt;linear profile&lt;/a&gt;
to determine the transitions between different body states. Then the transition for the
&lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/103b321c471ced6c8865680d1e550ab4f5893526/swerve_controller/trajectory.py#L42"&gt;body velocities (x-velocity, y-velocity, and rotational velocity)&lt;/a&gt;
can be determined from the start state to the end state. Once it is known what
the body motion looks like I can calculate the
&lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/103b321c471ced6c8865680d1e550ab4f5893526/swerve_controller/multi_wheel_steering_controller.py#L100"&gt;states for the drive modules&lt;/a&gt;
from the body state for all given states in the transition profile.&lt;/p&gt;
&lt;figure style="float:middle"&gt;
  &lt;a href="/assets/images/robotics/swerve/serve_sim_module_flip_steering_angle.png" target="_blank"&gt;
    &lt;img
        alt="Drive module steering angle flip"
        src="/assets/images/robotics/swerve/serve_sim_module_flip_steering_angle.png"
        width="840"
        height="368"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    Drive module steering angle flip.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;One issue with this approach is that every body state can be achieved by two different module states,
one with the wheel moving &amp;lsquo;forward&amp;rsquo; and one with the wheel moving 'backward'. Initially the simulation
was hard-coded to always select the &amp;lsquo;forward&amp;rsquo; moving state. However this leads to situations where
the steering angle is flipped 180 degrees very rapidly. This is shown in the figure where the yellow
line switches from 45 degrees (or 0.78 radians) to 225 degrees (or 3.9 radians) in two time steps, or
0.02 seconds. In real life this change in steering angle would likely not be possible because the
steering angle motor would not be capable of delivering the enormous velocities and accelerations
necessary to achieve this change.&lt;/p&gt;
&lt;p&gt;To resolve this issue the simulation calculated the difference in rotation and velocity between the
current point in time and the previous point in time along the transition profile, for both the
forward and backward motions. Then &lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/103b321c471ced6c8865680d1e550ab4f5893526/swerve_controller/multi_wheel_steering_controller.py#L140"&gt;general rule&lt;/a&gt;
is to pick the smallest change in both rotation and velocity of the drive module. If that isn't
possible then we pick the one with the smallest rotation difference for no real reason other than we
have to pick something. In future work a more thorough algorithm will be implemented.&lt;/p&gt;
&lt;p&gt;With all that code in place the simulation delivers a much smoother result for the transition from
a 45 degree linear motion into a in-place rotation. The ICR path for all the wheel pairs is
following a single path indicating that all the drive modules are all synchronised during the
movement transition.&lt;/p&gt;
&lt;figure style="float:middle"&gt;
  &lt;a href="/assets/images/robotics/swerve/swerve_sim_body_first_45_linear_to_inplace_rotation.png" target="_blank"&gt;
    &lt;img
        alt="45 degree linear track to in-place rotation with a body first control algorithm"
        src="/assets/images/robotics/swerve/swerve_sim_body_first_45_linear_to_inplace_rotation.png"
        width="840"
        height="368"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;
    Simulation data for a 45 degree linear track that transitions to an in-place rotation with a
    body-first control algorithm.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The body oriented control approach ensures synchronisation of the drive modules while allowing
nearly all the different motions a swerve drive can make. The only motions not possible are those
that involve drive module steering angle changes only.&lt;/p&gt;
&lt;p&gt;The next improvement will be to replace the linear interpolation between the start state and the
desired end state with a control approach that will provide
&lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)#In_motion_control"&gt;smooth transitions&lt;/a&gt; for velocity and
accelerations. This will make the motions of the robot less jerky and thus less likely to damage
parts while also providing a smoother ride for the payload.&lt;/p&gt;
&lt;h4&gt;Edits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;July 4th 2023: Changed the term &lt;code&gt;control trajectory&lt;/code&gt; to &lt;code&gt;control profile&lt;/code&gt; because the term
&lt;code&gt;trajectory&lt;/code&gt; is generally reserved for path planning situations.&lt;/li&gt;
&lt;li&gt;July 4th 2023: Changed the term &lt;code&gt;ICR trajectory&lt;/code&gt; to &lt;code&gt;ICR path&lt;/code&gt; because the the ICR does not
follow a trajectory, it follows a path.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>Swerve drive - Verification of the kinematics solution</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-drive-kinematics-verification</link>
			<description>&lt;p&gt;As I explained in an &lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;earlier post&lt;/a&gt; I have written some
code to simulate the movement of a four wheel swerve drive.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-drive-kinematics-verification</guid>
			<pubDate>Thu, 04 May 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As I explained in an &lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;earlier post&lt;/a&gt; I have written some
code to simulate the movement of a four wheel swerve drive.&lt;/p&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/swerve/swerve-dof.png" target="_blank"&gt;
    &lt;img alt="Swerve drive degrees of freedom" src="/assets/images/robotics/swerve/swerve-dof.png" /&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Degrees of freedom for a swerve drive system.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Currently I have only implemented a
&lt;a href="https://www.chiefdelphi.com/t/paper-4-wheel-independent-drive-independent-steering-swerve/107383"&gt;simple kinematics based approach&lt;/a&gt;.
This approach is based on the diagram displaying degrees of freedom for the swerve drive as well as
the coordinate systems for the different parts. The simple kinematics approach makes a number of
assumptions which greatly simplify the problem space.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The steering axis of a drive module is vertical and passes through the centre of the wheel, i.e.
no positional changes occur when the wheel steering angle changes.&lt;/li&gt;
&lt;li&gt;The robot is moving on a flat, horizontal surface, i.e. the contact point between the wheel and
the ground is always inline with the steering axis of a drive module.&lt;/li&gt;
&lt;li&gt;The robot has no suspension, i.e. the body doesn't move vertically relative to the contact
point between the wheel and the ground.&lt;/li&gt;
&lt;li&gt;There is no wheel slip.&lt;/li&gt;
&lt;li&gt;There is no wheel lift-off, i.e. the wheels of the robot are always in contact with the ground.&lt;/li&gt;
&lt;li&gt;The motors are infinitely powerful and fast, i.e. there are no limits on the motor performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With diagram and the given assumptions we can derive the equations for the wheel velocity
and the steering angle of each drive module.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;v_i = v + W x r_i

alpha_i = acos (v_i_x / |v_i|)
      = asin (v_i_y / |v_i|)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;v_i&lt;/code&gt; - the wheel velocity of the i-th drive module, i.e. the velocity at which the drive module
would move forward if there is no wheel slip. The &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; components of this vector are
named as &lt;code&gt;v_i_x&lt;/code&gt; and &lt;code&gt;v_i_y&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v&lt;/code&gt; - the linear velocity of the robot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;W&lt;/code&gt; - the angular velocity of the robot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;r_i&lt;/code&gt; - the position vector of the i-th drive module.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha_i&lt;/code&gt; - the steering angle of the i-th drive module relative to the robot coordinate system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on these equations we can determine the forward kinematics, which translates the movement
of the drive modules to the movement of the robot body, and the inverse kinematics, which translates
the movement of the robot body to the movement of the drive modules. When doing the calculations for
a four wheel swerve drive it is important to note that the forward kinematics calculations are
&lt;a href="https://en.wikipedia.org/wiki/Overdetermined_system"&gt;overdetermined&lt;/a&gt;, i.e. there are more control
variables than there are outputs. This means that there are additional control variables that we can
play with. One obvious one for a swerve drive is that we can control the orientation of the robot body
independent[*] from the direction of movement of the robot. This also means that the forward kinematics
calculations are based on the &lt;a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse"&gt;pseudoinverse&lt;/a&gt;
approach which computes a best fit, a.k.a. least squares, using the drive module wheel velocities and
steering angles. In order words the wheel velocities and steering angles are approximations, not exact
values.&lt;/p&gt;
&lt;p&gt;The transition between states, i.e. from one combination of x-velocity, y-velocity and rotation velocity
to another combination, is done by assuming that there is linear control for the drive module variables,
i.e. wheel velocity and steering angle. While linear control profiles are not the best control
method it does allow later on only changing the profile code to use a more suitable one, for
instance a &lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)#In_motion_control"&gt;jerk limited&lt;/a&gt; profile.&lt;/p&gt;
&lt;p&gt;The code I wrote gives me a graphs like the ones presented in my &lt;a href="/posts/Swerve-drive-kinematics-simulation"&gt;previous post&lt;/a&gt;.
However before I use this code to test new control algorithms I want to make sure my code is
actually producing the correct results. The &lt;a href="https://en.wikipedia.org/wiki/Software_verification_and_validation"&gt;verification&lt;/a&gt;
is done by running a bunch of simple simulations for which I am able to predict the behaviour using
some simple maths.&lt;/p&gt;
&lt;p&gt;To verify that my code is correct I a ran a number of sets of simulations. The
&lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/master/verification/linear_module_first/simple_4w_steering/linear_with_single_axis/README.md"&gt;first set&lt;/a&gt;
is used to ensure that both the positive and the negative direction behaviour for the main axis directions.
Any differences in behaviour between the positive and the negative direction point to issues in the
simulation code. So the simulations that were done for this verification set are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Drive the robot in x-direction while facing in the x-direction, one simulation going forward from
the origin and one simulation going backwards from the origin.&lt;/li&gt;
&lt;li&gt;Drive the robot in the y-direction while facing in the x-direction, one simulation going left from
the origin and one simulation going right from the origin.&lt;/li&gt;
&lt;li&gt;Drive the robot in a rotation only movement, one simulation going clockwise and one simulation going
counter-clockwise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/master/verification/linear_module_first/simple_4w_steering/rotation_with_single_axis/README.md"&gt;second simulation set&lt;/a&gt;
is designed to verify the coordinate calculations related to rotations. The
simulations that were done for this verification set are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rotate the robot by 90 degrees and then drive it in the robot x-direction, driving forwards
and backwards.&lt;/li&gt;
&lt;li&gt;Rotate the robot by 90 degrees and then drive it in the robot y-direction, driving left and right.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="float:right"&gt;
  &lt;a href="/assets/images/robotics/swerve/swerve_sim_circle.png" target="_blank"&gt;
    &lt;img
        alt="Drive the robot in a circle."
        src="/assets/images/robotics/swerve/swerve_sim_circle.png"
        width="833"
        height="800"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Swerve drive position and velocities for driving in a circle.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The &lt;a href="https://github.com/pvandervelde/basic-swerve-sim/blob/master/verification/linear_module_first/simple_4w_steering/combined/README.md"&gt;third set of simulations&lt;/a&gt;
is designed to verify the behaviour during combined movements. The
simulations that were done for this verification set are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Drive the robot on the 30 degree diagonals (30 degrees, 60 degrees, 120 degrees, 150 degrees),
while facing in the x-direction, one simulation going forwards from the origin and one going
backwards from the origin.&lt;/li&gt;
&lt;li&gt;Drive the robot in a circle around a centre point outside the robot body.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While running the verification sets a number of bugs were found and fixed. At the end of the process
all the validations passed indicating that the simulation code is usable.&lt;/p&gt;
&lt;p&gt;With all of this done it is now time to do some more complicated simulations both for robot behaviour
that is specific to swerve drive systems and for different control algorithms. More on that in the next
post.&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;p&gt;[*] Mostly independent. In reality the motors used in the drive modules will have
limits on how fast they can be driven, how much torque they can produce and
how fast they can change from one state to the next. This means that there
are limitations on movements of the drive modules and thus the robot body. For
instance if a drive module has a maximum linear velocity of 1.0 m/s it is
not possible to drive the robot diagonally any faster than this velocity, even
though we can drive the robot in x-direction at 1.0 m/s and we can drive the
robot in y-direction at 1.0 m/s. We just can't do both at the same time.&lt;/p&gt;
&lt;h4&gt;Edits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;July 4th 2023: Changed the term &lt;code&gt;control trajectory&lt;/code&gt; to &lt;code&gt;control profile&lt;/code&gt; because the term
&lt;code&gt;trajectory&lt;/code&gt; is generally reserved for path planning situations.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>Swerve drive - Kinematics simulation</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-drive-kinematics-simulation</link>
			<description>&lt;p&gt;As &lt;a href="/posts/Swerve-drive-introduction"&gt;mentioned&lt;/a&gt; I am designing and building four wheel steering
mobile robot for use in outdoor environments and rough terrain. In that post I mused that both
the mechanical design and the software would be the most complicated parts of the drive system and
thus the parts I should focus on first. At the moment I don't have good access to a workshop where I
can experiment with the mechanical design so for now I'm focussing on the creation of the control
software. Once I have the controller software working I can then use Gazebo to test the robot
virtually to ensure that the code will work in the actual robot.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-drive-kinematics-simulation</guid>
			<pubDate>Fri, 10 Feb 2023 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As &lt;a href="/posts/Swerve-drive-introduction"&gt;mentioned&lt;/a&gt; I am designing and building four wheel steering
mobile robot for use in outdoor environments and rough terrain. In that post I mused that both
the mechanical design and the software would be the most complicated parts of the drive system and
thus the parts I should focus on first. At the moment I don't have good access to a workshop where I
can experiment with the mechanical design so for now I'm focussing on the creation of the control
software. Once I have the controller software working I can then use Gazebo to test the robot
virtually to ensure that the code will work in the actual robot.&lt;/p&gt;
&lt;p&gt;Unlike a differential drive a four wheel swerve drive has more degrees of freedom than needed, eight
degrees of freedom (steering angle and wheel velocity for each drive module) for the control versus
three spatial degrees of freedom (forward, sideways and rotate). This means that a swerve drive is
an over-determined system, requiring the control system to carefully control the wheel velocities
and angles so that they agree with each other, otherwise the wheels slip or drag.&lt;/p&gt;
&lt;figure style="float:left"&gt;
  &lt;a href="/assets/images/robotics/swerve/swerve-dof.png" target="_blank"&gt;
    &lt;img alt="Swerve drive degrees of freedom" src="/assets/images/robotics/swerve/swerve-dof.png" /&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Degrees of freedom for a swerve drive system.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;While doing some research I found many
&lt;a href="https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=multi+wheel+steering&amp;amp;btnG="&gt;different&lt;/a&gt;
&lt;a href="https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0,5&amp;amp;q=multi+wheel+steering+icr+mobile+robots"&gt;scientific&lt;/a&gt;
&lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=H10kxZgAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate"&gt;papers&lt;/a&gt;
describing algorithms for determining the &lt;a href="https://en.wikipedia.org/wiki/Forward_kinematics"&gt;forward&lt;/a&gt;
and &lt;a href="https://en.wikipedia.org/wiki/Inverse_kinematics"&gt;inverse&lt;/a&gt; kinematics of a four wheel steering system.
There are however very &lt;a href="https://github.com/MarkNaeem/ros_controllers/tree/noetic-devel/swerve_steering_controller"&gt;few&lt;/a&gt;
&lt;a href="https://github.com/ros-controls/ros_controllers/pull/441"&gt;software&lt;/a&gt;
&lt;a href="https://github.com/james-yoo/swerve_drive"&gt;libraries&lt;/a&gt; which implement the different control algorithms.&lt;/p&gt;
&lt;p&gt;Most papers and libraries focus on the simpler case of an indoor robot that moves along a flat
horizontal surface. In this case relatively simple kinematics approaches can be used. Unfortunately
these algorithms fail when used in 3d uneven terrain. A different
&lt;a href="https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=H10kxZgAAAAJ&amp;amp;sortby=pubdate&amp;amp;citation_for_view=H10kxZgAAAAJ:Se3iqnhoufwC"&gt;approach&lt;/a&gt;
is required for that case. At the moment I too will be focussed on algorithms for driving on
flat surfaces. At least until I have correctly working control code.&lt;/p&gt;
&lt;p&gt;After reading a number of papers and looking at some of the available code I have come to the
conclusion that I don't understand what is required for a successful swerve algorithm. There are
many variables that influence the behaviour making it hard to visualize what goes on as the robot is
moving around. So I wrote some &lt;a href="https://github.com/pvandervelde/basic-swerve-sim"&gt;code&lt;/a&gt;
to simulate what is going on (roughly) in a swerve drive while it is moving and steering. I'm
going to use this code to answer a number of questions I have about the different control algorithms.&lt;/p&gt;
&lt;p&gt;For instance algorithms often calculate the desired end state and then set the wheel position and
velocity to those required for that desired state. The question is does this algorithm automatically
ensure that the intermediate states are synchronised, and if not does that matter? An example of
this would be a robot moving linearly at 45 degrees and transitioning to an in-place rotation, as pictured
below. During the transition all drive modules should be synchronised so that the center of rotation
for the robot matches with the state of the drive modules.&lt;/p&gt;
&lt;figure style="float:right"&gt;
  &lt;a href="/assets/images/robotics/swerve/transition-45-to-rotate.png" target="_blank"&gt;
    &lt;img
        alt="Transition from 45 degree linear motion to in-place rotation."
        src="/assets/images/robotics/swerve/transition-45-to-rotate.png"
        width="525"
        height="385"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Transition from 45 degree linear motion to in-place rotation.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Another example is that many of the code libraries add an optimization that reverses the wheel
direction in favour of reducing the steering angle. For instance instead of changing the steering
angle from 0 degrees to 225 degrees with a wheel velocity of 1.0 rad/s, change the angle to 45 degrees
and reverse the wheel velocity to -1.0 rad/s. Again there are a number of questions about
this optimization. For instance making a smaller steering angle change reduces the energy used, however
stopping and reversing the wheel motion also takes energy, so what are the benefits of this optimization,
if any? And how do the algorithms keep the wheel motions synchronised when performing the reversing
optimization?&lt;/p&gt;
&lt;p&gt;The following graphs show an example of the simulation output. The motion being simulated is that of
a robot transiting from moving at a 45 degree straight path to an in-place rotation. The graphs
display the status of the robot body, position and velocity and the status of the different drive modules,
the angular orientation and the wheel velocity. The last graph depicts the location of the
Instantaneous Centre of Rotation (ICR) for different combinations of drive modules. The ICR is the
point in space around which the robot turns. If the control algorithm is correct then the ICR points
for different drive module combinations all fall in the same location though out the entire movement
pattern.&lt;/p&gt;
&lt;figure style="float:middle"&gt;
  &lt;a href="/assets/images/robotics/swerve/swerve_sim_45_linear_to_inplace_rotation.png" target="_blank"&gt;
    &lt;img
        alt="45 degree linear track to in-place rotation"
        src="/assets/images/robotics/swerve/swerve_sim_45_linear_to_inplace_rotation.png"
        width="840"
        height="368"/&gt;
  &lt;/a&gt;
  &lt;figcaption&gt;Simulation data for a 45 degree linear track that transitions to an in-place rotation.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Looking at these graphs a couple of observations can be made. The first observation is that the ICR
paths for the different module combinations don't match each other. This means that the drive modules
are not synchronised and one or more wheels will be experiencing wheel slip.
The second observation is that the linear control algorithm causes sharp changes in velocities leading
to extremely high acceleration demands. It seems unlikely that the motors and the structure would
be able to cope with these demands.&lt;/p&gt;
&lt;p&gt;From this relatively simple simulation we can see that there are a number of behaviours that the
control system needs to cope with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The state of the drive modules actively needs to be kept in sync at all times, including during
transitions from one movement state to another. This indicates that dynamic control is required
and thus poses questions about the update frequency for drive module sensors and control commands.&lt;/li&gt;
&lt;li&gt;The capabilities and behaviour of the motors needs to be taken into account in order to prevent
impossible movement commands and also to ensure that the drive modules remain synchronised during
movement commands that require fast state changes from the motors, e.g. to deal with motor
&lt;a href="https://en.wikipedia.org/wiki/Deadband"&gt;deadband&lt;/a&gt; or fast accelerations.&lt;/li&gt;
&lt;li&gt;The structural and kinematic limitations need to be considered when giving and processing movement
commands.&lt;/li&gt;
&lt;li&gt;Linear control behaviour is not ideal as it causes large acceleration demands so a better approach
is necessary.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that I have some working simulation code what are the next steps? The first stage is to simulate
some more simple movement trajectories to validate the simulation code. Once I have confidence that
the code actually simulates real world behaviour I can implement different control algorithms. These
algorithms can then be compared to see which algorithm behaves the best. Currently I'm thinking to
implement&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A control algorithm that uses a known movement profile for the robot base. It can then calculate
the desired drive module state across time to match the robot base movement. The movement profile
could be linear, polynomial or any other sensible profile.&lt;/li&gt;
&lt;li&gt;A controller that optimizes module turn time by having it turn the shortest amount and reversing
the wheel velocity if required.&lt;/li&gt;
&lt;li&gt;A low &lt;a href="https://en.wikipedia.org/wiki/Jerk_(physics)"&gt;jerk&lt;/a&gt; controller that ensures smooth movement
of the robot body and drive modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In future posts I will provide more details about the different controller and model algorithms.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Swerve drive - Moving a robot in all directions, mostly</title>
			<link>https://www.petrikvandervelde.nl/posts/Swerve-drive-introduction</link>
			<description>&lt;p&gt;Over the last year I have been using my &lt;a href="https://www.scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; as a way of
learning about robotics and all the related fields like mechanics and electronics. A major part of
this journey is the desire to design and build an autonomous mobile robot from the ground up.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Swerve-drive-introduction</guid>
			<pubDate>Sun, 04 Dec 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Over the last year I have been using my &lt;a href="https://www.scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; as a way of
learning about robotics and all the related fields like mechanics and electronics. A major part of
this journey is the desire to design and build an autonomous mobile robot from the ground up.&lt;/p&gt;
&lt;p&gt;My goal is to build an off-road capable robot that can navigate autonomously between different
locations to execute tasks either by itself or in cooperation with other robots. This is
quite an inspirational goal that involves quite a few robot different parts, a lot
of code and many hours of building and testing to achieve.&lt;/p&gt;
&lt;p&gt;The chassis of the robot will have four drive modules. Each module has one wheel attached that will
both be independently driven, and independently steerable. This configuration is called
&lt;code&gt;four wheel independent steering&lt;/code&gt; or &lt;code&gt;swerve drive&lt;/code&gt;. These kind of steering systems are used in
&lt;a href="https://en.wikipedia.org/wiki/Self-propelled_modular_transporter"&gt;heavy transport&lt;/a&gt;,
agriculture machines, &lt;a href="https://en.wikipedia.org/wiki/Curiosity_(rover)"&gt;mars rovers&lt;/a&gt; and
&lt;a href="https://www.chiefdelphi.com/t/best-frc-swerve-drive/399865"&gt;robot competitions&lt;/a&gt;. The advantages of
a swerve drive system are that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It provides a high degree of mobility. In a swerve drive direction of movement and orientation
are independent so the robot can face forwards while driving sideways. Additionally in a swerve drive
the &lt;a href="https://en.wikipedia.org/wiki/Instant_centre_of_rotation#:%7E:text=The%20instant%20center%20of%20rotation,a%20particular%20instant%20of%20time."&gt;Instantaneous Center of Rotation (ICR)&lt;/a&gt;
is not fixed to a specific line as it is with &lt;a href="https://en.wikipedia.org/wiki/Ackermann_steering_geometry"&gt;Ackermann steering&lt;/a&gt;
or &lt;a href="https://en.wikipedia.org/wiki/Differential_wheeled_robot"&gt;differential drive&lt;/a&gt;. This flexibility
allows the swerve drive to combine rotational movements with linear movements in ways that
other drive systems cannot.&lt;/li&gt;
&lt;li&gt;It has normal size wheels which provide a high carry capacity. While
&lt;a href="https://en.wikipedia.org/wiki/Omni_wheel"&gt;omni-wheels&lt;/a&gt; have the similar degree of freedom as a
swerve drive does, omni-wheels can often not carry the same load due to the lower carrying
capacity of the rollers that allow the omni-wheels to move sideways.&lt;/li&gt;
&lt;li&gt;It doesn't rely on wheel slip, as multi-wheel differential drive does. This means that it has
lower power demands, so more of the motor torque can be used to move the robot forward.&lt;/li&gt;
&lt;li&gt;It has the ability to traverse rough and dirty terrain due to the fact that all wheels are
driven as well as using normal wheels on each drive module. Omni-wheels and &lt;a href="https://en.wikipedia.org/wiki/Mecanum_wheel"&gt;mecanum wheels&lt;/a&gt;
face more issues in these environments due to dust and dirt clogging up the wheels as well as
having greater difficulty tackling obstacles.&lt;/li&gt;
&lt;li&gt;It is able to keep ground disturbance to a minimum as it is able to steer the robot while minimizing
sliding movement. Other drive systems, e.g. tracks or multi-axle differential drives, have a bigger
impact due to the sliding movement required for these systems to turn the robot.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course the swerve drive system isn't a magical system that only has advantages. There are also plenty
of disadvantages. For instance swerve drive systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are mechanically complicated. They require multiple motors per unit and multiple units per robot.
On top of that there are usually a number of mechanical components, gears and bearings, involved
in getting a working swerve drive.&lt;/li&gt;
&lt;li&gt;Need a complicated control system. Swerve systems are generally
over-determined, i.e. they have more degrees of freedom in the drive system, 2 per drive module,
than there are degrees of freedom in the robot, 2 translation directions and a rotation. This
means that all modules have to be synchronised at all times in order to prevent wheels from being
dragged along. The available degrees of freedom combined with the synchronisation demand means some
complicated math is required to make a swerve drive control work.&lt;/li&gt;
&lt;li&gt;Similar to the control side of the drive determining the position and velocity of the robot using
wheel &lt;a href="https://en.wikipedia.org/wiki/Odometry"&gt;odometry&lt;/a&gt; requires more complicated math. This is
due to the fact that the different drive modules don't necessarily agree with each other.&lt;/li&gt;
&lt;li&gt;Have more failure modes than other drive systems due to the fact that there are more moving parts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So with all these complications why would I try to build a swerve drive as my second robot and not
a differential drive robot or something similar. As pointed out previously there are good
reasons to use a swerve drive in an outdoor environment, i.e. high agility, good load capacity,
traction from all wheels, low ground impact. However the main reason I want to design and build a
swerve drive is because it is a challenge. Swerve drives are complicated and designing and building
one involves solving interesting problems in mechanical engineering, electrical engineering and
software engineering.&lt;/p&gt;
&lt;p&gt;There is currently no complete design for this robot yet, however there is a short list of design
decisions that have been made so far.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It will be a four wheel swerve drive robot. Swerve drives have been built with anything from
three wheels up, e.g. the Curiosity mars rover has 6 drive modules. The reason to use four modules
is that it will be symmetrical and still minimize the number of parts necessary.&lt;/li&gt;
&lt;li&gt;The software for the robot will be using &lt;a href="https://docs.ros.org/en/humble/index.html"&gt;ROS2 Humble&lt;/a&gt;.
Using ROS should provide me with a base framework and a lot of standard capabilities, like the
navigation stack, that I won't have to write myself. Additionally ROS has a decent simulation
environment that will allow me to test my code before putting it on a real robot.&lt;/li&gt;
&lt;li&gt;The hardware will be controlled using &lt;a href="https://control.ros.org/master/index.html"&gt;ROS2 controllers&lt;/a&gt;.
This will allow me to abstract the hardware so that I can better test the controller.&lt;/li&gt;
&lt;li&gt;The initial design will be an indoor model and about the same size as my SCUTTLE robot is. This will
simplify the initial design and allow me to compare with SCUTTLE.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The parts of the robot that I expect to be complicated and quite possibly show stoppers are the
software and the mechanical design. For the software the drive controller software, which translates
the requested velocity commands to motor commands for both the drive and steering motors, will be
complicated as it needs to make sure that all drive modules are the correct state. This piece of
software also needs to handle all the error conditions that occur.&lt;/p&gt;
&lt;p&gt;On the mechanical side I need to design the drive module such that it can drive the wheel forwards
and backwards while allowing &amp;lsquo;infinite&amp;rsquo; steering rotation. This will require a co-axial setup and a
bit of gearing. The second complicated part of the mechanical design is the inclusion of a suspension
system. Ideally the motors should be attached to the sprung side of the suspension system so that
they don't get exposed to excessive vibration. This however will complicate the mechanical design.&lt;/p&gt;
&lt;iframe
    style="float:left"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/fR47Y7p4mtQ"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;My plan is to work on the control software first. I can test that software using simulation and so
figure out if I can even make it work. I have created a simple URDF model that uses
ROS2 controllers to simulate a four wheel steering platform. This allowed me to learn more about
ROS2, ROS2 controls and how the interaction of those two with Gazebo works.&lt;/p&gt;
&lt;p&gt;At the moment I'm implementing a prototype for the controller in python so that I can use the model to
test if my algorithm works before I turn it into a proper ROS2 controller, which will need to be
written in C++.&lt;/p&gt;
&lt;p&gt;Once I have some kind of controlling software I am aiming to build a single drive module with a drive
motor, a steering motor and the mechanical assembly that allows a single wheel to be steered and driven.
I will use this module to work out both the details on the mechanical and software sides of the project.&lt;/p&gt;
&lt;p&gt;Once I have the controller and the drive module working properly I will build a simple robot, similar
in size to my SCUTTLE robot to further work on swerve drives. It will take me a little while
to get to that state though. In the mean time I will keep working on my design and documenting my
journey.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics - Fixing the wheel encoders</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-fixing-scuttles-encoder</link>
			<description>&lt;p&gt;While testing the &lt;a href="/posts/Robotics-a-bumper-for-scuttle-electronics"&gt;bumper&lt;/a&gt; for SCUTTLE I noticed
that when reversing SCUTTLE would start a turn instead of driving straight backwards due to one
motor turning faster than the other motor. There are a number of reasons this could be happening,
for instance the driver code isn't properly commanding the motors, or the encoders are returning
incorrect data, etc..&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-fixing-scuttles-encoder</guid>
			<pubDate>Tue, 15 Nov 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;While testing the &lt;a href="/posts/Robotics-a-bumper-for-scuttle-electronics"&gt;bumper&lt;/a&gt; for SCUTTLE I noticed
that when reversing SCUTTLE would start a turn instead of driving straight backwards due to one
motor turning faster than the other motor. There are a number of reasons this could be happening,
for instance the driver code isn't properly commanding the motors, or the encoders are returning
incorrect data, etc..&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Encoder output for failing encode" src="/assets/images/robotics/scuttle/scuttle-encoder-fail.png" /&gt;
&lt;figcaption&gt;Encoder output for failing encoder&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The first thing I did for my investigation was to make sure my encoders provide sensible data. The
non-ROS SCUTTLE code contains a useful &lt;a href="https://github.com/scuttlerobot/SCUTTLE/blob/ce82a52ad025408a15f23f19c58add1321253783/software/python/basics/L1_encoder.py"&gt;Python script&lt;/a&gt;
to measure the current value of the wheel encoders in a loop. I ran this code for a few minutes
while the wheels were stationary. In theory the results of this measurement should be two consistent
values, one for the left encoder and one for the right encoder. As you can see in the graph
it turns out that the angle measurement provided by the left encoder was very noisy.&lt;/p&gt;
&lt;p&gt;While a noisy encoder shouldn't by itself cause the incorrect reversing pattern, it will make it more
difficult to find the actual cause of the problem. So before I address the bumper reversing behaviour
I needed to fix the encoder noise. The two main reasons for noisy encoder data that I could think of
were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The encoder is broken in some way. SCUTTLE uses the
&lt;a href="https://nz.mouser.com/ProductDetail/ams-OSRAM/AS5048B-TS_EK_AB?qs=Rt6VE0PE%2FOduJIB%252BRfeBZQ%3D%3D"&gt;AS5048A position sensor&lt;/a&gt;
which is relatively robust, but can be broken if you send the I2c commands on the wrong pins. Which
could happen if you say ... put the connector on the wrong way when assembling your SCUTTLE ...&lt;/li&gt;
&lt;li&gt;The distance between the encoder and the magnet on the motor shaft isn't correct. The specifications
for the encoder state that the distance between the chip and the magnet on the motor shaft should
be between 0.5mm and 2.5mm, assuming a magnet of the recommended size and strength is used.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I tried measuring the distance between the encoder and the magnet, but that isn't easy. There is no way
to get a measuring tool near that space so I had to resort to indirect measurements. I measured the
left and right brackets. They were about 0.5mm difference in size. That doesn't seem much but it
would be enough to push the encoder out too far from the magnet.&lt;/p&gt;
&lt;p&gt;One solution would be to slightly sand the taller bracket so that both brackets are the size height.
In doing that I would have to be very careful to ensure that I sand the bracket flat, i.e. with
no change in angle in the face. A second solution is to remove the encoders from their brackets and
swap the brackets over.&lt;/p&gt;
&lt;figure style="float:right"&gt;
&lt;img alt="Encoder output for functioning encoders" src="/assets/images/robotics/scuttle/scuttle-encoder-fixed.png" /&gt;
&lt;figcaption&gt;Encoder output for functioning encoders&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As the latter is relatively easy to do that is the first solution I tried. I took the encoders
of their brackets and attached them to the other bracket. These brackets were put back on SCUTTLE and
the test script was executed once again. And this time both encoders gave me consistent results. I was
expecting the other encoder to go bad due to distance, but apparently both encoders are with in the
distance specification with this new configuration.&lt;/p&gt;
&lt;p&gt;With the encoder issue fixed I can get back to diagnosing the issue with the bumper reverse action.
My current suspicion is that the issue is caused by the fact that the the current SCUTTLE driver
code is written as an &lt;a href="https://en.wikipedia.org/wiki/Open-loop_controller"&gt;open loop&lt;/a&gt;. This means
that there is no feedback to the motor control software that indicates how fast the wheels are
actually turning in response to a given motor input. And because even motors of the same type
are all slightly different, they all react differently to the same motor input. In my case at
low speeds one of my motors responds earlier than the other motor. In the end this means that
at low speeds the bumper code thinks scuttle is driving backwards in a straight line while it
is actually driving around in circles.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics - Building a bumper for scuttle. The electronics</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-a-bumper-for-scuttle-electronics</link>
			<description>&lt;p&gt;The final part of building a bumper for SCUTTLE is to assemble the electronics component which translates
the movement of the bumper into signals which can be processed by the bumper software. In order to do
this I designed a simple circuit using &lt;a href="https://www.kicad.org/"&gt;KiCad&lt;/a&gt; and advice from one of the
many robotics forums.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-a-bumper-for-scuttle-electronics</guid>
			<pubDate>Fri, 21 Oct 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The final part of building a bumper for SCUTTLE is to assemble the electronics component which translates
the movement of the bumper into signals which can be processed by the bumper software. In order to do
this I designed a simple circuit using &lt;a href="https://www.kicad.org/"&gt;KiCad&lt;/a&gt; and advice from one of the
many robotics forums.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="SCUTTLE bumper electronics schematic" src="/assets/images/robotics/scuttle/scuttle-bumper-kicad.png" /&gt;
&lt;figcaption&gt;SCUTTLE bumper electronics schematic made in KiCad&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The circuit contains a microswitch as the trigger. The switch is connected to one of the 3.3V output
pins on the raspberry PI 4 board on one side and to one of the GPIO pins on the other side. When
the switch is depressed the circuit closes and the GPIO pin is driven to 3.3V, which is considered
a high signal. In order to ensure that the voltage on the GPIO pin is 0V when the circuit is not
closed I added a &lt;a href="https://en.wikipedia.org/wiki/Pull-up_resistor"&gt;pull down&lt;/a&gt; resistor. On the
raspberry PI it is possible to programmatically add a pull down resistor, however because I'm
using this as a learning exercise I thought it would be more suitable to include a physical pull down
resistor in the circuit.&lt;/p&gt;
&lt;p&gt;The next thing I wanted from the circuit was the ability to see if the circuit was closed or not, so
that when I'm debugging it is obvious if there is a problem with the power, electronics or software.
For this purpose I added a yellow LED to the circuit, which lights up when the circuit is closed.
Adding the LED then adds the requirement to protect it from over current in case that the input pin
was programmed to be an output pin by mistake. For this purpose I added a resistor next to the input
pin.&lt;/p&gt;
&lt;figure style="float:right"&gt;
&lt;img alt="SCUTTLE bumper electronics boards" src="/assets/images/robotics/scuttle/scuttle-bumper-electronics-boards.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE bumper electronics boards&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After testing the circuit on the breadboard the next step was to solder two switch boards and
a distribution board. Each switch board would have a micro-switch, the LED, the resistors and a
JST-XH three pin connector for power, ground and signal wires.
The distribution board would have four JST-XH three pin connectors and one JST-XH six pin connector.
The four three pin connectors would allow me to have a front bumper and a rear bumper, each
having a switch board on the left and the right. I used JST-XH connectors instead of the Dupont
connectors because the JST connectors are directional, thereby removing any potential issues with
plugging the connector in the wrong way.&lt;/p&gt;
&lt;p&gt;Part of this journey involved learning how to solder electronics components. I bought a Weller WE 1010
soldering station for this and future jobs. I managed to do a reasonable job soldering the parts but
it is obvious that I still have a lot to learn when it comes to soldering.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="SCUTTLE bumper electronics assembled" src="/assets/images/robotics/scuttle/scuttle-bumper-assembled.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE bumper electronics assembled&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The last task was to crimp the connectors and connect the boards. Because the raspberry Pi has a
Dupont header I needed to crimp both Dupont connectors and JST-XH connectors. For the Dupont connectors
I got an Iwiss SN-025 crimper. It works pretty well for those connectors. However the crimper dies are
too wide for the JST-XH connectors. So to crimp the JST-XH connectors I had to get a different
crimper with narrower dies. So for this I got an Engineer PAD-11 crimper. While this is not a
ratcheting crimper it works really well, at least for the JST-XH connectors.&lt;/p&gt;
&lt;p&gt;So now that the mechanical setup is done, the software is, mostly, done and the electronics have
been soldered and connected my bumper works. Sort of. It turns out that there is a
problems with my SCUTTLE that make the bump response only work partially. It seems that
there is an issue with the wheel encoders, which causes the software to not know how much one of the
wheels has rotated. I will discuss fixing the encoders in another post.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics - Building a bumper for scuttle. The software</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-a-bumper-for-scuttle-software</link>
			<description>&lt;p&gt;In my &lt;a href="/posts/Robotics-a-bumper-for-scuttle-overview"&gt;previous post&lt;/a&gt; I talked about creating a bump
sensor for my SCUTTLE robot. After creating the mechanical design I started working on the software.
There are two parts to the software, translating the state of the micro switches and turning the state
changes into movement commands for the robot, i.e. if the robot runs into
an obstacle it should stop and reverse its last movement. I decided to create a
&lt;a href="https://www.ros.org/"&gt;ROS&lt;/a&gt; node for each of these actions, i.e. one node
for the movement generation and one to translate the switch states. The reason for
creating two nodes instead of putting all the code into one node is that this allows me to run the
movement generation code both in a simulation and on the physical robot. So I gain the ability to
test more of my code, but I lose a bit of performance because the two nodes communicate using
messages which is slower than just using method calls.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-a-bumper-for-scuttle-software</guid>
			<pubDate>Fri, 09 Sep 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In my &lt;a href="/posts/Robotics-a-bumper-for-scuttle-overview"&gt;previous post&lt;/a&gt; I talked about creating a bump
sensor for my SCUTTLE robot. After creating the mechanical design I started working on the software.
There are two parts to the software, translating the state of the micro switches and turning the state
changes into movement commands for the robot, i.e. if the robot runs into
an obstacle it should stop and reverse its last movement. I decided to create a
&lt;a href="https://www.ros.org/"&gt;ROS&lt;/a&gt; node for each of these actions, i.e. one node
for the movement generation and one to translate the switch states. The reason for
creating two nodes instead of putting all the code into one node is that this allows me to run the
movement generation code both in a simulation and on the physical robot. So I gain the ability to
test more of my code, but I lose a bit of performance because the two nodes communicate using
messages which is slower than just using method calls.&lt;/p&gt;
&lt;p&gt;To test the bump sensor code I use &lt;a href="https://gazebosim.org/home"&gt;Gazebo&lt;/a&gt; to simulate how the bump
sensor would work. Gazebo has the ability to calculate collisions
and create &lt;a href="http://docs.ros.org/en/api/gazebo_msgs/html/msg/ContactsState.html"&gt;ContactsState&lt;/a&gt;
messages based on these collision calculations. So I created a third ROS node to translate these
Gazebo messages to my own &lt;a href="https://github.com/pvandervelde/scuttle_ros_msgs/blob/noetic/msg/BumperEvent.msg"&gt;bumper event messages&lt;/a&gt;.
With that I can test my obstacle response code in Gazebo which provides a more controlled environment
than the physical robot does.&lt;/p&gt;
&lt;p&gt;The first thing to do is to update the robot definition to include the bumper. This is done by adding
the &lt;a href="http://wiki.ros.org/urdf/XML/link"&gt;links&lt;/a&gt; and &lt;a href="http://wiki.ros.org/urdf/XML/joint"&gt;joints&lt;/a&gt; that
make up my bumper to an &lt;a href="https://github.com/pvandervelde/scuttle_bumper/blob/master/urdf/bumper.xacro"&gt;URDF file&lt;/a&gt;.
This URDF file is linked to the main &lt;a href="https://github.com/scuttlerobot/scuttle_description"&gt;model description&lt;/a&gt;
for SCUTTLE. After that I added the information for the &lt;a href="https://classic.gazebosim.org/tutorials?tut=ros_gzplugins#Bumper"&gt;Gazebo bumper sensor&lt;/a&gt;.
In the URDF file this looks as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    &amp;lt;gazebo reference=&amp;quot;front_bumper_plate_left_link&amp;quot;&amp;gt;
        &amp;lt;sensor name=&amp;quot;front_bumper_left&amp;quot; type=&amp;quot;contact&amp;quot;&amp;gt;
            &amp;lt;selfCollide&amp;gt;true&amp;lt;/selfCollide&amp;gt;
            &amp;lt;alwaysOn&amp;gt;true&amp;lt;/alwaysOn&amp;gt;
            &amp;lt;updateRate&amp;gt;15.0&amp;lt;/updateRate&amp;gt;
            &amp;lt;material&amp;gt;Gazebo/WhiteGlow&amp;lt;/material&amp;gt;
            &amp;lt;contact&amp;gt;
                &amp;lt;collision&amp;gt;base_link_fixed_joint_lump__front_bumper_plate_left_cl_collision_3&amp;lt;/collision&amp;gt;
                &amp;lt;topic&amp;gt;bumper_contact&amp;lt;/topic&amp;gt;
            &amp;lt;/contact&amp;gt;
            &amp;lt;plugin name=&amp;quot;gazebo_ros_bumper_controller_front_left&amp;quot; filename=&amp;quot;libgazebo_ros_bumper.so&amp;quot;&amp;gt;
                &amp;lt;bumperTopicName&amp;gt;scuttle_bumper&amp;lt;/bumperTopicName&amp;gt;
                &amp;lt;frameName&amp;gt;front_bumper_plate_left_link&amp;lt;/frameName&amp;gt;
            &amp;lt;/plugin&amp;gt;
        &amp;lt;/sensor&amp;gt;
    &amp;lt;/gazebo&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I found that defining a contact sensor for Gazebo requires getting the link ID correct. In order to
do so you need to follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The bumper contact information needs to be defined inside a &lt;a href="https://classic.gazebosim.org/tutorials?tut=ros_urdf&amp;amp;cat=connect_ros"&gt;&lt;code&gt;gazebo&lt;/code&gt;&lt;/a&gt;
element. This &lt;code&gt;gazebo&lt;/code&gt; element should have a &lt;code&gt;reference&lt;/code&gt; attribute that points to the link that is
to be used as the bumper surface.&lt;/li&gt;
&lt;li&gt;The name for the &lt;code&gt;collision&lt;/code&gt; element needs to be found after translation to the SDF format. Normally
this is something Gazebo does internally. In this case you'll need to do this manually. If your
geometry is defined in an &lt;a href="http://wiki.ros.org/xacro"&gt;xacro&lt;/a&gt; file then you first need to translate
this to URDF using the &lt;code&gt;rosrun xacro xacro --inorder -o model.urdf model.urdf.xacro&lt;/code&gt; command. After
that you can translate the URDF to SDF with the &lt;code&gt;gz sdf -p scuttle.urdf &amp;gt; scuttle.sdf&lt;/code&gt; command. Once
you have the SDF file you can search for the
&lt;a href="https://answers.gazebosim.org/question/21992/what-collision-name-is-supposed-to-be-passed-to-contact-sensor/"&gt;correct element IDs&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the above example the collision name, &lt;code&gt;base_link_fixed_joint_lump__front_bumper_plate_left_cl_collision_3&lt;/code&gt;,
is the one you need to extract from the SDF file. Note that it can change if you make changes to
your xacro / URDF file, so in that case you will need to extract it again.&lt;/p&gt;
&lt;p&gt;Once you have defined the URDF Gazebo will generate contact messages when the bumper hits an object.
These contact messages contain information describing where the contact occurred on the geometry mesh,
so in theory I could have used this information to determine if the contact would trigger the left
limit switch or the right limit switch or both. However that requires a decent amount of calculations
which is both work and introduces the potential for errors. So I opted to split the bumper into
two parts, one for the left side and one for the right side. If anything contacts anywhere on the
left side I consider that a trigger for the left part of the bumper and similar for the right side.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="SCUTTLE with bumper in RViz" src="/assets/images/robotics/scuttle/scuttle-with-bumper-in-rviz.png" /&gt;
&lt;figcaption&gt;SCUTTLE with bumper in RViz&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;With the URDF work done I started writing the code for the different ROS nodes. First
the &lt;a href="https://github.com/pvandervelde/scuttle_bumper/blob/master/src/gazebo_contact_sensor_translator.py"&gt;gazebo translator node&lt;/a&gt;.
This node subscribes to the &lt;a href="http://docs.ros.org/en/api/gazebo_msgs/html/msg/ContactsState.html"&gt;ContactsState&lt;/a&gt;
messages that Gazebo sends when the bumper geometry collides with something. These messages are then
translated to a &lt;a href="https://github.com/pvandervelde/scuttle_ros_msgs/blob/noetic/msg/BumperEvent.msg"&gt;BumperEvent&lt;/a&gt;
message for further processing.&lt;/p&gt;
&lt;p&gt;One interesting observation about the Gazebo contact messages are that they exhibit something
similar to &lt;a href="https://www.pcmag.com/index.php/encyclopedia/term/switch-bounce"&gt;switch bounce&lt;/a&gt;, in other
words it seems that the contact is intermittent even if the bumper plate is in solid contact with
the object. I'm guessing that this is caused by the fact that calculating collisions between moving
surfaces is difficult. In the end it doesn't matter what causes this behaviour though because we
need to &lt;a href="https://github.com/pvandervelde/scuttle_bumper/blob/master/src/debounce.py"&gt;deal with it&lt;/a&gt; in
some sensible way.&lt;/p&gt;
&lt;p&gt;Once the bumper event messages have been generated they are processed by the
&lt;a href="https://github.com/pvandervelde/scuttle_bumper/blob/master/src/bumper_navigator.py"&gt;second node&lt;/a&gt;.
This node subscribes to both odometry events and bumper events. Internally it keeps track of the
motion state of SCUTTLE. On reception of a bumper event the node sends a
&lt;a href="http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/Twist.html"&gt;velocity command&lt;/a&gt;
to stop the robot. Once the robot has stopped it sends commands for it to reverse course far enough
that it is no longer contact with the obstacle.&lt;/p&gt;
&lt;p&gt;One thing to note is that while the bumper code is working to reverse course after hitting an obstacle,
the other parts of ROS, e.g. the navigation stack, are probably still trying to steer the robot in the
original direction because those parts don't know about the obstacle. After all it is not on the
navigation map. This results in many different velocity commands being send, which could be very
confusing for the robot. One design decision was to make the bumper code unaware of other nodes. This
was done because allowing the bumper code to supress velocity
commands from other nodes would imply that the bumper code is always the most important publisher
when it comes to velocity commands. There are cases where this isn't true, e.g. when using
the bumpers to park SCUTTLE against a specific object like in the case of a docking station.&lt;/p&gt;
&lt;p&gt;To ensure that a consistent set of velocity commands are sent to the motors I used the
&lt;a href="http://wiki.ros.org/twist_mux"&gt;twist multiplexer node&lt;/a&gt; which takes in velocity commands from
many nodes and forwards them using a priority-based scheme. In the current implementation the
priorities are, from low to high&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Navigation&lt;/li&gt;
&lt;li&gt;Keyboard&lt;/li&gt;
&lt;li&gt;Bumper&lt;/li&gt;
&lt;li&gt;Joypad / Joystick&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe
    style="float:right"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/SddexyGTJ0M"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;By using the twist multiplexer mode it is easy for users to change the priority by changing a configuration
file instead of having to change the bumper code.&lt;/p&gt;
&lt;p&gt;With that the bumper code is able to make SCUTTLE respond to objects it bumps into. The next step in
the process of adding bump sensors to SCUTTLE is to assemble the electronic components so that the
movement of the bumper can be registered and reported to the software components.&lt;/p&gt;
&lt;p&gt;Finally a addition to the software that needs to be made is the ability to add the obstacles in the
navigation map so that SCUTTLE can navigate around the newly discovered obstacles.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics - Building a bumper for scuttle. The overview</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-a-bumper-for-scuttle-overview</link>
			<description>&lt;p&gt;In order to allow my SCUTTLE robot to drive around autonomously it needs some sensors so that it
can &lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;perceive its surroundings&lt;/a&gt;.
One of the ways in which this is normally achieved is by adding a &lt;a href="https://en.wikipedia.org/wiki/Lidar"&gt;LIDAR&lt;/a&gt;
unit. Unfortunately I don't have one of those, and LIDAR units are relatively expensive in New Zealand.
Additionally I figured I'd learn more about robot sensors if I set up some simple sensors for my
SCUTTLE robot before getting a LIDAR.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-a-bumper-for-scuttle-overview</guid>
			<pubDate>Sat, 27 Aug 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;In order to allow my SCUTTLE robot to drive around autonomously it needs some sensors so that it
can &lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;perceive its surroundings&lt;/a&gt;.
One of the ways in which this is normally achieved is by adding a &lt;a href="https://en.wikipedia.org/wiki/Lidar"&gt;LIDAR&lt;/a&gt;
unit. Unfortunately I don't have one of those, and LIDAR units are relatively expensive in New Zealand.
Additionally I figured I'd learn more about robot sensors if I set up some simple sensors for my
SCUTTLE robot before getting a LIDAR.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="CAD model of the SCUTTLE bumper" src="/assets/images/robotics/scuttle/scuttle-bumper-cad-model.png" /&gt;
&lt;figcaption&gt;The CAD model of the SCUTTLE bumper&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;So I decided to start with the simplest kind of sensor, the bump sensor. This type of sensor
consists of one or more &lt;a href="https://en.wikipedia.org/wiki/Limit_switch"&gt;limit switches&lt;/a&gt; and a bumper
surface, i.e. some kind of plate that bumps into things. This plate is held in place by springs thereby
giving it the ability to move while also returning to its original position.
When the robot bumps into an object with the plate, the springs are compressed and the plate hits
the limit switches. These switches then signal the robot that it has hit something. At that point the
robot can stop its motion and back away from the obstacle. As the robot backs away the springs push the
bumper plate away from the limit switches. The change in switch state then signals to the robot that
it is no longer in contact with the obstacle and can thus continue on its journey.&lt;/p&gt;
&lt;p&gt;Overall this doesn't sound very complicated so it should be pretty quick and easy to build the
bump sensor ...&lt;/p&gt;
&lt;p&gt;Several weeks down the line I nearly finished my bump sensors&lt;/p&gt;
&lt;p&gt;The first part of my build was the design of the physical parts that I need to attach to SCUTTLE. I
build the bumper with simple parts because I don't have access to a workshop or a 3D printer. So
the bumper is a UHMWPE plate reinforced by some 90 degree aluminium extrusions. This prevents the
plate from flexing when it contacts an obstacle. The plate is attached to two angle brackets, one on
the left side and one on the right side. It is attached using four M3 bolts wrapped with springs,
2 bolts for each side. The idea behind using 2 bolts per side is that this should provide some
rotational stability while keeping the construction simple. Finally the angle brackets is attached
to the scuttle frame with two T slot nuts.&lt;/p&gt;
&lt;p&gt;I found that the springs that keep the plate in the extended position have to be quite soft, otherwise
SCUTTLE won't be able to compress the springs when it hits something at low speed. The first
set of springs I got from Amazon were too stiff. For the second set of springs I got some ballpoint
pen springs of Aliexpress which are much less stiff.&lt;/p&gt;
&lt;figure style="float:right"&gt;
&lt;img alt="SCUTTLE with its new bumper" src="/assets/images/robotics/scuttle/scuttle-with-bumper.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE with its new bumper&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This first version of the bumper mechanics works relatively well but for the next version there
are some things I want to change&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The bolts have full thread on them which sometimes causes the bolts to get hung up on the tread. I
had some issues finding partially threaded bolts with the right length and diameter.&lt;/li&gt;
&lt;li&gt;The current design allows left to right tilt which is good, but also top to bottom tilt. The latter
is not so good as it potentially allows SCUTTLE to hit an obstacle without noticing. A better design
would allow left to right rotation of the bumper but little to no top to bottom rotation. This
allows the left and right limit switches to be triggered individually when SCUTTLE hits an obstacle
on the left or right side respectively while ensuring that the limit switches are always triggered
if the bumper hits something, even if the obstacle is low to the ground.&lt;/li&gt;
&lt;li&gt;The current bumper shape is a flat plate, it would be better if the bumper plate was curved. If the
curved bumper is given the right radius it would allow SCUTTLE to rotate in place after hitting
an obstacle without getting stuck. Obviously creating a curved bumper will increase construction
effort.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The mechanical design of the bumper is only one part of the task. In order for SCUTTLE to be able
to respond to hitting an object I need to have have software that can respond to signals from the
limit switches and the appropriate electronics that transfers the limit switch state to the
raspberry pi. The electronics and the software that I created for this bumper will be described in
separate posts.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics - Driving scuttle with ROS - Gazebo simulation</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-driving-scuttle-with-ros-gazebo-simulation</link>
			<description>&lt;p&gt;After getting familiar with &lt;a href="/posts/Robotics-learning-ros"&gt;ROS&lt;/a&gt;, the next step was to get the
navigation stack working in Gazebo for the SCUTTLE robot. Fortunately, the SCUTTLE developers
had already created a number of ROS packages containing the
&lt;a href="https://github.com/scuttlerobot/scuttle_description"&gt;scuttle model&lt;/a&gt;,
&lt;a href="https://github.com/scuttlerobot/scuttle_bringup"&gt;the startup scripts&lt;/a&gt; and the
&lt;a href="https://github.com/scuttlerobot/scuttle_driver"&gt;driver code&lt;/a&gt;. All the bits you need to drive SCUTTLE
around using ROS.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-driving-scuttle-with-ros-gazebo-simulation</guid>
			<pubDate>Wed, 01 Jun 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;After getting familiar with &lt;a href="/posts/Robotics-learning-ros"&gt;ROS&lt;/a&gt;, the next step was to get the
navigation stack working in Gazebo for the SCUTTLE robot. Fortunately, the SCUTTLE developers
had already created a number of ROS packages containing the
&lt;a href="https://github.com/scuttlerobot/scuttle_description"&gt;scuttle model&lt;/a&gt;,
&lt;a href="https://github.com/scuttlerobot/scuttle_bringup"&gt;the startup scripts&lt;/a&gt; and the
&lt;a href="https://github.com/scuttlerobot/scuttle_driver"&gt;driver code&lt;/a&gt;. All the bits you need to drive SCUTTLE
around using ROS.&lt;/p&gt;
&lt;p&gt;The first challenge is to drive the SCUTTLE model around in &lt;a href="https://gazebosim.org/"&gt;Gazebo&lt;/a&gt;
using the &lt;a href="http://wiki.ros.org/teleop_twist_keyboard"&gt;keyboard&lt;/a&gt;.
This was easily achieved using Gazebo and RViz using WSL2, but it was a bit slow.&lt;/p&gt;
&lt;iframe
    style="float:right"
    width="560"
    height="315"
    src="https://www.youtube.com/embed/TI9tfzn8yXE"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"&gt;
&lt;/iframe&gt;
&lt;p&gt;The next challenge was to make the virtual SCUTTLE drive to a specific point using a custom
ROS node. While doing the &lt;a href="https://www.udemy.com/course/ros-essentials/"&gt;ROS for beginners I&lt;/a&gt; course
I wrote some code to do a move-to-goal for the &lt;a href="http://wiki.ros.org/turtlesim"&gt;turtlesim&lt;/a&gt; virtual
robot. The &lt;a href="https://gist.github.com/pvandervelde/35200cce52d416d899c3db600c98a4a5"&gt;code&lt;/a&gt; was updated
and then used for the SCUTTLE robot. The result is a robot that moves to a goal, mission accomplished!&lt;/p&gt;
&lt;p&gt;The final challenge, for now at least, is to get the navigation stack to work for SCUTTLE. In order
to do so a few new packages needed to be created. The packages I created are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/scuttlerobot/scuttle_slam"&gt;scuttle_slam&lt;/a&gt; - Contains the configurations and
launch files for running &lt;a href="http://wiki.ros.org/gmapping"&gt;gmapping&lt;/a&gt; SLAM using the SCUTTLE LIDAR.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scuttlerobot/scuttle_navigation"&gt;scuttle_navigation&lt;/a&gt; - Contains the
configuration for working with the &lt;a href="http://wiki.ros.org/navigation"&gt;navigation stack&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scuttlerobot/scuttle_gazebo"&gt;scuttle_gazebo&lt;/a&gt; - Contains a number of Gazebo
worlds for testing our virtual SCUTTLE.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;scuttle_slam&lt;/code&gt; and &lt;code&gt;scuttle_navigation&lt;/code&gt; packages are based on the similar packages for
&lt;a href="https://github.com/ROBOTIS-GIT/turtlebot3"&gt;turtlebot3&lt;/a&gt; with adjustments so that the configuration
matches SCUTTLE's performance.&lt;/p&gt;
&lt;p&gt;For navigation in ROS1, you need two different types of path planners and a map for each planner. The
first type is the global planner, which uses a map to determine the fastest path from the current
location to the goal location. The second type, called the local planner, navigates the robot to
the goal by trying to follow the path created by the global planner. The path followed by the local
planner may deviate from the global path due to previously unknown obstacles and limitations of the
robot, e.g. the ability to follow a turn. The map for each planner is known as a &lt;a href="http://wiki.ros.org/costmap_2d"&gt;costmap&lt;/a&gt;
which indicates which part of the surroundings are occupied by obstacles and which parts can be
navigated. After configuring the planners and the costmaps, the navigation worked. I could use
RViz to set a point on the map and the virtual SCUTTLE would navigate to that location automatically.
Success!&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Scuttle navigating in RViz" src="/assets/images/robotics/scuttle/scuttle-navigate-in-rviz.png" /&gt;
&lt;figcaption&gt;SCUTTLE robot navigating a room in RViz&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Now, not everything was fine. The &lt;a href="https://answers.ros.org/question/397737/dwa-local-planner-cant-find-a-trajectory-unless-rotate-recovery-runs"&gt;first issue&lt;/a&gt;
is some weird behaviour with the &lt;a href="http://wiki.ros.org/dwa_local_planner?distro=noetic"&gt;DWA local planner&lt;/a&gt;.
Once the robot is moving the local planner mostly does a good job. However, when starting a path it
takes a while for the local planner to pick up the global path. In fact, the DWA planner doesn't seem
to &lt;a href="https://www.youtube.com/watch?v=Nt9XyJHzfas&amp;amp;ab_channel=PatrickvanderVelde"&gt;accept the global plan&lt;/a&gt;
until after a &lt;a href="http://wiki.ros.org/rotate_recovery?distro=noetic"&gt;rotate recovery&lt;/a&gt; has taken place.
So far, I haven't found a solution to this problem.&lt;/p&gt;
&lt;p&gt;The second issue is that, in some cases, the navigation stack fails to find a path out of a narrow
hallway or though a narrow door. In general, this happens when exploring a location, e.g. using the
&lt;a href="http://wiki.ros.org/explore_lite"&gt;explore_lite&lt;/a&gt; package. It seems that the algorithm
can't find a turn that will rotate the robot in the available space, even though scuttle is
able to perform in-place rotations. At the start of a navigation exercise, in-place rotations are
gladly used. However once the robot is on the move, the algorithm doesn't seem to apply in-place
rotations anymore.&lt;/p&gt;
&lt;p&gt;Finally, you have to keep in mind that the default planners for ROS are path planners. This means that
they plan a path from the start to the destination. These paths, however, only describe the direction
a robot should take at a given location. They don't describe velocity or acceleration. Only
describing the direction can generate paths with abrupt turns that force the robot to slow
down significantly. Using a trajectory planner, which at least prescribes velocities, makes for a
smoother experience for robot and cargo.&lt;/p&gt;
&lt;h4&gt;Edits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;October 26th 2022: Added the &lt;code&gt;Gazebo&lt;/code&gt; tag.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics - Learning Robot Operating System (ROS)</title>
			<link>https://www.petrikvandervelde.nl/posts/Robotics-learning-ros</link>
			<description>&lt;p&gt;As part of my &lt;a href="tags/Robotics"&gt;journey into robotics&lt;/a&gt; I have been working on updating my
&lt;a href="https://www.scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; to use the
&lt;a href="http://wiki.ros.org/"&gt;Robot Operating System (ROS)&lt;/a&gt;. ROS provides a number of different things
that make robot development much easier. The main items are a middleware layer for communication
between different parts of the robot, hardware abstractions for different sensors, motors and
controllers, device drivers and many other libraries and packages.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Robotics-learning-ros</guid>
			<pubDate>Tue, 03 May 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As part of my &lt;a href="tags/Robotics"&gt;journey into robotics&lt;/a&gt; I have been working on updating my
&lt;a href="https://www.scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; to use the
&lt;a href="http://wiki.ros.org/"&gt;Robot Operating System (ROS)&lt;/a&gt;. ROS provides a number of different things
that make robot development much easier. The main items are a middleware layer for communication
between different parts of the robot, hardware abstractions for different sensors, motors and
controllers, device drivers and many other libraries and packages.&lt;/p&gt;
&lt;p&gt;The main benefit of using ROS is that it provides a lot of integrations and functionality that you
can quickly use. On the other hand the drawback that comes with all of this is that the learning curve
for ROS is very steep. The documentation is pretty good and so are the tutorials, however there are
a lot of different parts in ROS, which makes for a lot of interesting ways to get confused. So to
speed up my progress with ROS I decided to do the &lt;a href="https://www.udemy.com/course/ros-essentials/"&gt;ROS for beginners I&lt;/a&gt;
and &lt;a href="https://www.udemy.com/course/ros-navigation/"&gt;II courses&lt;/a&gt; on Udemy. These courses were very helpful
to reduce the learning curve for ROS and quickly get me familiar with ROS.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Scuttle in Gazebo" src="/assets/images/robotics/scuttle/scuttle-in-gazebo.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot in Gazebo&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This post won't explain how ROS works, there are
&lt;a href="https://www.google.com/search?q=getting+started+with+ros&amp;amp;rlz=1C1CHBF_enNZ825NZ825&amp;amp;oq=getting+started+with+ros&amp;amp;aqs=chrome..69i57j69i61.2633j0j7&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8"&gt;many, many, many tutorials out on the web&lt;/a&gt;
that will do a far better job than I can. However I do want to share some of the things I learned
from working with ROS.&lt;/p&gt;
&lt;p&gt;The first thing to note is the operating system on which you want to run ROS. ROS is developed
to be run on Ubuntu. My home PC runs on the Windows Operating System. ROS 1 wasn't designed to run
directly on Windows (ROS2 will be able to) but there are several ways to run it. First you can run
ROS Noetic straight on Windows using &lt;a href="https://robostack.github.io/"&gt;Robostack&lt;/a&gt;. This uses the Conda
package manager and provides packages for all operating systems. I found that this works moderately
well, there are a number of packages missing and occasionally things error out. This approach works
well for simple learning exercises but may yet not be suitable for large ROS applications.&lt;/p&gt;
&lt;p&gt;A second approach is to &lt;a href="https://ishkapoor.medium.com/how-to-install-ros-noetic-on-wsl2-9bbe6c00b89a"&gt;run ROS on WSL2&lt;/a&gt;.
This is able to run the Ubuntu native packages so you can run all parts of ROS and with the help of
an XServer like &lt;a href="https://sourceforge.net/projects/vcxsrv/"&gt;VcXsrv&lt;/a&gt; you can even run all the graphical
tools. One thing to keep in mind if you use WSL is that networking may cause problems if you run
ROS distributed over more than one computing device, e.g. a laptop and a physical robot. With WSL
there is no easy way to expose WSL applications to uninitiated network connections, i.e.
a request started from inside WSL works, but a request started from outside WSL won't be able to
connect. This is important because ROS nodes need to be able to communicate with each other freely.
The result will be that the nodes on the WSL side will seem to be connected and functional while the
other nodes won't be able to send messages to the WSL nodes.&lt;/p&gt;
&lt;p&gt;The final approach to running ROS is to create an &lt;a href="https://gist.github.com/pvandervelde/2282dafc080945ecb7981edb740ed47c"&gt;Ubuntu VM&lt;/a&gt;
or physical machine. In this case as long as the machine is reachable over the network for other
compute devices, it is possible to run ROS distributed over the network. This is the way I currently
run ROS.&lt;/p&gt;
&lt;figure style="float:right"&gt;
&lt;img alt="Scuttle in RViz" src="/assets/images/robotics/scuttle/scuttle-in-rviz-no-sensors.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot in RViz&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Once you have a working ROS installation the next thing you'll find out is that ROS configurations
can be difficult to get right, especially when you're working with a physical robot where visibility
of what is going on may not be the best. There are a number of useful tools available to provide
insights into what is going on with your robot.&lt;/p&gt;
&lt;p&gt;The first tool is &lt;a href="https://gazebosim.org/"&gt;Gazebo&lt;/a&gt; which provides a simulated environment for
ROS robots. The simulation is based on a physics engine with good accuracy of real world physics. It
also provides models for sensors, like LIDAR and cameras, and sensor noise to simulate real-world
sensor behaviour. Having a simulated environment allows you to repeat behaviours many times in
the same way in rapid succession. Having a way to easily repeat behaviours and control the environment
means that you can quickly test and debug specific behaviours, something which can be much more difficult
with a physical robot.&lt;/p&gt;
&lt;p&gt;The second tool, &lt;a href="http://wiki.ros.org/rviz"&gt;RViz&lt;/a&gt;, provides visualization of the environment of the
robot and how the robot perceives that environment. It allows you to visualize what the robot can
&amp;lsquo;see&amp;rsquo;. RViz works by subscribing to the different message topics that are available. This means
it works both for simulated robots (using Gazebo) and physical robots.&lt;/p&gt;
&lt;p&gt;The final tool worth discussing is &lt;a href="https://foxglove.dev/"&gt;Foxglove studio&lt;/a&gt; which also provides
insight into the data that the robot generates, both from sensors but also in the form of messages
sent between the different components of the robot. One of the nice features of Foxglove is that
you can make plots with values provided by messages. For instance you can plot the velocity
components of a &lt;a href="http://docs.ros.org/en/lunar/api/geometry_msgs/html/msg/Twist.html"&gt;Twist message&lt;/a&gt;.
This is useful to compare requested velocities compared to actual achieved velocities.
Another great feature of Foxglove is that it is able to display the &lt;a href="http://wiki.ros.org/rosout"&gt;ROS logs&lt;/a&gt;
and it also allows you to filter and search these logs. Given that ROS logs can quickly become
large the ability to filter is very useful.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Scuttle in RViz with LIDAR overlay" src="/assets/images/robotics/scuttle/scuttle-in-rviz-slam-enabled.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot in RViz with LIDAR overlay&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;When working with a mobile robot, like I am, getting the robot to navigate a space is often one of
the first achievable goals. The &lt;a href="http://wiki.ros.org/navigation"&gt;navigation stack&lt;/a&gt; in ROS provides
a lot of the basic capabilities to get started with robot navigation in a reasonable time span. Do
note however that the navigation stack in ROS is fairly large and has a lot of different configuration
options so it is wise to set some time aside for learning about the different options. I'll talk about
navigating with SCUTTLE in a future post.&lt;/p&gt;
&lt;p&gt;As mentioned I started learning ROS1 with Udemy. My goal for learning ROS was to use it for
navigation with my SCUTTLE  robot, more on that in a future post. Once I manage to get navigation
working for SCUTTLE I plan to start adding different sensors. Finally I want to enable task planning
for SCUTTLE, e.g. tasks like &amp;ldquo;drive to the living room and collect my coffee cup and bring it back
to me&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Another part of my plans is to upgrade to using &lt;a href="https://docs.ros.org/en/galactic/index.html"&gt;ROS2&lt;/a&gt;.
ROS1 end-of-life is 2025, which is only 3 years away, and additionally ROS2 has a more modern stack
with python 3, better communication security, an improved navigation stack and more active development.
More on this will follow in a future post once I have upgraded my robot to ROS2&lt;/p&gt;
&lt;h4&gt;Edits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;October 26th 2022: Added the &lt;code&gt;Gazebo&lt;/code&gt; tag.&lt;/li&gt;
&lt;/ul&gt;
</content:encoded>
		</item>
		<item>
			<title>Starting robotics with the SCUTTLE robot</title>
			<link>https://www.petrikvandervelde.nl/posts/Starting-robotics-with-scuttle</link>
			<description>&lt;p&gt;As mentioned in my last post I have started tinkering with mobile robots. My current goal is to
build an outdoor capable &lt;a href="https://en.wikipedia.org/wiki/Autonomous_robot"&gt;autonomous mobile robot&lt;/a&gt;.
The first problem I have to solve in order to move towards my goal is that I know a decent amount
about software, a reasonable amount about structures and mechanics and very little about electronics.
Oh and I know nothing about the robotics algorithms like how navigation works, the fact that robots
may have a hard time
&lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;figuring out where they are&lt;/a&gt;
and that decision making is hard for robots.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Starting-robotics-with-scuttle</guid>
			<pubDate>Fri, 18 Mar 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;As mentioned in my last post I have started tinkering with mobile robots. My current goal is to
build an outdoor capable &lt;a href="https://en.wikipedia.org/wiki/Autonomous_robot"&gt;autonomous mobile robot&lt;/a&gt;.
The first problem I have to solve in order to move towards my goal is that I know a decent amount
about software, a reasonable amount about structures and mechanics and very little about electronics.
Oh and I know nothing about the robotics algorithms like how navigation works, the fact that robots
may have a hard time
&lt;a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"&gt;figuring out where they are&lt;/a&gt;
and that decision making is hard for robots.&lt;/p&gt;
&lt;p&gt;So in order to not have to learn all the things at the same time I decided it would be sensible to
start off buying a kit that I could assemble and learn to work with. The basic requirements were&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Something that didn't require me to solder electronics or 3d print parts, because I have neither
of those tools, yet ...&lt;/li&gt;
&lt;li&gt;Capable of actually carrying a load of some sort. Most robot kits are fun platforms to play with
but other than driving around they're not actually capable of carrying things. I want my robot to
be able to carry things for me.&lt;/li&gt;
&lt;li&gt;With accessible hardware and software so that I could modify and extend it.&lt;/li&gt;
&lt;li&gt;Affordable, because money is still finite&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="float:right"&gt;
&lt;img alt="Scuttle assembled" src="/assets/images/robotics/scuttle/scuttle-assembled.jpg" /&gt;
&lt;figcaption&gt;SCUTTLE robot assembled&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After a little bit of searching I decided to buy the &lt;a href="https://scuttlerobot.org/"&gt;SCUTTLE robot&lt;/a&gt; kit.
The SCUTTLE kit is an &lt;a href="https://github.com/scuttlerobot"&gt;open source kit&lt;/a&gt; for which all the build
information is available online, from the &lt;a href="https://grabcad.com/library/scuttle-robot-v2-3-1"&gt;3D drawings&lt;/a&gt;
to the material BOM. Additionally there is a lot of sample code that makes it easy to get going with
the robot. There are code samples that allow you to drive the robot with a gamepad or by putting it in
follow mode where it follows a coloured object. Note that when you pick a coloured object apparently
orange is the best colour because of the colour difference with the surroundings. In my case initially
I picked a dark red object in a poorly lit environment with lots of other variations of red around.
You can probably imagine how well that went ...&amp;hellip; [*]&lt;/p&gt;
&lt;p&gt;Assembly of the SCUTTLE robot is pretty easy, it consists of aluminium T-slot lengths, some 3D
printed parts and some electronics parts. The T-slot lengths are fastened with angle brackets and
the 3D printed parts bolt to the T-slot lengths. The kit I bought only required connecting electronic parts
with connectors, no soldering required. If you build a SCUTTLE from scratch there is some soldering to
be done.&lt;/p&gt;
&lt;p&gt;Once you have assembled your SCUTTLE you can test the functionality by using the code samples to
verify the encoders and the motors. Note that it is wise to review your cabling before turning anything
on because it is possible to connect some of the electronics incorrectly. I ended up breaking my
Raspberry Pi, quite possibly by connecting the encoders backwards or something similar.&lt;/p&gt;
&lt;figure style="float:left"&gt;
&lt;img alt="Scuttle in RViz" src="/assets/images/robotics/scuttle/scuttle-rviz.png" /&gt;
&lt;figcaption&gt;SCUTTLE in RViz&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After verifying that the motors rotate in the correct direction you can try controlling the
robot via the gamepad and drive it around the house.&lt;/p&gt;
&lt;p&gt;Currently I'm working to update my SCUTTLE with the &lt;a href="https://www.ros.org/"&gt;ROS&lt;/a&gt; software. Currently
I'm testing with ROS noetic but I am looking to eventually switch to using ROS2 as it seems to have
a more flexible navigation stack. More on that in a future post.&lt;/p&gt;
&lt;p&gt;I'm also planning to add some sensors to my SCUTTLE to make it a bit more autonomous. The first
plan is to add a bumper that will tell the robot if it has hit something. I have picked up some
contact switches but am still thinking about the design for the bumper. Later on I want to add
sonar, &lt;a href="https://en.wikipedia.org/wiki/Time-of-flight_camera"&gt;Time of Flight (ToF) sensors&lt;/a&gt; and
potentially cameras as well.&lt;/p&gt;
&lt;p&gt;[*] SCUTTLE drove straight at the red coloured couch instead of following the red object I wanted
it to follow&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Building an autonomous mobile robot - But why?</title>
			<link>https://www.petrikvandervelde.nl/posts/Building-autonomous-mobile-robot-why</link>
			<description>&lt;p&gt;Over the last decade I have been developing software of all kinds. I have coded both for work and
for my own learning and entertainment. The majority of the coding I did over that period was code
that lived only in the virtual world. Things from from numerical simulations to building software
infrastructure.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Building-autonomous-mobile-robot-why</guid>
			<pubDate>Mon, 07 Mar 2022 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;Over the last decade I have been developing software of all kinds. I have coded both for work and
for my own learning and entertainment. The majority of the coding I did over that period was code
that lived only in the virtual world. Things from from numerical simulations to building software
infrastructure.&lt;/p&gt;
&lt;p&gt;While I have always greatly enjoyed writing code and learning new skills I have found myself being
less and less interested in writing more code that only lives in a virtual world. It feels like
there is no purpose for this code, like something is missing.&lt;/p&gt;
&lt;p&gt;The main thing I feel is missing is interaction with the physical world. The ability to see the
effects of the code when it is executed, to see things move and react to the world. While in
university I studied &lt;a href="https://www.tudelft.nl/en/ae"&gt;Aerospace engineering&lt;/a&gt; and
&lt;a href="https://www.auckland.ac.nz/en/engineering.html"&gt;Mechanical engineering&lt;/a&gt;. Two disciplines heavily
involved with real world physics. The fact that a design lives in the physical world adds all kinds
of additional constraints and behaviours. Some of which the result of interesting physical
behaviour and others the result of the continuous and analogue nature of the physical world.
All of this makes the engineering problems more interesting and challenging.&lt;/p&gt;
&lt;p&gt;One of the ways I have been incorporating making physical things is by working with timber. I've
build several bits of furniture and generally greatly enjoy working with wood. However working with
wood misses the technological side that I do also enjoy.&lt;/p&gt;
&lt;p&gt;A domain that combines both the physical and virtual worlds with large amounts of technology that
recently caught my eye is the world of robotics. As robotics is a combination of many different
fields there is a lot to discover and learn about. Even for the design of a simple robot you will
have to deal with fields like structural mechanics, electronics, software, perception, machine
intelligence etc.. As a bonus thanks to the availability of relatively cheap electronics and
structural components it is possible to build an interesting robot yourself.&lt;/p&gt;
&lt;p&gt;My current goal is to build an &lt;a href="https://en.wikipedia.org/wiki/Autonomous_robot"&gt;autonomous mobile robot&lt;/a&gt;
that is capable of navigating outdoor spaces while carrying some kind of cargo from
one location to another. Before I get to designing and building a robot from the ground
up there are a lot of things to learn. In order to speed up the learning process I
picked up a &lt;a href="https://scuttlerobot.org/"&gt;SCUTTLE&lt;/a&gt; robot kit and started learning about
&lt;a href="https://www.ros.org/"&gt;ROS&lt;/a&gt;. More on that will follow in a future post.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Security</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-security</link>
			<description>&lt;p&gt;One of the final chapters in the description of the development pipeline deals with security. In
this case I specifically mean the security of the pipeline and the underlying infrastructure,
not the security of the applications which are created using the pipeline.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-security</guid>
			<pubDate>Sun, 08 Nov 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;One of the final chapters in the description of the development pipeline deals with security. In
this case I specifically mean the security of the pipeline and the underlying infrastructure,
not the security of the applications which are created using the pipeline.&lt;/p&gt;
&lt;p&gt;The first question is why should you care about the security of the pipeline? After all
developers use the development pipelines via secured networks and their access permissions will be
set at the source control level. Additionally high trust levels exist between the pipeline
processes, the infrastructure and the source repository. In general this leads to the
security of the pipeline being placed lower on the priority list.&lt;/p&gt;
&lt;p&gt;Which issues could you run into if you deem the security of the pipeline less critical? One
argument comes from
&lt;a href="https://www.researchgate.net/publication/332834111_Vulnerabilities_in_Continuous_Delivery_Pipelines_A_Case_Study"&gt;pen tests&lt;/a&gt;
which show that CI/CD systems are a great way into
&lt;a href="https://www.blackhat.com/docs/eu-15/materials/eu-15-Mittal-Continuous-Intrusion-Why-CI-Tools-Are-An-Attackers-Best-Friend.pdf"&gt;corporate networks&lt;/a&gt;.
Additionally there have been a number of attacks aimed at
&lt;a href="https://medium.com/&amp;#64;hkparker/analysis-of-a-supply-chain-attack-2bd8fa8286ac"&gt;distributing&lt;/a&gt; malicious
code through trusted software packages. These so called
&lt;a href="https://en.wikipedia.org/wiki/Supply_chain_attack"&gt;supply chain attacks&lt;/a&gt; try to compromise the user
by inserting malicious code in third-party dependencies, i.e. the source code supply chain.&lt;/p&gt;
&lt;p&gt;In essence the problem comes down to the fact that the build pipeline and its associated infrastructure
have access to many different systems and resources which are normally not easily accessible for
its users. This makes your pipeline a target for malicious actors who could abuse some of the
following states for their own purposes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The development pipeline runs all tasks with the same user account on the executors and thereby
the same permissions. Obviously the worst case scenario would be running as an administrator.&lt;/li&gt;
&lt;li&gt;Multiple pipeline invocations executed on a single machine, either in parallel or sequential,
which allows a task in a pipeline to access the workspace of another pipeline. This ability can
for instance be used to by-pass access controls on source code.&lt;/li&gt;
&lt;li&gt;Downloading packages directly from external package repositories, e.g. NPM or Docker.&lt;/li&gt;
&lt;li&gt;Direct access to the internet, which allows downloading of malicious code and uploading of artefacts
to undesired locations.&lt;/li&gt;
&lt;li&gt;The development pipeline has the ability to update or overwrite existing artefacts.&lt;/li&gt;
&lt;li&gt;The executors have direct access to different resources that normal pipeline users don't have
access to. Specifically if the same infrastructure is used to build artefacts and
to deploy them to the production environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the problems with securing the development pipeline is that all the actions mentioned above
are in one way or another required for the pipeline to function, after all the pipeline needs to be
able to build and distribute artefacts. The follow up question then becomes can you distinguish between
normal use and malicious use?&lt;/p&gt;
&lt;p&gt;It turns out that distinguishing that this will be difficult because both forms of actions are
essentially the same, they both use the development pipeline for its intended purpose. So then in
order to prevent malicious use put up as many barriers to malicious use as possible, aka
&lt;a href="https://en.wikipedia.org/wiki/Defense_in_depth_(computing)"&gt;defence in depth&lt;/a&gt;. The following are a
number of possible ways to add barriers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grant the minimal possible permissions for the executors, both on the executor and from the executor
to the external resources. It is better to run the pipeline actions as a local user on the executor,
rather than using a domain user. Grant permissions to a specific resource to the action that
interacts with the resource.&lt;/li&gt;
&lt;li&gt;Execute a single pipeline per executor and never reuse the executor.&lt;/li&gt;
&lt;li&gt;Limit network connections to and from executors. In general executors do not need internet access,
save for a few pre-approved sites, .e.g. an artefact storage. There is also very little reason
for executors to connect to each other, especially if executors are short lived.&lt;/li&gt;
&lt;li&gt;Pull packages, e.g. NPM or Docker, only from an internal feed. Additions to the internal feed are made
after the specific package has been reviewed.&lt;/li&gt;
&lt;li&gt;The artefacts created with the pipeline should be tracked so that you know the origin, creation time,
storage locations and other data that can help identity an exact instance of an artefact. Under
ideal circumstances you would know exactly which sources and packages were used to create
the artefact as well.&lt;/li&gt;
&lt;li&gt;Artefacts should be immutable and never be allowed to overwritten.&lt;/li&gt;
&lt;li&gt;Do not use the executors that perform builds for deployments, use a set of executors that only
have deployment permissions but no permissions to source control etc..&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beyond these changes there are many other ways to reduce the attack surface as documented in the
security literature. In the end the goal of this post is more to point out that the security of the
development pipeline is important, rather than providing ways to make a pipeline more secure. The
exact solutions for pipeline security depend very heavily on the way the pipeline is constructed and
what other forms of security validation have been placed around the pipeline.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Software development pipeline - Infrastructure dependency reduction</title>
			<link>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-infrastructure-dependency-reduction</link>
			<description>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-considerations-for-infrastructure-improvements.html"&gt;last post&lt;/a&gt;
I explained a few ways to improve the development pipeline infrastructure while
keeping downtime minimal. One important consideration for the
&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;resilience&lt;/a&gt; of the
pipeline is to reduce the dependencies between the pipeline and the infrastructure.&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Software-development-pipeline-infrastructure-dependency-reduction</guid>
			<pubDate>Sun, 01 Nov 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;The &lt;a href="/posts/Software-development-pipeline-considerations-for-infrastructure-improvements.html"&gt;last post&lt;/a&gt;
I explained a few ways to improve the development pipeline infrastructure while
keeping downtime minimal. One important consideration for the
&lt;a href="/posts/Software-development-pipeline-Design-resilience.html"&gt;resilience&lt;/a&gt; of the
pipeline is to reduce the dependencies between the pipeline and the infrastructure.&lt;/p&gt;
&lt;p&gt;So what does unwanted coupling between the pipeline and the
infrastructure mean? After all the pipeline code makes assumptions about the capabilities
of the infrastructure it runs on, just like every other piece of code.
For development pipelines examples of unwanted dependencies are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pipeline is created by using tasks specific to a CI/CD system which means that the artefacts
can only be created on the infrastructure&lt;/li&gt;
&lt;li&gt;Pipeline stages expect the outputs from previous stages to be available on the executor,
or worse, they expect the outputs from other pipelines to be on the executor&lt;/li&gt;
&lt;li&gt;A pipeline task assumes that certain tools have been installed on the executors&lt;/li&gt;
&lt;li&gt;A pipeline task assumes that it has specific permissions on an executor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first two items mentioned impacts the velocity at which development can take place. By relying
completely on the CI/CD system for artefact creation cycle times increases. Additionally
making changes to the pipeline often requires executing it multiple times to debug it.&lt;/p&gt;
&lt;p&gt;The second set of items are related to the ease of switching tooling, e.g. when changing vendors
or during disaster recovery. These may or may not be of concern depending on the direction of
development and technology. In my experience vendor lock-in is something to keep in mind if for no
other reason then that switching vendors can be prohibitively complicated if pipeline processes are
too tightly coupled to the CI/CD system.&lt;/p&gt;
&lt;p&gt;If any of the issues mentioned are concerning to you then partially or completely decoupling
the development pipeline from the infrastructure will be a worthwhile exercise. This
can be achieved with some of the following steps.&lt;/p&gt;
&lt;h4&gt;Versions for everybody&lt;/h4&gt;
&lt;p&gt;Ensure that all code, tools, scripts and resources used in the pipeline are versioned. That way you
know exactly what is required for a specific pipeline execution. Which make executions repeatable
in the future when newer versions of tools and resources have been released.&lt;/p&gt;
&lt;h4&gt;Only the workspace is yours&lt;/h4&gt;
&lt;p&gt;Pipeline actions should work only in their own workspace. Any code, scripts or tools they require
are put in the workspace. This reduces scattering of data during the execution of a pipeline and
reduces pollution of the executors.&lt;/p&gt;
&lt;h4&gt;Lean executors&lt;/h4&gt;
&lt;p&gt;Make your pipeline actions assume they will run on a bare minimum runtime. This will
ensure that your pipelines will obtain their preferred version of tools they need and
install them in their own workspace. The benefit of doing this that a pipeline can be executed on
any available executor, either in your CI/CD system or on a developer machine, as it will not be
making any assumptions about the capabilities of an executor.&lt;/p&gt;
&lt;h4&gt;Single use executors are cleaner&lt;/h4&gt;
&lt;p&gt;Configure the CI/CD system to use a clean executor for every job. This will ensure that changes made
during a previous job don't interfere with the current job. Additionally it will enforce the use of
immutable infrastructure for the executors, thus allowing versioning of the executors.&lt;/p&gt;
&lt;h4&gt;Use the CI/CD system as a task executor&lt;/h4&gt;
&lt;p&gt;Keep the configuration for the jobs in the CI/CD system to a minimum. Ideally the entire configuration
is the execution of a script or tools with a simple set of arguments. By reducing the job of the
CI/CD system to executing a script or tool it is simple to execute the pipeline actions somewhere
else, e.g. on a developer machine or a different CI/CD system.&lt;/p&gt;
&lt;h4&gt;Treat pipeline services as critical&lt;/h4&gt;
&lt;p&gt;All services used by the pipeline should be treated as mission critical systems. After all a failure
in one of these systems can stop all your pipelines from executing. So reduce the number of services
you use in your pipeline and improve the resilience the services you have to rely on using the standard
approaches.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Calvinverse - An example build infrastructure</title>
			<link>https://www.petrikvandervelde.nl/posts/Calvinverse-an-example-build-infrastructure</link>
			<description>&lt;p&gt;This post introduces the &lt;a href="https://www.calvinverse.net/"&gt;Calvinverse&lt;/a&gt;
&lt;a href="https://github.com/Calvinverse"&gt;project&lt;/a&gt; which provides the source code for the different resources
required to create the infrastructure for a build pipeline. The Calvinverse resources have been
developed for two main reasons:&lt;/p&gt;</description>
			<guid>https://www.petrikvandervelde.nl/posts/Calvinverse-an-example-build-infrastructure</guid>
			<pubDate>Fri, 28 Jun 2019 00:00:00 GMT</pubDate>
			<content:encoded>&lt;p&gt;This post introduces the &lt;a href="https://www.calvinverse.net/"&gt;Calvinverse&lt;/a&gt;
&lt;a href="https://github.com/Calvinverse"&gt;project&lt;/a&gt; which provides the source code for the different resources
required to create the infrastructure for a build pipeline. The Calvinverse resources have been
developed for two main reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To provide me with a way to experiment with and learn more about
&lt;a href="https://thenewstack.io/a-brief-look-at-immutable-infrastructure-and-why-it-is-such-a-quest/"&gt;immutable&lt;/a&gt; &lt;a href="https://twitter.com/jezhumble/status/970334897544900609"&gt;infrastructure&lt;/a&gt; and
&lt;a href="https://en.wikipedia.org/wiki/Infrastructure_as_code"&gt;infrastructure-as-code&lt;/a&gt; as applied to build infrastructure.&lt;/li&gt;
&lt;li&gt;To provide resources that can be used to set up the infrastructure for a complete
&lt;a href="/posts/On-prem-vs-cloud-build-systems"&gt;on-prem&lt;/a&gt; build system. The system should provide a build
controller with build agents, artefact storage and all the necessary tools to monitor the different
services and diagnose issues.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Calvinverse resources can be configured and deployed for different sizes of infrastructure, from
small setups only used by a few developers to a large setup used by many developers for the development
of many products. How to configure the resources for small, medium or large environments and their
hardware requirements will be discussed in future posts.&lt;/p&gt;
&lt;p&gt;The resources in the Calvinverse project are designed to be run as a self-contained system. While
daily maintenance is minimal it is not a hosted system so some maintenance is required. For instance
OS updates will be required on a regular basis. These can either be applied to existing resources,
through the automatic updates, or by applying the new updates to the templates and then replacing
the existing resources with a new instance. The latter approach case can be automated, however there
is no code in any of the Calvinverse repositories to do this automatically.&lt;/p&gt;
&lt;p&gt;The different resources in the Calvinverse project contain a set of tools and applications which
provide all the necessary capabilities to create the infrastructure for a build pipeline. Amongst these
capabilities are service discovery, build execution, artefact storage, metrics, alerting and
log processing.&lt;/p&gt;
&lt;p&gt;The following applications and approaches are used for service discovery and configuration storage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using &lt;a href="https://www.consul.io"&gt;Consul&lt;/a&gt; to provide service discovery and machine discovery via
&lt;a href="https://www.consul.io/docs/agent/dns.html"&gt;DNS&lt;/a&gt; inside an environment. An environment is defined
as all machines that are part of a &lt;a href="https://www.consul.io/docs/internals/architecture.html"&gt;consul datacenter&lt;/a&gt;.
It is possible to have multiple environments where the machines may all be on the same network but
in general will not be communicating across environments. This is useful for cases where having
multiple environments makes sense, for instance when having a production environment and a test
environment. The benefit of using Consul as a DNS is that it allows a resource to have a consistent
name across different environments without the DNS names clashing. For instance if there is a
production environment and a test environment then it is possible to use the same DNS name
for a resource, even though the actual machine names will be different. This allows using the
Consul DNS name in tools and scripts without having to keep in mind the environment the tool
is deployed in.
Finally Consul is also used for the distributed key-value store that all applications can obtain
configuration information from thereby centralizing the configuration information.&lt;/li&gt;
&lt;li&gt;Using one or more &lt;a href="https://vaultproject.io"&gt;Vault&lt;/a&gt; instance to handle all the secrets required
for the environment. Vault provides authenticated access for resources to securely access secrets,
login credentials and other information that should be kept secure. This allows centralizing the
storage and distribution of secrets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the build work Calvinverse uses the following applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://jenkins.io"&gt;Jenkins&lt;/a&gt; is used as the
&lt;a href="https://github.com/Calvinverse/resource.build.master"&gt;build controller&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Build executors connect to Jenkins using the &lt;a href="https://plugins.jenkins.io/swarm"&gt;swarm plugin&lt;/a&gt; so
that agents can connect when it starts. In the Calvinverse project there are currently only
&lt;a href="https://github.com/Calvinverse/resource.build.agent.windows"&gt;Windows&lt;/a&gt; based executors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;a href="https://github.com/Calvinverse/resource.artefacts"&gt;artefact storage&lt;/a&gt; Calvin verse uses the
&lt;a href="https://www.sonatype.com/nexus-repository-oss"&gt;Nexus&lt;/a&gt; application. The image is configured such that
a new instance of the image will create artefact repositories for &lt;a href="https://www.nuget.org"&gt;NuGet&lt;/a&gt;,
&lt;a href="https://www.npmjs.com/"&gt;Npm&lt;/a&gt;, &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; and general ZIP artefacts.&lt;/p&gt;
&lt;p&gt;For &lt;a href="https://github.com/Calvinverse/resource.queue"&gt;message distribution&lt;/a&gt; Calvinverse uses the &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt; application. The image is configured such that a new instance of the image will
try to connect to the existing cluster in the environment. If no cluster exists then the first
instance of the image will form the start of the cluster in the environment.&lt;/p&gt;
&lt;p&gt;Metrics, monitoring and alerting capabilities are provided by the
&lt;a href="https://www.influxdata.com/"&gt;Influx&lt;/a&gt; stack, consisting of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.influxdata.com/time-series-platform/"&gt;InfluxDB&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.metrics.storage"&gt;metrics collection&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; for &lt;a href="https://github.com/Calvinverse/resource.metrics.dashboard"&gt;dashboards&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.rabbitmq.com/"&gt;Telegraf&lt;/a&gt; is installed on each resource to collect metrics and send
them to InfluxDb.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.influxdata.com/time-series-platform/chronograf/"&gt;Chronograf&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.metrics.monitoring"&gt;alert configurations&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.influxdata.com/time-series-platform/kapacitor/"&gt;Kapacitor&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.metrics.monitoring"&gt;alerting&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Build and system logs are processed by the &lt;a href="https://www.elastic.co/"&gt;Elastic&lt;/a&gt; stack consisting off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/products/elasticsearch"&gt;Elasticsearch&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.documents.storage"&gt;log storage&lt;/a&gt;, both system logs
and build logs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/products/kibana"&gt;Kibana&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.documents.dashboard"&gt;log dashboards&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Logs collected via &lt;a href="https://www.syslog-ng.com/products/open-source-log-management/"&gt;syslog-ng&lt;/a&gt; on
Linux and a modified version of &lt;a href="https://github.com/pvandervelde/filebeat.mqtt"&gt;Filebeat&lt;/a&gt; on windows.
Logs are sent to RabbitMQ to ensure that the unprocessed logs aren't lost when something any part
of the log stack goes offline.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/products/logstash"&gt;Logstash&lt;/a&gt; for
&lt;a href="https://github.com/Calvinverse/resource.logs.processor"&gt;processing logs&lt;/a&gt; from RabbitMQ to
Elasticsearch using &lt;a href="https://github.com/Calvinverse/calvinverse.logs.filters"&gt;filters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It should be noted that while the Calvinverse resources combine to create a complete build environment
the resources might need some alterations to fit in with the work flow and processes that are being
followed. After all each company is different and applies different workflows. Additionally users
might want to replace some of the resources with versions of their own, e.g. to replace Influx with
&lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;.&lt;/p&gt;
</content:encoded>
		</item>
	</channel>
</rss>